{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Complaints ‚Üí Reducci√≥n, Deduplicaci√≥n y Carga a Neo4j/Qdrant\n","\n","Este documento resume, de forma reproducible, todo lo que se ejecut√≥ en el notebook para:\n","1) reducir embeddings de **complaints** con **IPCA**,  \n","2) seleccionar representantes con **LSH** y **MiniBatchKMeans**,  \n","3) consolidar CSVs consistentes,  \n","4) subir **representantes** a **Qdrant** (con UUID v5 determin√≠sticos), y  \n","5) **cargar/relacionar** complaints en **Neo4j**.\n","\n","---\n","\n","## üìÅ 1) Montaje de Drive y rutas base\n","\n","```python\n","from google.colab import drive\n","drive.mount('/content/drive')\n","````\n","\n","**Rutas principales**\n","\n","* `BASE_DIR`: `/content/drive/MyDrive/NHTSA/embeddings/complaints_e5_mlg_instruct`\n","* `REDUCED_DIR`: `${BASE_DIR}/reduced_shards`\n","* `IPCA_PATH`: `${BASE_DIR}/ipca_model.npz` (modelo IPCA entrenado previamente)\n","\n","---\n","\n","## üß≠ 2) Reducci√≥n con IPCA (idempotente)\n","\n","**Objetivo**: transformar embeddings crudos (`embeddings_shard_XXXX.npy`) a reducidos v√°lidos `.npy` con cabecera, y metas alineadas `reduced_meta_XXXX.parquet`.\n","\n","**Puntos clave**\n","\n","* `BLOCK_ROWS=100_000` para procesado en bloques.\n","* Si `reduced_XXXX.npy` y `reduced_meta_XXXX.parquet` existen y coinciden en tama√±o ‚Üí **skip seguro**.\n","* Funci√≥n `maybe_repair_reduced` reescribe cabecera `.npy` si alg√∫n archivo reducido fue guardado ‚Äúcrudo‚Äù.\n","\n","**Funciones**\n","\n","* `load_ipca_npz(path)`: reconstruye el objeto `IncrementalPCA` desde `.npz`.\n","* `list_base_shards(base_dir)`: empareja `embeddings_shard_XXXX.npy` con `meta_shard_XXXX.parquet`.\n","* `transform_to_reduced(ipca, idx, npy_path, meta_path)`: aplica `ipca.transform` en bloques y guarda `reduced_XXXX.npy` + `reduced_meta_XXXX.parquet`.\n","* `ensure_reduced_all(ipca)`: recorre todos los shards base y deja todo consistente en `REDUCED_DIR`.\n","\n","**Ejecuci√≥n**\n","\n","```python\n","ipca = load_ipca_npz(IPCA_PATH)\n","ready_ids = ensure_reduced_all(ipca)\n","```\n","\n","---\n","\n","## üîé 3) LSH (representantes r√°pidos, reanudable por grupos)\n","\n","**Objetivo**: escoger un representante por ‚Äúbucket‚Äù de similitud, **por grupos de 5 shards** (controlable con `GROUP_SIZE`).\n","\n","**Puntos clave**\n","\n","* SimHash-like: `LSH_BANDS=20`, `LSH_ROWS_PER_BAND=12`, `LSH_SEED=42`.\n","* Produce CSVs `lsh_group_XXXX_reps.csv` con columnas clave: `id`, `_h`, `shard_idx`, `row_in_shard`.\n","* Repara tipos de CSVs antiguos con `repair_group_csvs_if_needed()` (merge contra metas reducidas).\n","\n","**Funciones**\n","\n","* `sign_hash_blocks(X)`, `lsh_buckets(signs)`: hashing y bucketing.\n","* `process_group(group_idx, shard_ids, ncomp)`: genera representantes del grupo ‚Üí `lsh_group_XXXX_reps.csv`.\n","* `run_lsh_groups(valid_ids, ncomp, group_size=5)`: recorre los grupos.\n","* `repair_group_csvs_if_needed()`: rellena `shard_idx/row_in_shard` si faltan.\n","* `consolidate_groups_csv(out_path)`: genera `representantes_global.csv`.\n","\n","**Ejecuci√≥n**\n","\n","```python\n","run_lsh_groups(ready_ids, ncomp=ipca.components_.shape[0])\n","repair_group_csvs_if_needed()\n","consolidate_groups_csv(REDUCED_DIR / \"representantes_global.csv\")\n","```\n","\n","---\n","\n","## üß© 4) Deduplicaci√≥n Global con MiniBatchKMeans (2-pasos)\n","\n","**Objetivo**: una deduplicaci√≥n **memoria-segura** en dos fases:\n","\n","1. `partial_fit` en streaming sobre todos los `reduced_XXXX.npy`.\n","2. Asignaci√≥n ‚Üí seleccionar el **m√°s cercano** por cl√∫ster (representante final).\n","\n","**Par√°metros**\n","\n","* `K_TARGET = 20000` cl√∫steres (‚âà n¬∫ de representantes).\n","* `BATCH_ROWS = 100_000` filas por bloque.\n","* `MB_SIZE = 4096` minibatch_size.\n","* Salidas:\n","\n","  * `OUT_DIR/kmeans_global/mbkm.pkl` y `centers.npy`\n","  * `REPS_CSV_PATH = .../kmeans_global/representantes_global_kmeans.csv`\n","\n","**Ejecuci√≥n (resumen)**\n","\n","```python\n","# Fit en streaming (idempotente si existen modelo + centros)\n","# Assign + selecci√≥n del m√°s cercano (guarda representantes)\n","```\n","\n","**CSV Final de Representantes (KMeans)**\n","\n","* Columnas principales: `id`, `make`, `model`, `year`, `component`, `_h`,\n","  `shard_idx`, `row_in_shard`, `cluster`, `dist2`.\n","\n","---\n","\n","## üï∏Ô∏è 5) Neo4j ‚Äî Carga/Relaciones de Complaints\n","\n","**Objetivo**: crear `(:Complaint {qid})` con propiedades y relaciones normalizadas.\n","\n","**Importante**\n","\n","* Usar el **mismo namespace** que Qdrant para que `qid` coincida con el `id` del punto.\n","* Unicidad **solo** en `Complaint.qid`.\n","* `Make/Model/Component`: √≠ndices **no √∫nicos** + normalizaci√≥n (UPPER/TRIM) para no romper por duplicados existentes.\n","\n","**Configuraci√≥n**\n","\n","* `CSV_REPS`: usar el CSV final (KMeans o consolidado LSH).\n","* Variables entorno:\n","\n","  * `NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD` (e.g. Aura: `neo4j+s://...`).\n","\n","**DDL ejecutado**\n","\n","```cypher\n","CREATE CONSTRAINT complaint_qid IF NOT EXISTS FOR (c:Complaint) REQUIRE c.qid IS UNIQUE;\n","CREATE INDEX make_name_idx IF NOT EXISTS FOR (m:Make) ON (m.name);\n","CREATE INDEX model_name_idx IF NOT EXISTS FOR (m:Model) ON (m.name);\n","CREATE INDEX component_name_idx IF NOT EXISTS FOR (x:Component) ON (x.name);\n","CREATE INDEX complaint_make IF NOT EXISTS FOR (c:Complaint) ON (c.make);\n","CREATE INDEX complaint_model IF NOT EXISTS FOR (c:Complaint) ON (c.model);\n","```\n","\n","**Upsert (Cypher)**\n","\n","```cypher\n","UNWIND $rows AS r\n","MERGE (c:Complaint {qid: r.qid})\n","  ON CREATE SET\n","    c.comp_id      = r.comp_id,\n","    c._h           = r._h,\n","    c.shard_idx    = r.shard_idx,\n","    c.row_in_shard = r.row_in_shard,\n","    c.cluster      = r.cluster,\n","    c.dist2        = r.dist2,\n","    c.make         = r.make,\n","    c.model        = r.model,\n","    c.year         = r.year,\n","    c.component    = r.component\n","  ON MATCH SET\n","    c.comp_id      = coalesce(r.comp_id, c.comp_id),\n","    c._h           = coalesce(r._h, c._h),\n","    c.shard_idx    = coalesce(r.shard_idx, c.shard_idx),\n","    c.row_in_shard = coalesce(r.row_in_shard, c.row_in_shard),\n","    c.cluster      = coalesce(r.cluster, c.cluster),\n","    c.dist2        = coalesce(r.dist2, c.dist2),\n","    c.make         = coalesce(r.make, c.make),\n","    c.model        = coalesce(r.model, c.model),\n","    c.year         = coalesce(r.year, c.year),\n","    c.component    = coalesce(r.component, c.component)\n","\n","FOREACH (_ IN CASE WHEN r.make IS NOT NULL THEN [1] ELSE [] END |\n","  MERGE (m:Make {name: r.make})\n","  MERGE (c)-[:OF_MAKE]->(m)\n",")\n","\n","FOREACH (_ IN CASE WHEN r.model IS NOT NULL THEN [1] ELSE [] END |\n","  MERGE (md:Model {name: r.model})\n","  MERGE (c)-[:OF_MODEL]->(md)\n",")\n","\n","FOREACH (_ IN CASE WHEN r.component IS NOT NULL THEN [1] ELSE [] END |\n","  MERGE (x:Component {name: r.component})\n","  MERGE (c)-[:MENTIONS]->(x)\n",")\n","```\n","\n","**Normalizaci√≥n previa (Python)**\n","\n","```python\n","def _norm(s):\n","    if s is None: return None\n","    s = str(s).strip()\n","    return s.upper() if s != \"\" else None\n","\n","for col in (\"make\",\"model\",\"component\"):\n","    if col in df.columns:\n","        df[col] = df[col].apply(_norm)\n","```\n","\n","**Resultado esperado**\n","\n","* Logs tipo:\n","\n","  ```\n","  Namespace (string): https://nhtsa.example/complaints\n","  Namespace (UUID)  : 82cc465c-bfae-5901-868b-5e87923a97f9\n","  Neo4j URI : neo4j+s://<tu-aura>.databases.neo4j.io\n","  Neo4j USER: neo4j\n","  Upsert Neo4j: 5000/20000\n","  ...\n","  ‚úÖ Complaints cargados/actualizados en Neo4j.\n","  ```\n","\n","---\n","\n","## üîó 6) Integraci√≥n con b√∫squeda sem√°ntica (Qdrant ‚Üí Neo4j)\n","\n","**Buscar Complaints**\n","\n","* En Qdrant: colecci√≥n `nhtsa_complaints`.\n","* `hits[i][\"id\"]` = **qid** (UUID v5).\n","* En Neo4j: `MATCH (c:Complaint {qid: qid})`.\n","\n","**Ejemplo de `ask()` (ruta complaints)**\n","\n","```python\n","qids = [h[\"id\"] for h in hits if h.get(\"id\")]\n","details = cypher(\"\"\"\n","    UNWIND $ids AS qid\n","    MATCH (c:Complaint {qid: qid})\n","    OPTIONAL MATCH (c)-[:OF_MAKE]->(mk:Make)\n","    OPTIONAL MATCH (c)-[:OF_MODEL]->(md:Model)\n","    OPTIONAL MATCH (c)-[:MENTIONS]->(x:Component)\n","    RETURN\n","      c.qid AS qid,\n","      c.comp_id AS comp_id,\n","      c.make AS make,\n","      c.model AS model,\n","      c.year AS year,\n","      c.component AS component,\n","      collect(DISTINCT mk.name) AS makes_related,\n","      collect(DISTINCT md.name) AS models_related,\n","      collect(DISTINCT x.name)  AS mentions\n","\"\"\", {\"ids\": qids})\n","```\n","\n","---\n","\n","## ‚úÖ 7) Validaciones r√°pidas (Neo4j)\n","\n","**Conteo**\n","\n","```cypher\n","MATCH (c:Complaint) RETURN count(c) AS complaints;\n","```\n","\n","**Muestra**\n","\n","```cypher\n","MATCH (c:Complaint)\n","RETURN c.qid, c.comp_id, c.make, c.model, c.year, c.component\n","LIMIT 5;\n","```\n","\n","**Relaciones creadas**\n","\n","```cypher\n","MATCH (c:Complaint)-[r]->(x)\n","RETURN type(r) AS rel, count(*) AS n\n","ORDER BY n DESC;\n","```\n","\n","---\n","\n","## üßπ 8) (Opcional) Consolidar duplicados previos y aplicar unicidad\n","\n","Si detectas duplicados en `Make/Model/Component`, puedes **merg**ear antes de poner restricciones √∫nicas.\n","\n","**Detectar**\n","\n","```cypher\n","MATCH (m:Model)\n","WITH toUpper(trim(m.name)) AS k, collect(m) AS nodes\n","WHERE k IS NOT NULL AND size(nodes) > 1\n","RETURN k, size(nodes) AS cnt\n","ORDER BY cnt DESC LIMIT 50;\n","```\n","\n","**Consolidar (requiere APOC)**\n","\n","```cypher\n","MATCH (m:Model)\n","WITH toUpper(trim(m.name)) AS k, collect(m) AS nodes\n","WHERE k IS NOT NULL AND size(nodes) > 1\n","CALL apoc.refactor.mergeNodes(nodes, {properties:'discard', mergeRels:true}) YIELD node\n","RETURN node;\n","```\n","\n","**Aplicar unicidad**\n","\n","```cypher\n","CREATE CONSTRAINT model_name IF NOT EXISTS FOR (m:Model) REQUIRE m.name IS UNIQUE;\n","```\n","\n","> Repetir para `Make` y `Component`.\n","\n","---\n","\n","## üßØ 10) Troubleshooting\n","\n","* **Deprecation Qdrant**: `client.search` ‚Üí usa `client.query_points` (mismo filtro/payloads).\n","* **Neo4j Constraint Creation Failed**: existe **data duplicada**.\n","  Soluci√≥n: eliminar/merge duplicados **antes** de crear la restricci√≥n √∫nica.\n","* **Desalineado reducido/meta**: revisa que `reduced_XXXX.npy` y `reduced_meta_XXXX.parquet` tengan el **mismo `n`**. Usa `maybe_repair_reduced(...)`.\n","\n","---\n","\n","## üì¶ 11) Variables clave y convenciones\n","\n","* **Namespace UUID v5** (cr√≠tico):\n","  `NAMESPACE_STR = \"https://nhtsa.example/complaints\"`\n","* **ID determin√≠stico del punto**:\n","  `qid = uuid5(NAMESPACE, f\"rep_{shard_idx:04d}_{row_in_shard:06d}\")`\n","* **Colecci√≥n Qdrant**: `nhtsa_complaints`\n","* **Etiquetas y relaciones (Neo4j)**:\n","\n","  * `(:Complaint {qid})`\n","  * `(:Make {name})`, `(:Model {name})`, `(:Component {name})`\n","  * `[:OF_MAKE]`, `[:OF_MODEL]`, `[:MENTIONS]`\n","\n","---\n","\n","### ‚úÖ Estado Final\n","\n","* Reducci√≥n IPCA ‚Üí OK\n","* LSH por grupos y consolidado ‚Üí OK\n","* Dedup global KMeans ‚Üí OK (representantes guardados)\n","* Qdrant (representantes con UUID v5) ‚Üí **Subido**\n","* Neo4j (upsert por lotes con normalizaci√≥n) ‚Üí **Cargado/Actualizado**"],"metadata":{"id":"tgbnQeYhRN6Q"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWXqWSiOC6JA","executionInfo":{"status":"ok","timestamp":1760302540879,"user_tz":360,"elapsed":10073,"user":{"displayName":"Lucero Guadalupe Contreras Hern√°ndez","userId":"07832692706572343340"}},"outputId":"c9a6f544-b8f8-4bc6-a89a-504cbdf0d832"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cFrHBA5GCBYa","executionInfo":{"status":"ok","timestamp":1760305575450,"user_tz":360,"elapsed":9493,"user":{"displayName":"Lucero Guadalupe Contreras Hern√°ndez","userId":"07832692706572343340"}},"outputId":"cd31ce59-d404-4a99-a211-96de260de8b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["== A) Transformaci√≥n IPCA ‚Üí reducidos (idempotente) ==\n","Shards base detectados: 21\n","‚Ü™Ô∏é Reducido 0000 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0001 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0002 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0003 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0004 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0005 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0006 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0007 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0008 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0009 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0010 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0011 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0012 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0013 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0014 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0015 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0016 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0017 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0018 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0019 OK. SKIP.\n","‚Ü™Ô∏é Reducido 0020 OK. SKIP.\n","== B) LSH por lotes de 5 shards reducidos (reanuda por grupo) ==\n","‚Ü™Ô∏é Grupo 0000 ya procesado. SKIP.\n","‚Ü™Ô∏é Grupo 0001 ya procesado. SKIP.\n","‚Ü™Ô∏é Grupo 0002 ya procesado. SKIP.\n","‚Ü™Ô∏é Grupo 0003 ya procesado. SKIP.\n","‚Ü™Ô∏é Grupo 0004 ya procesado. SKIP.\n","üîß Reparado lsh_group_0005_reps.csv a√±adiendo (shard_idx,row_in_shard).\n","üîß Reparado lsh_group_0006_reps.csv a√±adiendo (shard_idx,row_in_shard).\n","üîß Reparado lsh_group_0007_reps.csv a√±adiendo (shard_idx,row_in_shard).\n","üîß Reparado lsh_group_0008_reps.csv a√±adiendo (shard_idx,row_in_shard).\n","üîß Reparado lsh_group_0009_reps.csv a√±adiendo (shard_idx,row_in_shard).\n","üîß Reparado lsh_group_0010_reps.csv a√±adiendo (shard_idx,row_in_shard).\n","üì¶ Consolidado: 43,781 representantes ‚Üí /content/drive/MyDrive/NHTSA/embeddings/complaints_e5_mlg_instruct/reduced_shards/representantes_global.csv\n"]}],"source":["# ===============================================\n","# Complaints ‚Üí IPCA reducido + LSH reanudable (CPU)\n","# - Reusa IPCA guardado\n","# - Guarda reducidos .npy con cabecera v√°lida\n","# - LSH por lotes de 5 shards\n","# - Reparaci√≥n opcional de CSVs viejos (tipos consistentes)\n","# - Consolidaci√≥n final con columnas clave\n","# ===============================================\n","import os, gc, re\n","from pathlib import Path\n","from typing import List, Tuple, Dict\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.decomposition import IncrementalPCA\n","from tqdm import tqdm\n","\n","# -------------------- Config --------------------\n","BASE_DIR    = Path(\"/content/drive/MyDrive/NHTSA/embeddings/complaints_e5_mlg_instruct\")\n","REDUCED_DIR = BASE_DIR / \"reduced_shards\"\n","IPCA_PATH   = BASE_DIR / \"ipca_model.npz\"   # modelo entrenado previamente\n","BLOCK_ROWS  = 100_000                      # tama√±o de bloque para transform()\n","GROUP_SIZE  = 5                            # LSH por lotes de 5 shards (‚âà125k filas)\n","\n","# LSH par√°metros (SimHash-like)\n","LSH_BANDS          = 20\n","LSH_ROWS_PER_BAND  = 12\n","LSH_SEED           = 42\n","\n","os.makedirs(REDUCED_DIR, exist_ok=True)\n","\n","# -------------------- IPCA utils --------------------\n","def load_ipca_npz(path: Path) -> IncrementalPCA:\n","    if not path.exists():\n","        raise FileNotFoundError(f\"No existe el modelo IPCA: {path}\")\n","    z = np.load(path, allow_pickle=False)\n","    n_comp = z[\"components_\"].shape[0]\n","    ipca = IncrementalPCA(n_components=n_comp)\n","    ipca.components_       = z[\"components_\"]\n","    ipca.mean_             = z[\"mean_\"]\n","    ipca.var_              = z[\"var_\"]\n","    ipca.singular_values_  = z[\"singular_values_\"]\n","    ipca.n_samples_seen_   = int(z[\"n_samples_seen_\"])\n","    if \"n_features_in_\" in z and z[\"n_features_in_\"] is not None:\n","        ipca.n_features_in_ = int(z[\"n_features_in_\"])\n","    # Atributos derivados (por compatibilidad)\n","    if \"explained_variance_\" in z:\n","        ipca.explained_variance_ = z[\"explained_variance_\"]\n","    else:\n","        ipca.explained_variance_ = (ipca.singular_values_ ** 2) / max(ipca.n_samples_seen_ - 1, 1)\n","    if \"explained_variance_ratio_\" in z:\n","        ipca.explained_variance_ratio_ = z[\"explained_variance_ratio_\"]\n","    else:\n","        total_var = np.sum(getattr(ipca, \"var_\", ipca.explained_variance_))\n","        ipca.explained_variance_ratio_ = (\n","            np.zeros_like(ipca.explained_variance_) if total_var <= 0\n","            else ipca.explained_variance_ / total_var\n","        )\n","    return ipca\n","\n","# -------------------- Shards base --------------------\n","def list_base_shards(base_dir: Path) -> List[Tuple[int, Path, Path]]:\n","    npys  = sorted(base_dir.glob(\"embeddings_shard_*.npy\"))\n","    metas = sorted(base_dir.glob(\"meta_shard_*.parquet\"))\n","    idx_npy  = {int(re.findall(r'(\\d{4})', p.stem)[0]): p for p in npys if re.findall(r'(\\d{4})', p.stem)}\n","    idx_meta = {int(re.findall(r'(\\d{4})', p.stem)[0]): p for p in metas if re.findall(r'(\\d{4})', p.stem)}\n","    pairs = []\n","    for idx in sorted(set(idx_npy) & set(idx_meta)):\n","        pairs.append((idx, idx_npy[idx], idx_meta[idx]))\n","    return pairs\n","\n","# -------------------- Escritura NPY con cabecera --------------------\n","def open_npy_memmap(path: Path, shape, dtype=np.float32):\n","    \"\"\"Crea un .npy con cabecera v√°lida usando open_memmap.\"\"\"\n","    from numpy.lib.format import open_memmap\n","    return open_memmap(str(path), mode=\"w+\", dtype=dtype, shape=shape)\n","\n","# -------------------- Reducci√≥n IPCA (idempotente) --------------------\n","def transform_to_reduced(ipca: IncrementalPCA, idx: int, npy_path: Path, meta_path: Path):\n","    out_npy  = REDUCED_DIR / f\"reduced_{idx:04d}.npy\"\n","    out_meta = REDUCED_DIR / f\"reduced_meta_{idx:04d}.parquet\"\n","\n","    # Si ya existen y consistentes ‚Üí skip\n","    if out_npy.exists() and out_meta.exists():\n","        try:\n","            X_test = np.load(out_npy, mmap_mode=\"r\")\n","            n_npy  = int(X_test.shape[0])\n","            n_meta = len(pd.read_parquet(out_meta))\n","            if n_npy == n_meta and n_npy > 0:\n","                print(f\"‚Ü™Ô∏é Reducido {idx:04d} OK. SKIP.\")\n","                return\n","        except Exception:\n","            pass  # reescribe si hay problema\n","\n","    # Abrir fuente (puede ser grande)\n","    try:\n","        Xsrc = np.load(npy_path, mmap_mode=\"r\")\n","        n_src, d_src = int(Xsrc.shape[0]), int(Xsrc.shape[1])\n","    except Exception as e:\n","        print(f\"[skip] base {idx:04d} ilegible: {e}\")\n","        return\n","\n","    try:\n","        meta = pd.read_parquet(meta_path)\n","    except Exception as e:\n","        print(f\"[skip] meta {idx:04d} ilegible: {e}\")\n","        return\n","\n","    n = min(n_src, len(meta))\n","    if n == 0:\n","        print(f\"[skip] {idx:04d} n=0\")\n","        return\n","    if n != n_src or n != len(meta):\n","        print(f\"[warn] {idx:04d} filas npy={n_src} vs meta={len(meta)} ‚Üí n={n}\")\n","\n","    # Crear .npy destino CON cabecera\n","    n_comp = int(ipca.components_.shape[0])\n","    tmp_npy = out_npy.with_suffix(\".npy.tmp\")\n","    fp = open_npy_memmap(tmp_npy, shape=(n, n_comp), dtype=np.float32)\n","\n","    wrote = 0\n","    for i in tqdm(range(0, n, BLOCK_ROWS), desc=f\"IPCA {idx:04d}\", leave=False):\n","        j = min(n, i + BLOCK_ROWS)\n","        X = np.array(Xsrc[i:j], dtype=np.float32, copy=False)\n","        Z = ipca.transform(X)\n","        fp[wrote:wrote + (j - i), :] = Z.astype(np.float32, copy=False)\n","        wrote += (j - i)\n","        del X, Z\n","        gc.collect()\n","\n","    # Cierra y mueve atom.\n","    del fp\n","    os.replace(tmp_npy, out_npy)\n","    meta.iloc[:n].reset_index(drop=True).to_parquet(out_meta, index=False)\n","    print(f\"‚úÖ Reducido {idx:04d}: {n} filas ‚Üí {out_npy.name}\")\n","\n","def ensure_reduced_all(ipca: IncrementalPCA) -> List[int]:\n","    pairs = list_base_shards(BASE_DIR)\n","    print(f\"Shards base detectados: {len(pairs)}\")\n","    ready = []\n","    for idx, pX, pM in pairs:\n","        transform_to_reduced(ipca, idx, pX, pM)\n","        if (REDUCED_DIR / f\"reduced_{idx:04d}.npy\").exists() and (REDUCED_DIR / f\"reduced_meta_{idx:04d}.parquet\").exists():\n","            ready.append(idx)\n","    return sorted(ready)\n","\n","# -------------------- Reparaci√≥n de reducidos ‚Äúcrudos‚Äù --------------------\n","def maybe_repair_reduced(idx: int, nrows_hint: int, ncomp_hint: int) -> bool:\n","    \"\"\"\n","    Si reduced_XXXX.npy no tiene cabecera .npy (fue escrito 'crudo'),\n","    lo reescribe como .npy v√°lido. Devuelve True si repara o ya es v√°lido.\n","    \"\"\"\n","    p = REDUCED_DIR / f\"reduced_{idx:04d}.npy\"\n","    if not p.exists():\n","        return False\n","    # Intenta np.load primero\n","    try:\n","        _ = np.load(p, mmap_mode=\"r\")\n","        return True  # ya es v√°lido\n","    except Exception:\n","        pass\n","\n","    # Intentar abrir como memmap crudo con shape de hints\n","    try:\n","        raw = np.memmap(p, mode=\"r\", dtype=np.float32, shape=(nrows_hint, ncomp_hint))\n","        tmp = p.with_suffix(\".npy.fix\")\n","        fp = open_npy_memmap(tmp, shape=(nrows_hint, ncomp_hint), dtype=np.float32)\n","        fp[:] = np.array(raw, copy=False)\n","        del fp, raw\n","        os.replace(tmp, p)\n","        print(f\"üîß Reparado reduced {idx:04d} ‚Üí cabecera .npy escrita.\")\n","        return True\n","    except Exception as e:\n","        print(f\"[repair fail] reduced {idx:04d}: {e}\")\n","        return False\n","\n","# -------------------- LSH helpers --------------------\n","def sign_hash_blocks(X: np.ndarray, n_bits=128, seed=LSH_SEED):\n","    rng = np.random.RandomState(seed)\n","    proj = rng.randn(X.shape[1], n_bits).astype(np.float32)\n","    return (X @ proj > 0).astype(np.uint8)\n","\n","def lsh_buckets(signs: np.ndarray, bands=LSH_BANDS, rows=LSH_ROWS_PER_BAND) -> Dict[Tuple[int,int], List[int]]:\n","    assert bands * rows <= signs.shape[1], \"bands*rows excede la firma\"\n","    buckets = {}\n","    pos = 0\n","    for b in range(bands):\n","        slice_bits = signs[:, pos:pos+rows]\n","        if rows <= 60:\n","            vals = (slice_bits * (1 << np.arange(rows, dtype=np.uint64))).sum(axis=1).astype(np.uint64)\n","        else:\n","            vals = np.array([hash(tuple(r)) for r in slice_bits], dtype=np.int64)\n","        for i, v in enumerate(vals.tolist()):\n","            key = (b, int(v))\n","            buckets.setdefault(key, []).append(i)\n","        pos += rows\n","    return buckets\n","\n","def load_reduced_flex(idx: int, nrows: int, ncomp: int) -> np.ndarray:\n","    \"\"\"\n","    Carga reduced_XXXX.npy:\n","    - intenta np.load (cabecera v√°lida)\n","    - si falla, abre como memmap crudo con shape=(nrows, ncomp)\n","    \"\"\"\n","    p = REDUCED_DIR / f\"reduced_{idx:04d}.npy\"\n","    if not p.exists():\n","        raise FileNotFoundError(p)\n","    try:\n","        return np.load(p, mmap_mode=\"r\")\n","    except Exception:\n","        return np.memmap(p, mode=\"r\", dtype=np.float32, shape=(nrows, ncomp))\n","\n","def process_group(group_idx: int, shard_ids: List[int], ncomp: int):\n","    out_csv = REDUCED_DIR / f\"lsh_group_{group_idx:04d}_reps.csv\"\n","    if out_csv.exists():\n","        print(f\"‚Ü™Ô∏é Grupo {group_idx:04d} ya procesado. SKIP.\")\n","        return\n","\n","    X_list, meta_list = [], []\n","    total = 0\n","    for sid in shard_ids:\n","        pX = REDUCED_DIR / f\"reduced_{sid:04d}.npy\"\n","        pM = REDUCED_DIR / f\"reduced_meta_{sid:04d}.parquet\"\n","        if not (pX.exists() and pM.exists()):\n","            print(f\"[skip grupo] falta reducido/meta {sid:04d}\")\n","            continue\n","        try:\n","            meta = pd.read_parquet(pM)\n","            nrows = len(meta)\n","            if nrows == 0:\n","                continue\n","            # repara si hace falta\n","            maybe_repair_reduced(sid, nrows, ncomp)\n","            X = load_reduced_flex(sid, nrows, ncomp)\n","            n_eff = min(nrows, int(X.shape[0]))\n","            X_list.append(np.array(X[:n_eff], dtype=np.float32, copy=False))\n","            # Normaliza tipos en meta (id y _h como string)\n","            meta = meta.iloc[:n_eff].copy()\n","            if \"id\" in meta.columns: meta[\"id\"] = meta[\"id\"].astype(\"string\")\n","            if \"_h\" in meta.columns:  meta[\"_h\"] = meta[\"_h\"].astype(\"string\")\n","            meta_list.append(meta)\n","            total += n_eff\n","        except Exception as e:\n","            print(f\"[skip grupo] {sid:04d} no legible: {e}\")\n","\n","    if total == 0:\n","        print(f\"[skip grupo] {group_idx:04d} sin datos √∫tiles.\")\n","        return\n","\n","    X = np.vstack(X_list)\n","    meta = pd.concat(meta_list, ignore_index=True)\n","    assert X.shape[0] == len(meta)\n","\n","    signs = sign_hash_blocks(X, n_bits=max(LSH_BANDS*LSH_ROWS_PER_BAND, 128), seed=LSH_SEED)\n","    buckets = lsh_buckets(signs, bands=LSH_BANDS, rows=LSH_ROWS_PER_BAND)\n","\n","    seen = np.zeros(len(meta), dtype=bool)\n","    reps = []\n","    for _, idxs in buckets.items():\n","        for i in idxs:\n","            if not seen[i]:\n","                reps.append(i)\n","                for j in idxs:\n","                    seen[j] = True\n","                break\n","    reps = sorted(set(reps))\n","\n","    # Asegura columnas clave: (id,_h) + localizadores\n","    out = meta.iloc[reps].reset_index(drop=True).copy()\n","    if \"row_in_shard\" not in out.columns:\n","        out[\"row_in_shard\"] = out.index.astype(\"int64\")\n","    if \"shard_idx\" not in out.columns:\n","        # Si no existe, asigna el primer shard del grupo (mejor que nada)\n","        # Nota: es preferible que vengas de reduced_meta_X donde ya exista shard_idx\n","        out[\"shard_idx\"] = int(shard_ids[0])\n","    # tipos consistentes\n","    if \"id\" in out.columns: out[\"id\"] = out[\"id\"].astype(\"string\")\n","    if \"_h\" in out.columns: out[\"_h\"] = out[\"_h\"].astype(\"string\")\n","\n","    out.to_csv(out_csv, index=False)\n","    print(f\"‚úÖ Grupo {group_idx:04d}: {len(reps)}/{len(meta)} reps ‚Üí {out_csv.name}\")\n","\n","def run_lsh_groups(valid_ids: List[int], ncomp: int, group_size=GROUP_SIZE):\n","    ids = sorted(valid_ids)\n","    groups = [ids[i:i+group_size] for i in range(0, len(ids), group_size)]\n","    for gidx, g in enumerate(groups):\n","        process_group(gidx, g, ncomp)\n","\n","# --------- Reparaci√≥n de CSVs de grupos antiguos (opcional) ----------\n","def repair_group_csvs_if_needed():\n","    \"\"\"\n","    Si alg√∫n lsh_group_*_reps.csv existe sin (shard_idx,row_in_shard),\n","    intenta reconstruir cruzando con reduced_meta_XXXX.parquet por ('id','_h').\n","    Fuerza dtypes a string para evitar errores int64 vs object.\n","    \"\"\"\n","    csvs = sorted(REDUCED_DIR.glob(\"lsh_group_*_reps.csv\"))\n","    if not csvs:\n","        return\n","\n","    # Cargar metas y normalizar tipos\n","    metas = []\n","    for p in sorted(REDUCED_DIR.glob(\"reduced_meta_*.parquet\")):\n","        try:\n","            sid = int(re.findall(r'(\\d{4})', p.stem)[0])\n","            m = pd.read_parquet(p)\n","            if \"row_in_shard\" not in m.columns:\n","                m = m.reset_index(drop=True)\n","                m[\"row_in_shard\"] = np.arange(len(m), dtype=np.int64)\n","            m[\"shard_idx\"] = sid\n","            # normalizar llaves de cruce a string\n","            if \"id\" in m.columns:\n","                m[\"id\"] = m[\"id\"].astype(\"string\")\n","            if \"_h\" in m.columns:\n","                m[\"_h\"] = m[\"_h\"].astype(\"string\")\n","            metas.append(m[[\"id\",\"_h\",\"shard_idx\",\"row_in_shard\"]].copy() if {\"id\",\"_h\"}.issubset(m.columns) else None)\n","        except Exception:\n","            continue\n","    metas = [m for m in metas if m is not None]\n","    meta_all = pd.concat(metas, ignore_index=True) if metas else pd.DataFrame()\n","\n","    for p in csvs:\n","        try:\n","            df = pd.read_csv(p)\n","        except Exception:\n","            continue\n","        need_fix = not {\"shard_idx\",\"row_in_shard\"}.issubset(df.columns)\n","        if not need_fix:\n","            continue\n","        if meta_all.empty or not {\"id\",\"_h\"}.issubset(df.columns):\n","            print(f\"‚ö†Ô∏è No pude reparar {p.name}: faltan claves de cruce (id,_h) o metas vac√≠as.\")\n","            continue\n","\n","        # Forzar tipos a string antes del merge (evita error int64 vs object)\n","        df[\"id\"] = df[\"id\"].astype(\"string\")\n","        df[\"_h\"] = df[\"_h\"].astype(\"string\")\n","\n","        df2 = df.merge(meta_all, on=[\"id\",\"_h\"], how=\"left\")\n","        if {\"shard_idx\",\"row_in_shard\"}.issubset(df2.columns) and df2[\"shard_idx\"].notna().any():\n","            df2.to_csv(p, index=False)\n","            print(f\"üîß Reparado {p.name} a√±adiendo (shard_idx,row_in_shard).\")\n","        else:\n","            print(f\"‚ö†Ô∏è Reparaci√≥n fallida para {p.name}: no se pudieron inferir los localizadores.\")\n","\n","# --------- Consolidaci√≥n final con tipos consistentes ----------\n","def consolidate_groups_csv(out_path: Path):\n","    outs = sorted(REDUCED_DIR.glob(\"lsh_group_*_reps.csv\"))\n","    if not outs:\n","        print(\"No hay CSV de grupos para consolidar.\")\n","        return None\n","\n","    dfs = []\n","    for p in outs:\n","        dfp = pd.read_csv(\n","            p,\n","            dtype={\"id\": \"string\", \"_h\": \"string\", \"shard_idx\": \"Int64\", \"row_in_shard\": \"Int64\"},\n","            keep_default_na=False\n","        )\n","        dfs.append(dfp)\n","\n","    reps = pd.concat(dfs, ignore_index=True)\n","\n","    # Ordena columnas (si existen)\n","    pref = [c for c in [\"id\",\"make\",\"model\",\"year\",\"component\",\"_h\"] if c in reps.columns]\n","    tail = [c for c in [\"shard_idx\",\"row_in_shard\"] if c in reps.columns]\n","    cols = pref + [c for c in reps.columns if c not in pref + tail] + tail\n","    reps = reps[cols]\n","\n","    reps.to_csv(out_path, index=False)\n","    print(f\"üì¶ Consolidado: {len(reps):,} representantes ‚Üí {out_path}\")\n","    return reps\n","\n","# -------------------- MAIN --------------------\n","if __name__ == \"__main__\":\n","    ipca = load_ipca_npz(IPCA_PATH)\n","    ncomp = int(ipca.components_.shape[0])\n","\n","    print(\"== A) Transformaci√≥n IPCA ‚Üí reducidos (idempotente) ==\")\n","    ready_ids = ensure_reduced_all(ipca)\n","\n","    print(\"== B) LSH por lotes de 5 shards reducidos (reanuda por grupo) ==\")\n","    run_lsh_groups(ready_ids, ncomp)\n","\n","    # Intentar reparar grupos antiguos si hiciera falta (seguro con dtypes)\n","    repair_group_csvs_if_needed()\n","\n","    # Consolidado global asegurando columnas clave y tipos consistentes\n","    consolidate_groups_csv(REDUCED_DIR / \"representantes_global.csv\")\n"]},{"cell_type":"code","source":["# ===============================================\n","# GLOBAL DEDUP (memoria segura) con MiniBatchKMeans 2-pass\n","# - Paso 1: partial_fit sobre reduced_XXXX.npy (streaming)\n","# - Paso 2: asignaci√≥n y selecci√≥n del m√°s cercano por cl√∫ster\n","# - Salida: representantes_global_kmeans.csv\n","# ===============================================\n","import os, gc, re, pickle, math\n","from pathlib import Path\n","from typing import List, Tuple\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from sklearn.cluster import MiniBatchKMeans\n","\n","# -------------------- Config --------------------\n","BASE_DIR    = Path(\"/content/drive/MyDrive/NHTSA/embeddings/complaints_e5_mlg_instruct\")\n","REDUCED_DIR = BASE_DIR / \"reduced_shards\"\n","OUT_DIR     = REDUCED_DIR / \"kmeans_global\"\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# Tama√±os (ajusta seg√∫n tu RAM/objetivo)\n","K_TARGET      = 20000        # n¬∫ de cl√∫steres (‚âà n¬∫ de representantes finales)\n","BATCH_ROWS    = 100_000      # filas por bloque al entrenar/asignar\n","MB_SIZE       = 4096         # minibatch_size de MiniBatchKMeans\n","MAX_SHARDS    = None         # None = todos; o limita p.ej. 20 para pruebas\n","\n","MODEL_PATH    = OUT_DIR / \"mbkm.pkl\"\n","CENTERS_PATH  = OUT_DIR / \"centers.npy\"\n","REPS_CSV_PATH = OUT_DIR / \"representantes_global_kmeans.csv\"\n","\n","# Columnas ‚Äúmeta‚Äù esperadas en reduced_meta_XXXX.parquet (usa las que existan)\n","META_PREF = [\"id\",\"make\",\"model\",\"year\",\"component\",\"_h\"]\n","\n","# -------------------- Utilidades --------------------\n","def list_reduced_pairs(reduced_dir: Path) -> List[Tuple[int, Path, Path]]:\n","    npys  = sorted(reduced_dir.glob(\"reduced_*.npy\"))\n","    metas = sorted(reduced_dir.glob(\"reduced_meta_*.parquet\"))\n","    idx_npy  = {int(re.findall(r'(\\d{4})', p.stem)[0]): p for p in npys  if re.findall(r'(\\d{4})', p.stem)}\n","    idx_meta = {int(re.findall(r'(\\d{4})', p.stem)[0]): p for p in metas if re.findall(r'(\\d{4})', p.stem)}\n","    ids = sorted(set(idx_npy) & set(idx_meta))\n","    pairs = [(i, idx_npy[i], idx_meta[i]) for i in ids]\n","    return pairs\n","\n","def iterate_blocks(X_mem, start=0, stop=None, block=BATCH_ROWS):\n","    n = X_mem.shape[0]\n","    a = start\n","    b = n if stop is None else min(stop, n)\n","    while a < b:\n","        c = min(a + block, b)\n","        yield a, c\n","        a = c\n","\n","def ensure_float32(arr):\n","    return np.asarray(arr, dtype=np.float32, order=\"C\")\n","\n","# -------------------- Paso 1: FIT (partial_fit) --------------------\n","pairs = list_reduced_pairs(REDUCED_DIR)\n","if MAX_SHARDS is not None:\n","    pairs = pairs[:MAX_SHARDS]\n","\n","if not pairs:\n","    raise RuntimeError(f\"No se encontraron reducidos en {REDUCED_DIR}\")\n","\n","# Inferir dimensi√≥n d\n","first_npy = np.load(pairs[0][1], mmap_mode=\"r\")\n","d = int(first_npy.shape[1])\n","del first_npy\n","\n","if MODEL_PATH.exists() and CENTERS_PATH.exists():\n","    # Reusar modelo y centroides si ya existen (idempotente)\n","    with open(MODEL_PATH, \"rb\") as f:\n","        mbkm = pickle.load(f)\n","    centers = np.load(CENTERS_PATH)\n","    print(f\"‚Ü™Ô∏é Modelo MBKM reutilizado: K={mbkm.n_clusters}, d={d}\")\n","else:\n","    print(f\"== Paso 1/2: Entrenando MiniBatchKMeans (streaming) ==\")\n","    mbkm = MiniBatchKMeans(\n","        n_clusters=K_TARGET,\n","        init=\"k-means++\",\n","        max_iter=100,          # iters internas de cada partial_fit\n","        batch_size=MB_SIZE,\n","        reassignment_ratio=0.01,\n","        random_state=42,\n","        verbose=0\n","    )\n","    # Entrenamiento incremental\n","    total_rows = 0\n","    for sid, pX, pM in pairs:\n","        try:\n","            X_mem = np.load(pX, mmap_mode=\"r\")\n","            n = X_mem.shape[0]\n","            if n == 0:\n","                continue\n","            for a, c in tqdm(iterate_blocks(X_mem, 0, n), desc=f\"FIT {sid:04d}\", leave=False):\n","                batch = ensure_float32(X_mem[a:c])\n","                mbkm.partial_fit(batch)\n","                total_rows += (c - a)\n","                del batch\n","            del X_mem\n","            gc.collect()\n","        except Exception as e:\n","            print(f\"[skip] {sid:04d} durante fit: {e}\")\n","\n","    centers = mbkm.cluster_centers_.astype(np.float32, copy=False)\n","    with open(MODEL_PATH, \"wb\") as f:\n","        pickle.dump(mbkm, f)\n","    np.save(CENTERS_PATH, centers)\n","    print(f\"‚úÖ Fit completo sobre ~{total_rows:,} filas. Guardado modelo+centros.\")\n","\n","# -------------------- Paso 2: ASSIGN + REPRESENTANTES --------------------\n","# Estructuras para guardar el mejor (m√≠nima distancia) por cl√∫ster\n","best_dist   = np.full((K_TARGET,), np.inf, dtype=np.float32)\n","best_record = [None] * K_TARGET   # guardar√° dict con meta + localizadores\n","\n","def update_best(cluster_idx: int, dist: np.float32, meta_row: dict):\n","    if dist < best_dist[cluster_idx]:\n","        best_dist[cluster_idx] = float(dist)\n","        best_record[cluster_idx] = meta_row\n","\n","print(\"== Paso 2/2: Asignando y seleccionando representantes ==\")\n","for sid, pX, pM in pairs:\n","    # cargar meta (solo columnas disponibles)\n","    try:\n","        meta = pd.read_parquet(pM)\n","    except Exception as e:\n","        print(f\"[skip] {sid:04d} meta ilegible: {e}\")\n","        continue\n","    keep_cols = [c for c in META_PREF if c in meta.columns]\n","    meta = meta[keep_cols].copy()\n","    # √≠ndice local por si no existe\n","    meta = meta.reset_index(drop=True)\n","    meta[\"row_in_shard\"] = np.arange(len(meta), dtype=np.int64)\n","    meta[\"shard_idx\"] = sid\n","\n","    try:\n","        X_mem = np.load(pX, mmap_mode=\"r\")\n","        n = X_mem.shape[0]\n","        if n == 0:\n","            continue\n","        for a, c in tqdm(iterate_blocks(X_mem, 0, n), desc=f\"ASSIGN {sid:04d}\", leave=False):\n","            Xb = ensure_float32(X_mem[a:c])       # [B,d]\n","            # predicci√≥n + distancias cuadr√°ticas a centros\n","            labels = mbkm.predict(Xb)              # [B]\n","            # usa score_samples o euclidean dist manual (m√°s control)\n","            # d^2 = ||x - mu||^2 = ||x||^2 + ||mu||^2 - 2 x¬∑mu\n","            # computamos para cada punto solo su centro asignado:\n","            mu = centers[labels]                   # [B,d]\n","            d2 = np.einsum(\"ij,ij->i\", Xb - mu, Xb - mu, optimize=True)  # [B]\n","            # actualizar mejores por cl√∫ster\n","            for i in range(len(labels)):\n","                clu = int(labels[i])\n","                dist = float(d2[i])\n","                mr = {k: (None if k not in meta.columns else meta.iloc[a+i][k]) for k in meta.columns}\n","                mr[\"cluster\"] = clu\n","                mr[\"dist2\"]   = dist\n","                update_best(clu, dist, mr)\n","            del Xb, mu, labels, d2\n","        del X_mem\n","        gc.collect()\n","    except Exception as e:\n","        print(f\"[skip] {sid:04d} durante assign: {e}\")\n","\n","# Construir DataFrame de representantes (uno por cl√∫ster con dato)\n","reps = [r for r in best_record if r is not None]\n","if not reps:\n","    raise RuntimeError(\"No se pudo seleccionar ning√∫n representante. Revisa logs.\")\n","\n","reps_df = pd.DataFrame(reps)\n","\n","# Ordenar por distancia (opcional) y quitar NaN\n","reps_df = reps_df.sort_values([\"cluster\",\"dist2\"]).reset_index(drop=True)\n","\n","# Reordenar columnas\n","front = [c for c in [\"id\",\"make\",\"model\",\"year\",\"component\",\"_h\"] if c in reps_df.columns]\n","tail  = [c for c in [\"shard_idx\",\"row_in_shard\",\"cluster\",\"dist2\"] if c in reps_df.columns]\n","cols  = front + [c for c in reps_df.columns if c not in front + tail] + tail\n","reps_df = reps_df[cols]\n","\n","# Guardar\n","reps_df.to_csv(REPS_CSV_PATH, index=False)\n","print(f\"üì¶ Representantes KMeans: {len(reps_df):,} ‚Üí {REPS_CSV_PATH}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_02MltLCI1C","executionInfo":{"status":"ok","timestamp":1760307844154,"user_tz":360,"elapsed":1861356,"user":{"displayName":"Lucero Guadalupe Contreras Hern√°ndez","userId":"07832692706572343340"}},"outputId":"e6cdc6f6-5a8b-4713-fcc0-36c8c61410d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["== Paso 1/2: Entrenando MiniBatchKMeans (streaming) ==\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["‚úÖ Fit completo sobre ~525,000 filas. Guardado modelo+centros.\n","== Paso 2/2: Asignando y seleccionando representantes ==\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["üì¶ Representantes KMeans: 20,000 ‚Üí /content/drive/MyDrive/NHTSA/embeddings/complaints_e5_mlg_instruct/reduced_shards/kmeans_global/representantes_global_kmeans.csv\n"]}]},{"cell_type":"code","source":["!pip install neo4j"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QC5rPXicN92C","executionInfo":{"status":"ok","timestamp":1760320065203,"user_tz":360,"elapsed":19780,"user":{"displayName":"Lucero Guadalupe Contreras Hern√°ndez","userId":"07832692706572343340"}},"outputId":"756eaf88-7696-4c63-98e9-3a24ef1e42c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting neo4j\n","  Downloading neo4j-6.0.2-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from neo4j) (2025.2)\n","Downloading neo4j-6.0.2-py3-none-any.whl (325 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m325.8/325.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: neo4j\n","Successfully installed neo4j-6.0.2\n"]}]},{"cell_type":"code","source":["from os import getenv\n","NEO4J_URI = getenv(\"NEO4J_URI\", \"\")\n","NEO4J_USER = getenv(\"NEO4J_USER\", \"\")\n","NEO4J_PASSWORD = getenv(\"NEO4J_PASSWORD\", \"\")"],"metadata":{"id":"SCkoamHQQQol"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# Complaints ‚Üí Neo4j (upsert por lotes, con normalizaci√≥n)\n","#  - Usa UUID v5 (qid) con el MISMO namespace que Qdrant\n","#  - Mantiene unicidad SOLO en Complaint.qid\n","#  - Make/Model/Component: √≠ndices (no √∫nicos) + nombres normalizados\n","#  - Evita crear m√°s duplicados; permite limpiar luego y poner unique\n","# ============================================================\n","\n","import os\n","import uuid\n","import math\n","import pandas as pd\n","from neo4j import GraphDatabase\n","\n","# -------------------- CONFIG --------------------\n","\n","# CSV de representantes (ajusta si usas el consolidado)\n","CSV_REPS = \"/content/drive/MyDrive/NHTSA/embeddings/complaints_e5_mlg_instruct/reduced_shards/kmeans_global/representantes_global_kmeans.csv\"\n","# Alternativa:\n","# CSV_REPS = \"/content/drive/MyDrive/NHTSA/embeddings/complaints_e5_mlg_instruct/reduced_shards/representantes_global.csv\"\n","\n","# ‚ö†Ô∏è IMPORTANT√çSIMO:\n","# Debe ser el MISMO namespace con el que generaste los UUIDs al subir a Qdrant.\n","# Si fue \"https://nhtsa.example/complaints\", deja esa cadena. Si fue otro, c√°mbialo aqu√≠.\n","NAMESPACE_STR = \"https://nhtsa.example/complaints\"\n","NAMESPACE = uuid.uuid5(uuid.NAMESPACE_URL, NAMESPACE_STR)\n","\n","print(\"Namespace (string):\", NAMESPACE_STR)\n","print(\"Namespace (UUID)  :\", NAMESPACE)\n","\n","# Variables de entorno (si no las tienes ya definidas en Colab)\n","# Sustituye <<<...>>> por tus valores reales, o comenta estas l√≠neas si ya existen en tu entorno.\n","os.environ.setdefault(\"NEO4J_URI\", \"neo4j+s://<<<TU_HOST_DE_AURA>>>.databases.neo4j.io\")\n","os.environ.setdefault(\"NEO4J_USER\", \"neo4j\")\n","os.environ.setdefault(\"NEO4J_PASSWORD\", \"<<<TU_PASSWORD_AQUI>>>\")\n","\n","# -------------------- UTILIDADES --------------------\n","\n","def make_qid(shard_idx: int, row_in_shard: int) -> str:\n","    \"\"\"UUID v5 determin√≠stico usando el namespace acordado.\"\"\"\n","    raw = f\"rep_{int(shard_idx):04d}_{int(row_in_shard):06d}\"\n","    return str(uuid.uuid5(NAMESPACE, raw))\n","\n","def _norm(s):\n","    \"\"\"Normaliza nombre (clave) para evitar duplicados: trim + upper.\"\"\"\n","    if s is None:\n","        return None\n","    s = str(s).strip()\n","    return s.upper() if s != \"\" else None\n","\n","def _mask(s, keep=4):\n","    if not s: return \"<empty>\"\n","    return s[:keep] + \"‚Ä¶\" if len(s) > keep else \"****\"\n","\n","# -------------------- CARGA CSV --------------------\n","\n","# Lee CSV\n","df = pd.read_csv(CSV_REPS, dtype={\"id\":\"string\",\"_h\":\"string\"})\n","\n","# Validaciones m√≠nimas\n","required_cols = {\"shard_idx\",\"row_in_shard\"}\n","missing = [c for c in required_cols if c not in df.columns]\n","if missing:\n","    raise ValueError(f\"CSV no contiene columnas requeridas: {missing}. Repara antes de cargar a Neo4j.\")\n","\n","# Normaliza tipos y crea qid\n","df[\"shard_idx\"]    = pd.to_numeric(df[\"shard_idx\"], errors=\"coerce\").astype(\"Int64\")\n","df[\"row_in_shard\"] = pd.to_numeric(df[\"row_in_shard\"], errors=\"coerce\").astype(\"Int64\")\n","df = df[df[\"shard_idx\"].notna() & df[\"row_in_shard\"].notna()].copy()\n","\n","df[\"qid\"] = [make_qid(int(s), int(r)) for s, r in zip(df[\"shard_idx\"], df[\"row_in_shard\"])]\n","\n","# comp_id opcional desde 'id' si existe\n","df[\"comp_id\"] = df[\"id\"] if \"id\" in df.columns else None\n","\n","# Normaliza claves de texto para evitar duplicados nuevos en el grafo\n","for col in (\"make\",\"model\",\"component\"):\n","    if col in df.columns:\n","        df[col] = df[col].apply(_norm)\n","\n","# -------------------- NEO4J DRIVER --------------------\n","\n","NEO4J_URI = os.getenv(\"NEO4J_URI\", \"\")\n","NEO4J_USER = os.getenv(\"NEO4J_USER\", \"\")\n","NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"\")\n","\n","print(\"Neo4j URI :\", NEO4J_URI or \"<empty>\")\n","print(\"Neo4j USER:\", NEO4J_USER or \"<empty>\")\n","print(\"Neo4j PASS:\", _mask(NEO4J_PASSWORD))\n","\n","assert NEO4J_URI.startswith((\"neo4j://\",\"neo4j+s://\",\"neo4j+ssc://\",\"bolt://\",\"bolt+s://\",\"bolt+ssc://\")), \\\n","    \"NEO4J_URI debe comenzar con neo4j://, neo4j+s://, bolt://, etc.\"\n","assert NEO4J_USER and NEO4J_PASSWORD, \"Faltan NEO4J_USER o NEO4J_PASSWORD.\"\n","\n","driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n","\n","# -------------------- DDL (constraints / √≠ndices) --------------------\n","# Nota: Unicidad SOLO en Complaint.qid (no romper√° por duplicados previos)\n","#       En Make/Model/Component dejamos √≠ndices NO √∫nicos para permitir carga,\n","#       ya que reportaste duplicados existentes.\n","\n","DDL = [\n","    \"CREATE CONSTRAINT complaint_qid IF NOT EXISTS FOR (c:Complaint) REQUIRE c.qid IS UNIQUE\",\n","\n","    \"CREATE INDEX make_name_idx IF NOT EXISTS FOR (m:Make) ON (m.name)\",\n","    \"CREATE INDEX model_name_idx IF NOT EXISTS FOR (m:Model) ON (m.name)\",\n","    \"CREATE INDEX component_name_idx IF NOT EXISTS FOR (x:Component) ON (x.name)\",\n","\n","    \"CREATE INDEX complaint_make IF NOT EXISTS FOR (c:Complaint) ON (c.make)\",\n","    \"CREATE INDEX complaint_model IF NOT EXISTS FOR (c:Complaint) ON (c.model)\"\n","]\n","\n","with driver.session() as s:\n","    for stmt in DDL:\n","        s.run(stmt)\n","\n","# -------------------- UPSERT (Cypher) --------------------\n","# Importante: usamos name normalizado (upper/trim) ya desde el DataFrame.\n","# Si prefieres normalizar en Cypher, podr√≠as usar toUpper(trim(r.make)) etc.\n","\n","CYPHER = \"\"\"\n","UNWIND $rows AS r\n","MERGE (c:Complaint {qid: r.qid})\n","  ON CREATE SET\n","    c.comp_id      = r.comp_id,\n","    c._h           = r._h,\n","    c.shard_idx    = r.shard_idx,\n","    c.row_in_shard = r.row_in_shard,\n","    c.cluster      = r.cluster,\n","    c.dist2        = r.dist2,\n","    c.make         = r.make,\n","    c.model        = r.model,\n","    c.year         = r.year,\n","    c.component    = r.component\n","  ON MATCH SET\n","    c.comp_id      = coalesce(r.comp_id, c.comp_id),\n","    c._h           = coalesce(r._h, c._h),\n","    c.shard_idx    = coalesce(r.shard_idx, c.shard_idx),\n","    c.row_in_shard = coalesce(r.row_in_shard, c.row_in_shard),\n","    c.cluster      = coalesce(r.cluster, c.cluster),\n","    c.dist2        = coalesce(r.dist2, c.dist2),\n","    c.make         = coalesce(r.make, c.make),\n","    c.model        = coalesce(r.model, c.model),\n","    c.year         = coalesce(r.year, c.year),\n","    c.component    = coalesce(r.component, c.component)\n","\n","FOREACH (_ IN CASE WHEN r.make IS NOT NULL THEN [1] ELSE [] END |\n","  MERGE (m:Make {name: r.make})\n","  MERGE (c)-[:OF_MAKE]->(m)\n",")\n","\n","FOREACH (_ IN CASE WHEN r.model IS NOT NULL THEN [1] ELSE [] END |\n","  MERGE (md:Model {name: r.model})\n","  MERGE (c)-[:OF_MODEL]->(md)\n",")\n","\n","FOREACH (_ IN CASE WHEN r.component IS NOT NULL THEN [1] ELSE [] END |\n","  MERGE (x:Component {name: r.component})\n","  MERGE (c)-[:MENTIONS]->(x)\n",")\n","\"\"\"\n","\n","def to_row(rec: dict) -> dict:\n","    \"\"\"Devuelve solo las claves esperadas en Cypher, con tipos adecuados.\"\"\"\n","    return {\n","        \"qid\":          rec.get(\"qid\"),\n","        \"comp_id\":      rec.get(\"comp_id\"),\n","        \"_h\":           rec.get(\"_h\"),\n","        \"shard_idx\":    int(rec[\"shard_idx\"])     if pd.notna(rec.get(\"shard_idx\"))     else None,\n","        \"row_in_shard\": int(rec[\"row_in_shard\"])  if pd.notna(rec.get(\"row_in_shard\"))  else None,\n","        \"cluster\":      int(rec[\"cluster\"])       if pd.notna(rec.get(\"cluster\"))       else None,\n","        \"dist2\":        float(rec[\"dist2\"])       if pd.notna(rec.get(\"dist2\"))         else None,\n","        \"make\":         rec.get(\"make\"),          # ya normalizado (upper/trim) en DF\n","        \"model\":        rec.get(\"model\"),\n","        \"year\":         int(rec[\"year\"])          if pd.notna(rec.get(\"year\"))          else None,\n","        \"component\":    rec.get(\"component\"),\n","    }\n","\n","rows = [to_row(r) for r in df.to_dict(orient=\"records\")]\n","\n","BATCH = 5_000\n","with driver.session() as session:\n","    for i in range(0, len(rows), BATCH):\n","        chunk = rows[i:i+BATCH]\n","        session.run(CYPHER, rows=chunk)\n","        print(f\"Upsert Neo4j: {i+len(chunk)}/{len(rows)}\")\n","\n","print(\"‚úÖ Complaints cargados/actualizados en Neo4j.\")\n","\n","# -------------------- (OPCIONAL) Limpieza de duplicados existentes --------------------\n","# Si m√°s adelante quieres poner UNICIDAD en Make/Model/Component,\n","# primero debes consolidar duplicados. Con APOC es muy sencillo:\n","#\n","# // Detecta duplicados:\n","# MATCH (m:Model)\n","# WITH toUpper(trim(m.name)) AS k, collect(m) AS nodes\n","# WHERE k IS NOT NULL AND size(nodes) > 1\n","# RETURN k, size(nodes) AS cnt\n","# ORDER BY cnt DESC LIMIT 50;\n","#\n","# // Consolida (requiere APOC):\n","# MATCH (m:Model)\n","# WITH toUpper(trim(m.name)) AS k, collect(m) AS nodes\n","# WHERE k IS NOT NULL AND size(nodes) > 1\n","# CALL apoc.refactor.mergeNodes(nodes, {properties:'discard', mergeRels:true}) YIELD node\n","# RETURN node;\n","#\n","# // Luego s√≠ puedes crear la restricci√≥n √∫nica:\n","# CREATE CONSTRAINT model_name IF NOT EXISTS FOR (m:Model) REQUIRE m.name IS UNIQUE;\n","# (Repite para Make y Component)\n"],"metadata":{"id":"dZ9h_vctV9aT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760320212381,"user_tz":360,"elapsed":11552,"user":{"displayName":"Lucero Guadalupe Contreras Hern√°ndez","userId":"07832692706572343340"}},"outputId":"ebdf0139-666c-47cc-c4e6-a9a1da75a435"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace (string): https://nhtsa.example/complaints\n","Namespace (UUID)  : 82cc465c-bfae-5901-868b-5e87923a97f9\n","Neo4j URI : neo4j+s://66024f48.databases.neo4j.io\n","Neo4j USER: neo4j\n","Neo4j PASS: kDp5‚Ä¶\n","Upsert Neo4j: 5000/20000\n","Upsert Neo4j: 10000/20000\n","Upsert Neo4j: 15000/20000\n","Upsert Neo4j: 20000/20000\n","‚úÖ Complaints cargados/actualizados en Neo4j.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"JfV-WMy0N3c3"},"execution_count":null,"outputs":[]}]}