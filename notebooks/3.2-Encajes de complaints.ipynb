{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Encaje Semántico de *Complaints* (NHTSA) con **intfloat/multilingual-e5-large-instruct**\n","\n","## 1. Objetivo\n","\n","Se realizaron **embeddings semánticos** del corpus de *Complaints* de NHTSA para habilitar búsqueda semántica, deduplicación analítica y vinculación posterior en un **grafo de conocimiento**. Se priorizó un pipeline **reproducible y eficiente** en memoria/tiempo, robusto a interrupciones.\n","\n","---\n","\n","## 2. Datos y alcance\n","\n","* **Fuente** (normalizada previamente):\n","\n","  * Parquet: `/content/drive/MyDrive/NHTSA/processed/complaints.parquet`\n","  * JSONL (alternativo): `/content/drive/MyDrive/NHTSA/processed/complaints_corpus.jsonl`\n","* **Texto base**: campo **`CDESCR`** (narrativa de la queja).\n","* **Metadatos**: `CMPLID`/`ODINO` (ID), `MAKETXT`, `MODELTXT`, `YEARTXT`, `COMPDESC`, etc.\n","\n","**Nota metodológica.** En esta etapa se optó por **no aplicar chunking** artificial: cada fila (queja) se representa con **un único embedding** del texto narrativo completo, favoreciendo la coherencia a nivel de caso. La opción de chunking queda reservada para tareas donde el *recall* dependa de pasajes cortos.\n","\n","---\n","\n","## 3. Modelo y configuración\n","\n","* **Modelo**: `intfloat/multilingual-e5-large-instruct` (1024 dims).\n","* **Formato instruct**: prefijo **`passage: `** al texto (estilo E5-instruct).\n","* **Normalización**: vectores **L2-normalizados** (adecuados para similitud coseno).\n","* **Dispositivo**: `cuda` si disponible (con **TF32** habilitado); *fallback* CPU.\n","* **Parámetros de rendimiento**:\n","\n","  * `max_seq_length = 256` (menor *padding* ⇒ mayor throughput).\n","  * `batch_size = 1024` (ajustable según VRAM).\n","  * Escritura **streaming** a disco (memmap) por **shards** reanudables.\n","\n","---\n","\n","## 4. Pipeline de encaje (resumen técnico)\n","\n","1. **Selección de origen**: Parquet (preferente) o JSONL.\n","2. **Lectura *streaming*** en lotes (≈200k filas por tirón) para **evitar materializar** el corpus completo en RAM.\n","3. **Preparación del texto**:\n","\n","   * Normalización de columnas mínimas (`ID`, `TEXT`).\n","   * Prefijo `passage: ` para E5-instruct (sin copiar toda la columna en RAM).\n","4. **(Opcional) deduplicación por batch** mediante *hash* (`hash_pandas_object`).\n","\n","   * En *Complaints* se dejó **desactivada** (*DEDUP_TEXTS = False*), dada la menor redundancia observada y para preservar matices por incidente.\n","5. **Encaje por shards** (p.ej., 25 000 filas/shard):\n","\n","   * *Batching* en GPU/CPU.\n","   * Escritura directa a **memmap `.npy`** (sin acumular todo el shard en RAM).\n","   * Guardado de **metadatos alineados** (`meta_shard_XXXX.parquet`).\n","6. **Manifest de ejecución** (`manifest.json`):\n","\n","   * Registro de shards, filas, dimensión, *flags* (normalize, fp16), y estado (**reanudable** ante interrupciones).\n","\n","---\n","\n","## 5. Gestión de recursos y robustez\n","\n","* **RAM host** contenida**:** lectura por lotes + memmap de salida.\n","* **VRAM** aprovechada**:** `batch_size` alto y `max_seq_length` reducido (menos *padding*).\n","* **Reanudación** ante cortes**:** cada shard se cierra atómicamente y queda inventariado en `manifest.json`.\n","* **Opcional FP16 en disco**: `USE_FP16_STORAGE=True` reduce ≈50 % el tamaño de los `.npy` con pérdida marginal en *recall*.\n","\n","---\n","\n","## 6. Artefactos generados\n","\n","Directorio de salida:\n","`/content/drive/MyDrive/NHTSA/embeddings/complaints_e5_mlg_instruct/`\n","\n","* `embeddings_shard_0000.npy`, `embeddings_shard_0001.npy`, … (vectores 1024-D).\n","* `meta_shard_0000.parquet`, `meta_shard_0001.parquet`, … (ID y metadatos alineados).\n","* `manifest.json` (descripción integral para reanudar e indexar).\n","\n","---\n","\n","## 7. Controles de calidad\n","\n","Se aplicaron verificaciones automáticas por shard:\n","\n","* **Dimensión** consistente (=1024).\n","* **Cardinalidad**: filas de embeddings == filas de metadatos.\n","* **Normalización**: norma L2 ≈ 1.0 (muestreo).\n","* **Integridad**: `id` no nulo; proporción de textos vacíos ≈ 0 % (previa limpieza).\n","* **Trazabilidad**: cada shard queda registrado en el *manifest* con estado `complete`.\n","\n","---\n","\n","## 8. Justificación metodológica\n","\n","* **Sin chunking**: preserva contexto de incidente y reduce costo total de inferencia; apropiado cuando las preguntas operativas se refieren al **caso completo** (e.g., co-ocurrencias entre narrativa y metadatos).\n","* **E5-instruct**: alineado con recuperación semántica multilingüe y buen equilibrio **calidad/latencia/almacenamiento** observado en evaluaciones previas.\n","* **Streaming + shards**: requisito práctico dado el volumen (≈millones de registros) y limitaciones de RAM y tiempo de sesión.\n","\n","---\n","\n","## 9. Limitaciones y mitigaciones\n","\n","* **Textos muy largos**: truncados a `max_seq_length=256`; si emergen consultas sensibles a contexto distal, elevar a 384/512 en una **re-codificación focalizada**.\n","* **Deduplicación desactivada**: conserva granularidad; si se detectan redundancias temáticas, se puede activar por lotes o aplicar **agregación por similitud** posterior (clustering).\n","* **Variabilidad idiomática**: E5 es multilingüe; sin embargo, para dominios mixtos EN/ES/tech, la cobertura es adecuada pero no perfecta. Mitigar con **re-ranking** de pasajes relevantes si se integran consultas complejas.\n","\n","---\n","\n","## 10. Próximos pasos\n","\n","1. **Indexación** (FAISS/HNSW) por shards ⇒ índice global (coseno/IP).\n","2. **Evaluación R2R/Q2R** sobre *Complaints* (métrica: Recall@k, nDCG@k) con *queries* canónicas.\n","3. **Integración** al **grafo**: unificación por `VIN`, `MMY`, `COMPONENT`, *campaign* y *investigations*.\n","4. **RAG/Agente**: *retrieval* híbrido (BM25 + vectores) + *re-ranker* ligero para respuestas explicables.\n","\n","---\n","\n","## 11. Reproducibilidad (resumen de parámetros)\n","\n","* `model = \"intfloat/multilingual-e5-large-instruct\"`\n","* `max_seq_length = 256`, `batch_size = 1024`, `normalize = True`\n","* `STREAM_READ_ROWS = 200_000`, `SHARD_ROWS = 25_000`\n","* `USE_FP16_STORAGE = False` (opcionalmente `True`)\n","* Salida: `…/complaints_e5_mlg_instruct/{embeddings_shard_*.npy, meta_shard_*.parquet, manifest.json}`\n","\n","---\n"],"metadata":{"id":"Jrc-OjewyrO5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJMHmE28-p3c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760254486319,"user_tz":360,"elapsed":31853,"user":{"displayName":"Lucero Guadalupe Contreras Hernandez","userId":"06647893651071570506"}},"outputId":"86a4ccdc-0a3b-4c6b-8fcc-a321580b407f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDevice: cuda | Batch size: 64\n","Entradas posibles:\n"," - /content/drive/MyDrive/NHTSA/processed/complaints.parquet | existe: True\n"," - /content/drive/MyDrive/NHTSA/processed/complaints_corpus.jsonl | existe: True\n","Salidas -> /content/drive/MyDrive/NHTSA/embeddings/complaints_e5_mlg_instruct\n"]}],"source":["# ── Montaje de Drive  ─────────────────────────────────────────────\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ── Dependencias ──────────────────────────────────────────────────────────────\n","!pip -q install sentence-transformers==3.0.1\n","\n","# ── Librerías ─────────────────────────────────────────────────────────────────\n","import os, re, json, math, hashlib, textwrap, gc, time\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","\n","# ── Detección de dispositivo ─────────────────────────────────────────────────\n","import torch\n","if torch.cuda.is_available():\n","    DEVICE = \"cuda\"\n","else:\n","    try:\n","        DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","    except Exception:\n","        DEVICE = \"cpu\"\n","\n","MODEL_NAME = \"intfloat/multilingual-e5-large-instruct\"   # mismo que elegimos\n","NORMALIZE  = True\n","\n","# Batches sugeridos (ajusta si tu GPU es T4/V100/A100/CPU)\n","BATCH_SIZE = 64 if DEVICE in (\"cuda\",\"mps\") else 16\n","\n","# ── Rutas I/O ─────────────────────────────────────────────────────────────────\n","BASE_IN   = Path(\"/content/drive/MyDrive/NHTSA/processed\")\n","PARQ_IN   = BASE_IN / \"complaints.parquet\"\n","JSONL_IN  = BASE_IN / \"complaints_corpus.jsonl\"  # alternativo\n","\n","BASE_OUT  = Path(\"/content/drive/MyDrive/NHTSA/embeddings/complaints_e5_mlg_instruct\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","VEC_NPY   = BASE_OUT / \"complaints_embeddings.npy\"\n","META_OUT  = BASE_OUT / \"complaints_chunks_meta.parquet\"\n","MAP_CSV   = BASE_OUT / \"id_map.csv\"\n","CONF_JSON = BASE_OUT / \"config.json\"\n","\n","print(f\"Device: {DEVICE} | Batch size: {BATCH_SIZE}\")\n","print(\"Entradas posibles:\")\n","print(\" -\", PARQ_IN,  \"| existe:\", PARQ_IN.exists())\n","print(\" -\", JSONL_IN, \"| existe:\", JSONL_IN.exists())\n","print(\"Salidas ->\", BASE_OUT)\n"]},{"cell_type":"code","source":["# ──────────────────────────────────────────────────────────────────────────────\n","# CE L D A 1 · Cargar corpus de Complaints y generar chunks (~900 caracteres)\n","# ──────────────────────────────────────────────────────────────────────────────\n","import pandas as pd, numpy as np, json, textwrap\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","PARQ_IN  = Path(\"/content/drive/MyDrive/NHTSA/processed/complaints.parquet\")\n","JSONL_IN = Path(\"/content/drive/MyDrive/NHTSA/processed/complaints_corpus.jsonl\")\n","\n","def normalize_year(x):\n","    try:\n","        v = int(str(x))\n","        if 1949 <= v <= 2035:\n","            return v\n","    except:\n","        pass\n","    return np.nan\n","\n","def first_nonnull(d, keys, default=None):\n","    for k in keys:\n","        if k in d and pd.notna(d[k]) and str(d[k]).strip() != \"\":\n","            return d[k]\n","    return default\n","\n","def hard_wrap(text: str, width: int = 900) -> list[str]:\n","    \"\"\"Divide texto en fragmentos ≤ width respetando espacios.\"\"\"\n","    if not isinstance(text, str) or not text.strip():\n","        return []\n","    return textwrap.wrap(text.strip(), width=width, break_long_words=False, break_on_hyphens=False)\n","\n","def load_complaints_corpus(parquet_path: Path, jsonl_path: Path) -> pd.DataFrame:\n","    \"\"\"Devuelve DataFrame con columnas mínimas id, text (+ metadatos si existen).\"\"\"\n","    if parquet_path.exists():\n","        df = pd.read_parquet(parquet_path)\n","        cols = [c.upper() for c in df.columns]\n","        df.columns = cols\n","\n","        id_col   = \"CMPLID\" if \"CMPLID\" in cols else \"ODINO\" if \"ODINO\" in cols else None\n","        text_col = \"CDESCR\" if \"CDESCR\" in cols else None\n","        assert id_col and text_col, \"No se encontraron columnas mínimas CMPLID/ODINO y CDESCR\"\n","\n","        base = pd.DataFrame({\n","            \"id\": df[id_col].astype(str),\n","            \"text\": df[text_col].astype(str)\n","        })\n","        if \"MAKETXT\" in cols:   base[\"make\"]      = df[\"MAKETXT\"].astype(str)\n","        if \"MODELTXT\" in cols:  base[\"model\"]     = df[\"MODELTXT\"].astype(str)\n","        if \"YEARTXT\" in cols:   base[\"year\"]      = df[\"YEARTXT\"].map(normalize_year)\n","        if \"COMPDESC\" in cols:  base[\"component\"] = df[\"COMPDESC\"].astype(str)\n","        return base\n","\n","    elif jsonl_path.exists():\n","        rows = []\n","        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n","            for line in f:\n","                try:\n","                    obj = json.loads(line)\n","                except json.JSONDecodeError:\n","                    continue\n","                _id  = obj.get(\"id\")\n","                _txt = obj.get(\"text\")\n","                meta = obj.get(\"metadata\", {}) or {}\n","                if not _id or not _txt:\n","                    continue\n","                rows.append({\n","                    \"id\": str(_id),\n","                    \"text\": str(_txt),\n","                    \"make\": first_nonnull(meta, [\"MAKETXT\",\"make\"]),\n","                    \"model\": first_nonnull(meta, [\"MODELTXT\",\"model\"]),\n","                    \"year\": normalize_year(first_nonnull(meta, [\"YEARTXT\",\"year\"])),\n","                    \"component\": first_nonnull(meta, [\"COMPDESC\",\"component\"])\n","                })\n","        return pd.DataFrame(rows)\n","    else:\n","        raise FileNotFoundError(\"No se encontraron complaints.parquet ni complaints_corpus.jsonl\")\n","\n","def make_chunks(df_base: pd.DataFrame, wrap_width: int = 900) -> pd.DataFrame:\n","    \"\"\"Crea chunks textuales ≤ wrap_width con metadatos básicos.\"\"\"\n","    records = []\n","    for _, r in tqdm(df_base.iterrows(), total=len(df_base), desc=\"Chunking\"):\n","        rid = str(r[\"id\"])\n","        text = str(r[\"text\"]) if pd.notna(r[\"text\"]) else \"\"\n","        chunks = hard_wrap(text, width=wrap_width)\n","        if not chunks:\n","            continue\n","        for j, ch in enumerate(chunks):\n","            records.append({\n","                \"chunk_id\": f\"{rid}::ch{j}\",\n","                \"id\": rid,\n","                \"make\": r.get(\"make\"),\n","                \"model\": r.get(\"model\"),\n","                \"year\": r.get(\"year\"),\n","                \"component\": r.get(\"component\"),\n","                \"chunk_idx\": j,\n","                \"text\": ch\n","            })\n","    return pd.DataFrame.from_records(records)\n","\n","# Ejecutar carga y chunking\n","corpusC = load_complaints_corpus(PARQ_IN, JSONL_IN)\n","print(\"Corpus base complaints:\", corpusC.shape)\n","display(corpusC.head(3))\n","\n","complaints_ch = make_chunks(corpusC, wrap_width=1200)\n","print(\"Chunks generados:\", complaints_ch.shape)\n","display(complaints_ch.head(5))\n","\n","# Chequeos mínimos\n","assert {\"id\",\"text\",\"chunk_id\"}.issubset(complaints_ch.columns), \"Faltan columnas mínimas\"\n","assert len(complaints_ch) > 0, \"No hay chunks generados\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":609},"id":"qZNf-Ix7Ader","executionInfo":{"status":"ok","timestamp":1760254900456,"user_tz":360,"elapsed":405612,"user":{"displayName":"Lucero Guadalupe Contreras Hernandez","userId":"06647893651071570506"}},"outputId":"33427a48-df87-4b8e-e319-0befed9bbc90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Corpus base complaints: (2137711, 6)\n"]},{"output_type":"display_data","data":{"text/plain":["  id                                               text   make        model  \\\n","0  1  RADIATOR FAILED @ HIGHWAY SPEED OBSTRUCTING DR...  VOLVO          760   \n","1  2  FUEL LEAKED FROM FUEL TANK AREA, EMITTING STRO...   FORD  THUNDERBIRD   \n","2  3  SHIFTED INTO REVERSE VEHICLE JERKED VIOLENTLY....    KIA       SEPHIA   \n","\n","   year                                          component  \n","0   NaN  ENGINE AND ENGINE COOLING:COOLING SYSTEM:RADIA...  \n","1   NaN                     FUEL SYSTEM, GASOLINE:DELIVERY  \n","2   NaN                 POWER TRAIN:AUTOMATIC TRANSMISSION  "],"text/html":["\n","  <div id=\"df-bfdebf03-40ab-482d-9c50-12597fba5037\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>make</th>\n","      <th>model</th>\n","      <th>year</th>\n","      <th>component</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>RADIATOR FAILED @ HIGHWAY SPEED OBSTRUCTING DR...</td>\n","      <td>VOLVO</td>\n","      <td>760</td>\n","      <td>NaN</td>\n","      <td>ENGINE AND ENGINE COOLING:COOLING SYSTEM:RADIA...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>FUEL LEAKED FROM FUEL TANK AREA, EMITTING STRO...</td>\n","      <td>FORD</td>\n","      <td>THUNDERBIRD</td>\n","      <td>NaN</td>\n","      <td>FUEL SYSTEM, GASOLINE:DELIVERY</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>SHIFTED INTO REVERSE VEHICLE JERKED VIOLENTLY....</td>\n","      <td>KIA</td>\n","      <td>SEPHIA</td>\n","      <td>NaN</td>\n","      <td>POWER TRAIN:AUTOMATIC TRANSMISSION</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bfdebf03-40ab-482d-9c50-12597fba5037')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-bfdebf03-40ab-482d-9c50-12597fba5037 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-bfdebf03-40ab-482d-9c50-12597fba5037');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-d05ddb96-68ce-48db-8aad-f20831b0dacd\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d05ddb96-68ce-48db-8aad-f20831b0dacd')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-d05ddb96-68ce-48db-8aad-f20831b0dacd button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"assert len(complaints_ch) > 0, \\\"No hay chunks generados\\\"\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"1\",\n          \"2\",\n          \"3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"RADIATOR FAILED @ HIGHWAY SPEED OBSTRUCTING DRIVERS VISION TEMPORARY. PLEASE DESCRIBE DETAILS. TT\",\n          \"FUEL LEAKED FROM FUEL TANK AREA, EMITTING STRONG FUMES IN GARAGE. *AK\",\n          \"SHIFTED INTO REVERSE VEHICLE JERKED VIOLENTLY. RESULTING IN VEHICLE ACCIDENT. TT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"make\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"VOLVO\",\n          \"FORD\",\n          \"KIA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"760\",\n          \"THUNDERBIRD\",\n          \"SEPHIA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"component\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Chunking: 100%|██████████| 2137711/2137711 [06:08<00:00, 5795.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Chunks generados: (2299879, 8)\n"]},{"output_type":"display_data","data":{"text/plain":["  chunk_id id   make        model  year  \\\n","0   1::ch0  1  VOLVO          760   NaN   \n","1   2::ch0  2   FORD  THUNDERBIRD   NaN   \n","2   3::ch0  3    KIA       SEPHIA   NaN   \n","3   4::ch0  4  DODGE          600   NaN   \n","4   5::ch0  5  DODGE      CARAVAN   NaN   \n","\n","                                           component  chunk_idx  \\\n","0  ENGINE AND ENGINE COOLING:COOLING SYSTEM:RADIA...          0   \n","1                     FUEL SYSTEM, GASOLINE:DELIVERY          0   \n","2                 POWER TRAIN:AUTOMATIC TRANSMISSION          0   \n","3        FUEL SYSTEM, GASOLINE:STORAGE:TANK ASSEMBLY          0   \n","4                                              SEATS          0   \n","\n","                                                text  \n","0  RADIATOR FAILED @ HIGHWAY SPEED OBSTRUCTING DR...  \n","1  FUEL LEAKED FROM FUEL TANK AREA, EMITTING STRO...  \n","2  SHIFTED INTO REVERSE VEHICLE JERKED VIOLENTLY....  \n","3  FUEL TANK ; LEAKS BECAUSE OF RUST GAS LEAK BY ...  \n","4  DRIVER SIDE SEAT FRAME BROKE IN TWO, CAUSING S...  "],"text/html":["\n","  <div id=\"df-8f6aa756-584f-477d-ad7c-a4db7966aa9a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chunk_id</th>\n","      <th>id</th>\n","      <th>make</th>\n","      <th>model</th>\n","      <th>year</th>\n","      <th>component</th>\n","      <th>chunk_idx</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1::ch0</td>\n","      <td>1</td>\n","      <td>VOLVO</td>\n","      <td>760</td>\n","      <td>NaN</td>\n","      <td>ENGINE AND ENGINE COOLING:COOLING SYSTEM:RADIA...</td>\n","      <td>0</td>\n","      <td>RADIATOR FAILED @ HIGHWAY SPEED OBSTRUCTING DR...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2::ch0</td>\n","      <td>2</td>\n","      <td>FORD</td>\n","      <td>THUNDERBIRD</td>\n","      <td>NaN</td>\n","      <td>FUEL SYSTEM, GASOLINE:DELIVERY</td>\n","      <td>0</td>\n","      <td>FUEL LEAKED FROM FUEL TANK AREA, EMITTING STRO...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3::ch0</td>\n","      <td>3</td>\n","      <td>KIA</td>\n","      <td>SEPHIA</td>\n","      <td>NaN</td>\n","      <td>POWER TRAIN:AUTOMATIC TRANSMISSION</td>\n","      <td>0</td>\n","      <td>SHIFTED INTO REVERSE VEHICLE JERKED VIOLENTLY....</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4::ch0</td>\n","      <td>4</td>\n","      <td>DODGE</td>\n","      <td>600</td>\n","      <td>NaN</td>\n","      <td>FUEL SYSTEM, GASOLINE:STORAGE:TANK ASSEMBLY</td>\n","      <td>0</td>\n","      <td>FUEL TANK ; LEAKS BECAUSE OF RUST GAS LEAK BY ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5::ch0</td>\n","      <td>5</td>\n","      <td>DODGE</td>\n","      <td>CARAVAN</td>\n","      <td>NaN</td>\n","      <td>SEATS</td>\n","      <td>0</td>\n","      <td>DRIVER SIDE SEAT FRAME BROKE IN TWO, CAUSING S...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f6aa756-584f-477d-ad7c-a4db7966aa9a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-8f6aa756-584f-477d-ad7c-a4db7966aa9a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-8f6aa756-584f-477d-ad7c-a4db7966aa9a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-27497da8-576f-4d6c-8ca9-2a3f89c267ca\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-27497da8-576f-4d6c-8ca9-2a3f89c267ca')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-27497da8-576f-4d6c-8ca9-2a3f89c267ca button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"assert len(complaints_ch) > 0, \\\"No hay chunks generados\\\"\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"chunk_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2::ch0\",\n          \"5::ch0\",\n          \"3::ch0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2\",\n          \"5\",\n          \"3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"make\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"FORD\",\n          \"DODGE\",\n          \"VOLVO\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"THUNDERBIRD\",\n          \"CARAVAN\",\n          \"SEPHIA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"component\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FqIzTScZNj-I","executionInfo":{"status":"ok","timestamp":1760298285852,"user_tz":360,"elapsed":17503,"user":{"displayName":"Lucero Guadalupe Contreras Hernández","userId":"07832692706572343340"}},"outputId":"86f73a29-1a15-4b99-faae-28ef0fdc46cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","# Mitiga fragmentación de VRAM (PyTorch 2.0+)\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"],"metadata":{"id":"5V4U8-qR2Xt5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================== Complaints → e5-multilingual-large-instruct (RESUME + REPAIR) =====================\n","import os, json, time, gc, re, math\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import pyarrow.parquet as pq\n","import torch\n","from sentence_transformers import SentenceTransformer\n","from contextlib import nullcontext\n","from torch.cuda.amp import autocast\n","\n","# -------------------- Config --------------------\n","MODEL_NAME        = \"intfloat/multilingual-e5-large-instruct\"\n","OUT_DIR           = Path(\"/content/drive/MyDrive/NHTSA/embeddings/complaints_e5_mlg_instruct\")\n","PARQ_IN           = Path(\"/content/drive/MyDrive/NHTSA/processed/complaints.parquet\")\n","JSONL_IN          = Path(\"/content/drive/MyDrive/NHTSA/processed/complaints_corpus.jsonl\")\n","\n","DEVICE            = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","BATCH_SIZE        = 512\n","INIT_BATCH        = 512\n","MIN_BATCH         = 64\n","SHARD_ROWS        = 25_000\n","STREAM_READ_ROWS  = 200_000\n","MAX_SEQ_LEN       = 256\n","NORMALIZE         = True\n","\n","# Al principio guardaste en float32 (USE_FP16_STORAGE=False), mantenemos eso:\n","USE_FP16_STORAGE  = False\n","DEDUP_TEXTS       = False\n","\n","SAFE_MODE_NO_DELETE = True  # no borrar nada\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","manifest_path   = OUT_DIR / \"manifest.json\"\n","checkpoint_path = OUT_DIR / \"checkpoint.json\"\n","\n","# -------------------- Utilidades --------------------\n","def load_manifest(path: Path):\n","    if path.exists():\n","        with open(path, \"r\") as f:\n","            return json.load(f)\n","    return {\"dataset\":\"complaints\",\"model\":MODEL_NAME,\"device\":DEVICE,\"dim\":None,\n","            \"batch_size\":BATCH_SIZE,\"max_seq_len\":MAX_SEQ_LEN,\"normalize\":NORMALIZE,\n","            \"fp16_storage\":USE_FP16_STORAGE,\"shard_rows\":SHARD_ROWS,\"shards\":[]}\n","\n","def save_manifest(obj, path: Path):\n","    tmp = path.with_suffix(\".tmp\")\n","    with open(tmp, \"w\") as f: json.dump(obj, f, indent=2)\n","    os.replace(tmp, path)\n","\n","def load_checkpoint(path: Path):\n","    if path.exists():\n","        with open(path, \"r\") as f:\n","            return json.load(f)\n","    return {\"rows_done\": 0, \"next_shard_idx\": 0}\n","\n","def save_checkpoint(rows_done: int, next_idx: int):\n","    tmp = checkpoint_path.with_suffix(\".tmp\")\n","    with open(tmp, \"w\") as f:\n","        json.dump({\"rows_done\": int(rows_done), \"next_shard_idx\": int(next_idx)}, f)\n","    os.replace(tmp, checkpoint_path)\n","\n","def pick_source():\n","    if PARQ_IN.exists(): return \"parquet\"\n","    if JSONL_IN.exists(): return \"jsonl\"\n","    raise FileNotFoundError(\"No se encontró complaints.parquet ni complaints_corpus.jsonl\")\n","\n","def format_e5_passage_col(series: pd.Series) -> pd.Series:\n","    return \"passage: \" + series.astype(\"string\").str.strip()\n","\n","def hash64(s: pd.Series) -> pd.Series:\n","    return pd.util.hash_pandas_object(s.astype(\"string\"), index=False).astype(\"int64\")\n","\n","def stream_parquet_batches(parquet_path: Path, cols=None, batch_rows=STREAM_READ_ROWS):\n","    pf = pq.ParquetFile(str(parquet_path))\n","    read_cols = cols or pf.schema.names\n","    for batch in pf.iter_batches(batch_size=batch_rows, columns=read_cols):\n","        yield batch.to_pandas(types_mapper=None)\n","\n","def stream_jsonl_batches(jsonl_path: Path, batch_rows=STREAM_READ_ROWS):\n","    buf = {\"ID\":[], \"TEXT\":[], \"MAKETXT\":[], \"MODELTXT\":[], \"YEARTXT\":[], \"COMPDESC\":[]}\n","    n = 0\n","    def flush():\n","        nonlocal buf\n","        if buf[\"ID\"]:\n","            yield pd.DataFrame(buf)\n","        buf = {\"ID\":[], \"TEXT\":[], \"MAKETXT\":[], \"MODELTXT\":[], \"YEARTXT\":[], \"COMPDESC\":[]}\n","    import json as _json\n","    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            try: obj = _json.loads(line)\n","            except _json.JSONDecodeError: continue\n","            _id = obj.get(\"id\"); _txt = obj.get(\"text\"); meta = obj.get(\"metadata\") or {}\n","            if not _id or not _txt: continue\n","            buf[\"ID\"].append(str(_id)); buf[\"TEXT\"].append(str(_txt))\n","            buf[\"MAKETXT\"].append(meta.get(\"MAKETXT\")); buf[\"MODELTXT\"].append(meta.get(\"MODELTXT\"))\n","            buf[\"YEARTXT\"].append(meta.get(\"YEARTXT\")); buf[\"COMPDESC\"].append(meta.get(\"COMPDESC\"))\n","            n += 1\n","            if n % batch_rows == 0:\n","                yield from flush()\n","    yield from flush()\n","\n","# ---------- REPAIR: shards escritos como bruto -> convertir a .npy válido ----------\n","def _idx_from_name(p: Path):\n","    m = re.search(r'(\\d{4})', p.stem)\n","    return int(m.group(1)) if m else None\n","\n","def _try_npy_shape(path: Path):\n","    # Prueba cargar como .npy normal\n","    arr = np.load(path, mmap_mode=\"r\", allow_pickle=False)\n","    return int(arr.shape[0]), int(arr.shape[1])\n","\n","def _repair_headerless_npy(p_npy: Path, p_meta: Path):\n","    \"\"\"\n","    Si el archivo es binario crudo (sin encabezado .npy), lo rehace a formato .npy usando:\n","      - filas = len(meta)\n","      - dim    = file_size / (rows * bytes_per_elem)\n","      - dtype  = float32 por default (o float16 si detectas que cuadra)\n","    \"\"\"\n","    rows_meta = len(pd.read_parquet(p_meta))\n","    size_bytes = p_npy.stat().st_size\n","\n","    # Intentar con float32 primero\n","    bpe32 = 4\n","    dim32 = size_bytes / (rows_meta * bpe32) if rows_meta > 0 else 0\n","    ok32  = dim32.is_integer() and dim32 > 0\n","\n","    # Intentar con float16 también\n","    bpe16 = 2\n","    dim16 = size_bytes / (rows_meta * bpe16) if rows_meta > 0 else 0\n","    ok16  = dim16.is_integer() and dim16 > 0\n","\n","    if not ok32 and not ok16:\n","        raise ValueError(f\"No cuadra tamaño con filas meta. size={size_bytes}, rows={rows_meta}, dim32={dim32}, dim16={dim16}\")\n","\n","    # Preferimos float32 (coincide con tu config original)\n","    if ok32:\n","        dtype = np.float32\n","        dim   = int(dim32)\n","        bpe   = bpe32\n","    else:\n","        dtype = np.float16\n","        dim   = int(dim16)\n","        bpe   = bpe16\n","\n","    # Abrir el binario crudo como memmap y re-empaquetar a .npy con encabezado\n","    mm = np.memmap(str(p_npy), mode=\"r\", dtype=dtype, shape=(rows_meta, dim))\n","    tmp = p_npy.with_suffix(\".npy.tmp\")  # mismo nombre + .tmp\n","\n","    # USAR open_memmap para crear un .npy válido con encabezado\n","    from numpy.lib.format import open_memmap\n","    fp = open_memmap(str(tmp), mode=\"w+\", dtype=dtype, shape=(rows_meta, dim))\n","    fp[:] = mm[:]\n","    del fp, mm\n","    os.replace(tmp, p_npy)  # commit atómico\n","\n","    return rows_meta, dim, dtype\n","\n","def repair_and_read_shape(p_npy: Path, p_meta: Path):\n","    try:\n","        return _try_npy_shape(p_npy)\n","    except Exception:\n","        # Intentar reparar si era raw\n","        rows, dim, dtype = _repair_headerless_npy(p_npy, p_meta)\n","        # Verificar ahora sí como .npy\n","        r2, d2 = _try_npy_shape(p_npy)\n","        assert (rows, dim) == (r2, d2), f\"Reparado pero forma inconsistente: {(rows,dim)} vs {(r2,d2)}\"\n","        return rows, dim\n","\n","# -------------------- HARD RESUME con REPAIR --------------------\n","def hard_resume_from_disk_with_repair(out_dir: Path, manifest_path: Path, checkpoint_path: Path):\n","    npys  = { _idx_from_name(p): p for p in out_dir.glob(\"embeddings_shard_*.npy\") if _idx_from_name(p) is not None and not str(p).endswith(\".tmp\") }\n","    metas = { _idx_from_name(p): p for p in out_dir.glob(\"meta_shard_*.parquet\")  if _idx_from_name(p) is not None and not str(p).endswith(\".tmp\") }\n","\n","    complete = []\n","    warnings = []\n","    dim_detected = None\n","\n","    for idx in sorted(set(npys) & set(metas)):\n","        p_npy, p_meta = npys[idx], metas[idx]\n","        try:\n","            rows_npy, dim = repair_and_read_shape(p_npy, p_meta)  # <-- repara si hace falta\n","            rows_meta = len(pd.read_parquet(p_meta))\n","            if rows_npy == rows_meta and rows_npy > 0:\n","                complete.append((idx, rows_npy, p_npy, p_meta, dim))\n","                dim_detected = dim\n","            else:\n","                warnings.append(f\"[skip] {idx:04d} filas_npy={rows_npy} != filas_meta={rows_meta}\")\n","        except Exception as e:\n","            warnings.append(f\"[skip] {idx:04d} error al leer/reparar: {e}\")\n","\n","    if warnings:\n","        print(\"\\n\".join(warnings))\n","\n","    rows_done = sum(r for (i, r, *_ ) in complete)\n","    next_idx = (max([i for (i, *_ ) in complete], default=-1) + 1)\n","\n","    man = {\n","        \"dataset\": \"complaints\",\n","        \"model\": MODEL_NAME,\n","        \"device\": DEVICE,\n","        \"dim\": dim_detected,\n","        \"batch_size\": BATCH_SIZE,\n","        \"max_seq_len\": MAX_SEQ_LEN,\n","        \"normalize\": NORMALIZE,\n","        \"fp16_storage\": USE_FP16_STORAGE,\n","        \"shard_rows\": SHARD_ROWS,\n","        \"shards\": [\n","            {\"shard_idx\": int(i), \"rows\": int(r), \"embeddings\": str(pn), \"meta\": str(pm), \"complete\": True}\n","            for (i, r, pn, pm, _d) in sorted(complete, key=lambda x: x[0])\n","        ]\n","    }\n","    save_manifest(man, manifest_path)\n","    save_checkpoint(int(rows_done), int(next_idx))\n","\n","    print(f\"[HARD RESUME + REPAIR] shards_ok={len(complete)} | rows_done={rows_done:,} | next_shard_idx={next_idx}\")\n","    return rows_done, next_idx, dim_detected\n","\n","# -------------------- Modelo --------------------\n","t0 = time.time()\n","model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n","try:\n","    model.max_seq_length = MAX_SEQ_LEN\n","except Exception:\n","    pass\n","if DEVICE == \"cuda\":\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    try: torch.set_float32_matmul_precision(\"high\")\n","    except Exception: pass\n","print(f\"Device: {DEVICE} | Modelo cargado en {time.time()-t0:.2f}s (max_len={MAX_SEQ_LEN}, bs={BATCH_SIZE})\")\n","\n","# -------------------- Reconstrucción de estado (con REPAIR) --------------------\n","rows_done, shard_idx, _ = hard_resume_from_disk_with_repair(OUT_DIR, manifest_path, checkpoint_path)\n","ck = load_checkpoint(checkpoint_path)\n","rows_done = max(rows_done, ck.get(\"rows_done\", 0))\n","shard_idx = max(shard_idx, ck.get(\"next_shard_idx\", 0))\n","print(f\"▶︎ Reanudación efectiva: rows_done={rows_done:,} | next_shard_idx={shard_idx}\")\n","\n","# -------------------- Encode + commit atómico (usa open_memmap para .npy válido) --------------------\n","def encode_and_flush_shard_streaming(cur_sub: pd.DataFrame, shard_idx: int):\n","    shard_emb_final  = OUT_DIR / f\"embeddings_shard_{shard_idx:04d}.npy\"\n","    shard_meta_final = OUT_DIR / f\"meta_shard_{shard_idx:04d}.parquet\"\n","    shard_emb_tmp    = OUT_DIR / f\"embeddings_shard_{shard_idx:04d}.npy.tmp\"\n","    shard_meta_tmp   = OUT_DIR / f\"meta_shard_{shard_idx:04d}.parquet.tmp\"\n","\n","    # Skip si ya existe (consistente)\n","    if shard_emb_final.exists() and shard_meta_final.exists():\n","        try:\n","            rows_expected = len(pd.read_parquet(shard_meta_final))\n","            arr = np.load(shard_emb_final, mmap_mode=\"r\", allow_pickle=False)\n","            rows_npy, dim = int(arr.shape[0]), int(arr.shape[1])\n","            if rows_npy == rows_expected:\n","                man = load_manifest(manifest_path)\n","                man[\"dim\"] = int(dim) if man.get(\"dim\") is None else man[\"dim\"]\n","                man[\"shards\"] = [s for s in man[\"shards\"] if int(s[\"shard_idx\"]) != shard_idx]\n","                man[\"shards\"].append({\"shard_idx\": shard_idx, \"rows\": rows_npy,\n","                                      \"embeddings\": str(shard_emb_final), \"meta\": str(shard_meta_final),\n","                                      \"complete\": True})\n","                man[\"shards\"] = sorted(man[\"shards\"], key=lambda s: s[\"shard_idx\"])\n","                save_manifest(man, manifest_path)\n","                save_checkpoint(sum(s[\"rows\"] for s in man[\"shards\"]), shard_idx+1)\n","                print(f\"↪︎ Shard {shard_idx:04d} ya existía y es consistente. SKIP.\")\n","                return rows_npy, dim\n","        except Exception as e:\n","            print(f\"⚠️ Shard {shard_idx:04d} existente pero no comprobable ({e}). Reescribiendo de forma segura…\")\n","\n","    # Preparar textos\n","    texts = cur_sub[\"__model_text\"].astype(\"string\").tolist()\n","    rows  = len(texts)\n","    if rows == 0:\n","        man = load_manifest(manifest_path)\n","        man[\"shards\"] = [s for s in man[\"shards\"] if int(s[\"shard_idx\"]) != shard_idx]\n","        save_manifest(man, manifest_path)\n","        save_checkpoint(sum(s[\"rows\"] for s in man[\"shards\"]), shard_idx+1)\n","        print(f\"↪︎ Shard {shard_idx:04d} vacío. SKIP.\")\n","        return 0, man.get(\"dim\")\n","\n","    cur_bs = min(BATCH_SIZE, rows)\n","    dtype = np.float16 if USE_FP16_STORAGE else np.float32\n","    use_autocast = (DEVICE == \"cuda\")\n","    # torch.amp.autocast recomendado (mensaje deprecación)\n","    amp_ctx = (lambda: torch.amp.autocast(\"cuda\", dtype=torch.float16)) if use_autocast else nullcontext\n","\n","    # 1) Embeddings → open_memmap *.tmp (crea .npy válido con header)\n","    from numpy.lib.format import open_memmap\n","    fp = None\n","    dim = None\n","    pbar = tqdm(total=rows, desc=f\"Shard {shard_idx:04d}\", unit=\"row\", leave=False)\n","    rows_written = 0\n","    try:\n","        i = 0\n","        while i < rows:\n","            bs = min(cur_bs, rows - i)\n","            try:\n","                batch = texts[i:i+bs]\n","                with amp_ctx():\n","                    with torch.no_grad():\n","                        vec = model.encode(batch, batch_size=bs, convert_to_numpy=True,\n","                                           normalize_embeddings=NORMALIZE, show_progress_bar=False\n","                                           ).astype(\"float32\", copy=False)\n","                if dim is None:\n","                    dim = vec.shape[1]\n","                    fp  = open_memmap(str(shard_emb_tmp), mode=\"w+\", dtype=dtype, shape=(rows, dim))\n","                n_now = vec.shape[0]\n","                fp[rows_written:rows_written+n_now, :] = vec.astype(dtype, copy=False)\n","                rows_written += n_now; i += n_now; pbar.update(n_now)\n","                del batch, vec; gc.collect()\n","                if torch.cuda.is_available(): torch.cuda.empty_cache()\n","                if DEVICE == \"cuda\":\n","                    free, total = torch.cuda.mem_get_info()\n","                    if free/total > 0.50 and cur_bs < INIT_BATCH:\n","                        cur_bs = min(INIT_BATCH, cur_bs*2)\n","            except RuntimeError as e:\n","                msg = str(e).lower()\n","                if \"out of memory\" in msg or \"cuda\" in msg:\n","                    del batch; gc.collect()\n","                    if torch.cuda.is_available(): torch.cuda.empty_cache()\n","                    new_bs = max(MIN_BATCH, bs//2)\n","                    if new_bs < bs:\n","                        cur_bs = new_bs\n","                        continue\n","                    else:\n","                        raise\n","                else:\n","                    raise\n","    finally:\n","        pbar.close()\n","        if fp is not None:\n","            del fp\n","        gc.collect()\n","\n","    # 2) Meta → parquet *.tmp\n","    meta_cols = [c for c in [\"id\",\"make\",\"model\",\"year\",\"component\",\"_h\"] if c in cur_sub.columns]\n","    cur_sub[meta_cols].to_parquet(shard_meta_tmp, index=False)\n","\n","    # 3) Commit atómico\n","    os.replace(shard_emb_tmp, shard_emb_final)\n","    os.replace(shard_meta_tmp, shard_meta_final)\n","\n","    # 4) Actualizar manifest y checkpoint\n","    man = load_manifest(manifest_path)\n","    man[\"dim\"] = int(dim) if man.get(\"dim\") is None else man[\"dim\"]\n","    man[\"shards\"] = [s for s in man[\"shards\"] if int(s[\"shard_idx\"]) != shard_idx]\n","    man[\"shards\"].append({\"shard_idx\": shard_idx, \"rows\": rows_written,\n","                          \"embeddings\": str(shard_emb_final), \"meta\": str(shard_meta_final),\n","                          \"complete\": True})\n","    man[\"shards\"] = sorted(man[\"shards\"], key=lambda s: s[\"shard_idx\"])\n","    save_manifest(man, manifest_path)\n","    save_checkpoint(sum(s[\"rows\"] for s in man[\"shards\"]), shard_idx+1)\n","\n","    print(f\"→ Shard {shard_idx:04d} COMPLETO | filas={rows_written:,} | dim={dim} | {shard_emb_final.name}\")\n","    return rows_written, dim\n","\n","# -------------------- Bucle principal (respeta checkpoint/shards existentes) --------------------\n","SOURCE = pick_source()\n","print(f\"Origen de datos: {SOURCE}\")\n","\n","source_iter = (stream_parquet_batches(PARQ_IN, cols=None, batch_rows=STREAM_READ_ROWS)\n","               if SOURCE==\"parquet\" else\n","               stream_jsonl_batches(JSONL_IN, batch_rows=STREAM_READ_ROWS))\n","\n","ck = load_checkpoint(checkpoint_path)\n","skip_rows = ck.get(\"rows_done\", 0)           # <- salta filas ya embebidas\n","next_idx  = ck.get(\"next_shard_idx\", 0)\n","shard_idx = max(shard_idx, next_idx)\n","\n","acc_parts, acc_rows = [], 0\n","total_rows_processed = skip_rows\n","\n","for df in source_iter:\n","    df.columns = [c.upper() for c in df.columns]\n","\n","    if \"TEXT\" not in df.columns:\n","        if \"CDESCR\" in df.columns:\n","            df[\"TEXT\"] = df[\"CDESCR\"].astype(str)\n","        else:\n","            raise AssertionError(\"No se encontró columna de texto (TEXT o CDESCR).\")\n","    if \"ID\" not in df.columns:\n","        cid = \"CMPLID\" if \"CMPLID\" in df.columns else (\"ODINO\" if \"ODINO\" in df.columns else None)\n","        assert cid is not None, \"No se encontró ID (CMPLID/ODINO).\"\n","        df[\"ID\"] = df[cid].astype(str)\n","\n","    # Skip por checkpoint (salta lotes completos/parciales)\n","    if skip_rows > 0:\n","        if len(df) <= skip_rows:\n","            skip_rows -= len(df)\n","            continue\n","        else:\n","            df = df.iloc[skip_rows:].copy()\n","            skip_rows = 0\n","\n","    df[\"_h\"] = hash64(df[\"TEXT\"])\n","    if DEDUP_TEXTS:\n","        df = df.drop_duplicates(\"_h\")\n","    df[\"__model_text\"] = format_e5_passage_col(df[\"TEXT\"])\n","\n","    keep = [\"ID\",\"__model_text\",\"_h\"]\n","    for c in [\"MAKETXT\",\"MODELTXT\",\"YEARTXT\",\"COMPDESC\"]:\n","        if c in df.columns: keep.append(c)\n","\n","    acc_parts.append(df[keep]); acc_rows += len(df)\n","    del df; gc.collect()\n","\n","    if acc_rows >= SHARD_ROWS:\n","        cur = pd.concat(acc_parts, ignore_index=True)\n","        if len(cur) > SHARD_ROWS:\n","            cur_sub = cur.iloc[:SHARD_ROWS].copy()\n","            rest    = cur.iloc[SHARD_ROWS:].copy()\n","            acc_parts = [rest] if len(rest) else []\n","            acc_rows  = 0 if not acc_parts else len(rest)\n","        else:\n","            cur_sub = cur; acc_parts = []; acc_rows = 0\n","\n","        cur_sub = cur_sub.rename(columns={\n","            \"ID\":\"id\", \"MAKETXT\":\"make\", \"MODELTXT\":\"model\",\n","            \"YEARTXT\":\"year\", \"COMPDESC\":\"component\"\n","        })\n","        n_written, dim = encode_and_flush_shard_streaming(cur_sub, shard_idx)\n","        shard_idx += 1\n","        total_rows_processed += n_written\n","        print(f\"Total filas procesadas: {total_rows_processed:,}\")\n","\n","# flush final\n","if acc_parts:\n","    cur = pd.concat(acc_parts, ignore_index=True)\n","    cur = cur.rename(columns={\n","        \"ID\":\"id\", \"MAKETXT\":\"make\", \"MODELTXT\":\"model\",\n","        \"YEARTXT\":\"year\", \"COMPDESC\":\"component\"\n","    })\n","    n_written, dim = encode_and_flush_shard_streaming(cur, shard_idx)\n","    shard_idx += 1\n","    total_rows_processed += n_written\n","    print(f\"Total filas procesadas: {total_rows_processed:,}\")\n","\n","print(\"Manifest:\", manifest_path)\n","print(\"Checkpoint:\", checkpoint_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":807},"id":"I-9_Q7G9dSz5","outputId":"694f6908-c244-45d1-d041-6b9e2687dc12","executionInfo":{"status":"error","timestamp":1760301850815,"user_tz":360,"elapsed":2905913,"user":{"displayName":"Lucero Guadalupe Contreras Hernández","userId":"07832692706572343340"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda | Modelo cargado en 4.81s (max_len=256, bs=512)\n","[HARD RESUME + REPAIR] shards_ok=11 | rows_done=275,000 | next_shard_idx=11\n","▶︎ Reanudación efectiva: rows_done=275,000 | next_shard_idx=11\n","Origen de datos: parquet\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["→ Shard 0011 COMPLETO | filas=25,000 | dim=1024 | embeddings_shard_0011.npy\n","Total filas procesadas: 300,000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["→ Shard 0012 COMPLETO | filas=25,000 | dim=1024 | embeddings_shard_0012.npy\n","Total filas procesadas: 325,000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["→ Shard 0013 COMPLETO | filas=25,000 | dim=1024 | embeddings_shard_0013.npy\n","Total filas procesadas: 350,000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["→ Shard 0014 COMPLETO | filas=25,000 | dim=1024 | embeddings_shard_0014.npy\n","Total filas procesadas: 375,000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["→ Shard 0015 COMPLETO | filas=25,000 | dim=1024 | embeddings_shard_0015.npy\n","Total filas procesadas: 400,000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["→ Shard 0016 COMPLETO | filas=25,000 | dim=1024 | embeddings_shard_0016.npy\n","Total filas procesadas: 425,000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["→ Shard 0017 COMPLETO | filas=25,000 | dim=1024 | embeddings_shard_0017.npy\n","Total filas procesadas: 450,000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["→ Shard 0018 COMPLETO | filas=25,000 | dim=1024 | embeddings_shard_0018.npy\n","Total filas procesadas: 475,000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["→ Shard 0019 COMPLETO | filas=25,000 | dim=1024 | embeddings_shard_0019.npy\n","Total filas procesadas: 500,000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["→ Shard 0020 COMPLETO | filas=25,000 | dim=1024 | embeddings_shard_0020.npy\n","Total filas procesadas: 525,000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3703706401.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;34m\"YEARTXT\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"COMPDESC\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"component\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     })\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0mn_written\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_and_flush_shard_streaming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0mshard_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0mtotal_rows_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn_written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3703706401.py\u001b[0m in \u001b[0;36mencode_and_flush_shard_streaming\u001b[0;34m(cur_sub, shard_idx)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mamp_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                         vec = model.encode(batch, batch_size=bs, convert_to_numpy=True,\n\u001b[0m\u001b[1;32m    303\u001b[0m                                            \u001b[0mnormalize_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNORMALIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                                            ).astype(\"float32\", copy=False)\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Batches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0msentences_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts, **kwargs)\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \"\"\"\n\u001b[1;32m   1620\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts, padding)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         output.update(\n\u001b[0;32m--> 319\u001b[0;31m             self.tokenizer(\n\u001b[0m\u001b[1;32m    320\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0mto_tokenize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3024\u001b[0m                 )\n\u001b[1;32m   3025\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   3027\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3225\u001b[0m         )\n\u001b[1;32m   3226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3228\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"8tnNLqrHm6Re"},"execution_count":null,"outputs":[]}]}