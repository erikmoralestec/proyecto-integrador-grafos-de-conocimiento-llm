{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Informe de Ejecución del Proceso de Carga de Vectores a Qdrant\n","\n","## Resumen\n","\n","Este documento describe la ingesta de tres conjuntos vectoriales — **Recalls**, **Investigations** y **Complaints (representantes)** — en **Qdrant Cloud** desde **Google Colab**. Se fijaron versiones de dependencias, se estandarizó la creación/validación de colecciones, se implementó carga por lotes y, donde aplica, **UUID v5 determinísticos**. Se corrigieron rutas y nombres de colecciones a:\n","\n","* `nhtsa_investigations`\n","* `nhtsa_recalls`\n","* `nhtsa_complaints`\n","\n","---\n","\n","## 1) Configuración del entorno\n","\n","* **Montaje de Drive:** `drive.mount('/content/drive')`\n","* **Credenciales:**\n","  `QDRANT_URL=\"https://…qdrant.io\"`\n","  `QDRANT_API_KEY=\"…\"`.\n","* **Dependencias (fijadas):**\n","  `qdrant-client==1.9.2`, `pandas==2.2.2`, `numpy==1.26.4`, `pyarrow==17.0.0`.\n","* **Ruta base en Drive:**\n","\n","  ```\n","  BASE = /content/drive/MyDrive/NHTSA\n","  ```\n","\n","---\n","\n","## 2) Funciones auxiliares (resumen)\n","\n","* `ensure_collection(client, name, dim, distance)`: crea o **recrea** la colección si la dimensión no coincide.\n","* `df_to_payloads(df)`: convierte `NaN→None` y tipos `numpy` a nativos.\n","* `upsert_batches(...)`: `upsert` por lotes (batch) con `wait=True`.\n","* **(Complaints)** `detect_rep_columns`, `load_reps_and_vectors`, `make_point_uuid(...)` (UUID v5).\n","\n","---\n","\n","## 3) Procesos de carga\n","\n","### 3.1 Recalls\n","\n","**Colección destino:** `nhtsa_recalls`\n","**Rutas:**\n","\n","```\n","RCL_EMB_DIR = BASE/embeddings/recalls_e5_mlg_instruct\n","RCL_META    = RCL_EMB_DIR/recalls_chunks_meta.parquet\n","RCL_EMB     = RCL_EMB_DIR/recalls_embeddings.npy\n","```\n","\n","**Flujo:**\n","\n","1. Cargar `recalls_embeddings.npy` y `recalls_chunks_meta.parquet`.\n","2. `assert len(vecs) == len(meta)`.\n","3. `ensure_collection(client, \"nhtsa_recalls\", dim=vecs.shape[1])`.\n","4. `upsert_batches(..., batch_size=512–1024)` con **IDs enteros secuenciales** (reiniciados si se recrea la colección).\n","\n","> **Nota de robustez:** si se recrea la colección o se fuerza recarga limpia, **no reutilizar checkpoint** y comenzar IDs en `0`.\n","\n","---\n","\n","### 3.2 Investigations\n","\n","**Colección destino:** `nhtsa_investigations`\n","**Rutas:**\n","\n","```\n","INVDIR = BASE/embeddings/investigations_e5_mlg_instruct\n","META   = INVDIR/invest_chunks_meta.parquet\n","NPY    = INVDIR/invest_embeddings.npy    # (6354, 1024)\n","```\n","\n","**Flujo:**\n","\n","1. Verificar correspondencia: `invest_embeddings.npy` (≈**6,354** × **1,024**) ↔ `invest_chunks_meta.parquet` (**6,354** filas).\n","2. `ensure_collection(client, \"nhtsa_investigations\", dim=1024)`.\n","3. `upsert_batches(..., batch_size=512)`.\n","\n","**Verificación posterior:**\n","\n","* `client.count(\"nhtsa_investigations\", exact=True).count` → **6354**.\n","* `scroll` con `with_payload=True` para muestreo.\n","\n","---\n","\n","### 3.3 Complaints (representantes)\n","\n","**Colección destino:** `nhtsa_complaints`\n","**Rutas:**\n","\n","```\n","REDUCED  = BASE/embeddings/complaints_e5_mlg_instruct/reduced_shards\n","CSV_REPS = REDUCED/kmeans_global/representantes_global_kmeans.csv\n","# Fallback:\n","# CSV_REPS = REDUCED/representantes_global.csv\n","# Vectores reducidos por shard: REDUCED/reduced_0000.npy, reduced_0001.npy, ...\n","```\n","\n","**Características clave:**\n","\n","* Los **vectores** viven **por shard** (`reduced_####.npy`); el CSV indica `shard_idx` y `row_in_shard`.\n","* **IDs determinísticos:** `UUID v5` a partir de `rep_{shard_idx}_{row_in_shard}` con un **namespace fijo** del proyecto.\n","* **Carga por shard:** para cada `shard_idx`, se extraen únicamente las filas (índices) necesarias y se suben por lotes (`UPSERT_BATCH` ≈ 2000, ajustar si el free tier se satura).\n","\n","**Flujo:**\n","\n","1. `detect_rep_columns` mapea columnas (`shard_idx`, `row_in_shard`, `make`, `model`, `year`, etc.); si falta, se **infieren** desde `id`.\n","2. `load_reps_and_vectors` carga CSV y vectorizadores por shard (mmap).\n","3. `ensure_collection(client, \"nhtsa_complaints\", dim=<dim reducido>)`.\n","4. Construir `payload_df` (ligero).\n","5. Iterar por `shard_idx`:\n","\n","   * `point_ids = uuid5(namespace, f\"rep_{shard:04d}_{row:06d}\")`\n","   * `vecs = stack(arr[row] for row in local_rows)`\n","   * `upsert_batches(client, \"nhtsa_complaints\", vecs, payload, point_ids, batch_size=UPSERT_BATCH)`.\n","\n","**Verificación posterior:**\n","\n","* `client.count(\"nhtsa_complaints\", exact=True).count`.\n","* `scroll` de muestra con payload.\n","\n","---\n","\n","## 4) Configuración de Qdrant (Cloud Free)\n","\n","* **Timeout recomendado:** `timeout=180`.\n","* **Distancia:** `models.Distance.COSINE`.\n","* **Tamaño de lote:** 512–1024 (o 2000 para UUID reps si no hay timeouts).\n","* **Recreación segura:** si la dimensión no coincide, **borrar y recrear** colección antes de cargar.\n","\n","---\n","\n","## 5) Validaciones y control de integridad\n","\n","* **Aserciones previas:** igualdad `#vectores == #metadatos`; dimensión constante.\n","* **Tipos payload:** `NaN→None`, `numpy.* →` tipos nativos Python.\n","* **IDs:**\n","\n","  * Recalls/Investigations: enteros secuenciales (reiniciar si recarga limpia).\n","  * Complaints reps: **UUID v5** (estables entre ejecuciones).\n","\n","---\n","\n","## 6) Conclusión\n","\n","Se pobló Qdrant con:\n","\n","* **Investigations:** `nhtsa_investigations` (≈ **6,354** puntos, **dim 1024**).\n","* **Recalls:** `nhtsa_recalls` (dim acorde al modelo e5).\n","* **Complaints (representantes):** `nhtsa_complaints` (IDs **UUID v5** determinísticos).\n","\n","La **carga por lotes**, la **recreación condicionada** de colecciones y los **identificadores estables** aseguran reproducibilidad y coherencia para **búsqueda semántica**, **alineación entre fuentes** y **serving** posterior.\n","\n","---\n","\n","### Apéndice A — Rutas resumidas\n","\n","```\n","BASE = /content/drive/MyDrive/NHTSA\n","\n","# Investigations\n","/embeddings/investigations_e5_mlg_instruct/invest_embeddings.npy\n","/embeddings/investigations_e5_mlg_instruct/invest_chunks_meta.parquet\n","→ colección: nhtsa_investigations\n","\n","# Recalls\n","/embeddings/recalls_e5_mlg_instruct/recalls_embeddings.npy\n","/embeddings/recalls_e5_mlg_instruct/recalls_chunks_meta.parquet\n","→ colección: nhtsa_recalls\n","\n","# Complaints (representantes globales)\n","/embeddings/complaints_e5_mlg_instruct/reduced_shards/kmeans_global/representantes_global_kmeans.csv\n","# o: /embeddings/complaints_e5_mlg_instruct/reduced_shards/representantes_global.csv\n","# vectores por shard:\n","# /embeddings/complaints_e5_mlg_instruct/reduced_shards/reduced_0000.npy, reduced_0001.npy, ...\n","→ colección: nhtsa_complaints\n","```\n","\n","### Apéndice B — Verificaciones rápidas\n","\n","```python\n","# Conteo exacto\n","client.count(\"nhtsa_investigations\", exact=True).count\n","client.count(\"nhtsa_recalls\", exact=True).count\n","client.count(\"nhtsa_complaints\", exact=True).count\n","\n","# Muestra de payloads\n","scrolled, _ = client.scroll(\"nhtsa_investigations\", limit=3, with_payload=True, with_vectors=False)\n","for p in scrolled: print(p.id, p.payload)\n","```\n","\n","---\n"],"metadata":{"id":"wmyihUW65lum"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lyhQatzRNycm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760302782875,"user_tz":360,"elapsed":7744,"user":{"displayName":"Lucero Guadalupe Contreras Hernandez","userId":"06647893651071570506"}},"outputId":"220bf7ab-48f6-4222-9a96-3d6cd4773811"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  tree\n","0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n","Need to get 47.9 kB of archives.\n","After this operation, 116 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n","Fetched 47.9 kB in 0s (335 kB/s)\n","Selecting previously unselected package tree.\n","(Reading database ... 126675 files and directories currently installed.)\n","Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n","Unpacking tree (2.0.2-1) ...\n","Setting up tree (2.0.2-1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n"]}],"source":["!apt install tree"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ERIwLCmHOOG2","executionInfo":{"status":"ok","timestamp":1760315303369,"user_tz":360,"elapsed":16483,"user":{"displayName":"Lucero Guadalupe Contreras Hernández","userId":"07832692706572343340"}},"outputId":"e0188320-21fc-4c38-caa1-50ff9d5e0f26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!tree drive/MyDrive/NHTSA/"],"metadata":{"id":"da3TIiYbN3A3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760302824266,"user_tz":360,"elapsed":1425,"user":{"displayName":"Lucero Guadalupe Contreras Hernandez","userId":"06647893651071570506"}},"outputId":"e3522a88-cd3a-47ca-eac3-70dca26f58e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01;34mdrive/MyDrive/NHTSA/\u001b[0m\n","├── \u001b[01;34membeddings\u001b[0m\n","│   ├── \u001b[01;34mcomplaints_e5_mlg_instruct\u001b[0m\n","│   │   ├── \u001b[00mcheckpoint.json\u001b[0m\n","│   │   ├── \u001b[01;34mclustered\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0000.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0001.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0002.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0003.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0004.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0005.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0006.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0007.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0008.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0009.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0010.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0011.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0012.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0013.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0014.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0015.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0016.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0017.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0018.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0019.npy\u001b[0m\n","│   │   ├── \u001b[00membeddings_shard_0020.npy\u001b[0m\n","│   │   ├── \u001b[00mmanifest.json\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0000.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0001.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0002.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0003.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0004.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0005.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0006.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0007.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0008.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0009.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0010.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0011.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0012.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0013.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0014.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0015.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0016.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0017.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0018.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0019.parquet\u001b[0m\n","│   │   ├── \u001b[00mmeta_shard_0020.parquet\u001b[0m\n","│   │   └── \u001b[01;34mreduced_shards\u001b[0m\n","│   │       ├── \u001b[00mreduced_0000.npy\u001b[0m\n","│   │       ├── \u001b[00mreduced_0001.npy\u001b[0m\n","│   │       ├── \u001b[00mreduced_0002.npy\u001b[0m\n","│   │       ├── \u001b[00mreduced_0003.npy\u001b[0m\n","│   │       ├── \u001b[00mreduced_0004.npy\u001b[0m\n","│   │       ├── \u001b[00mreduced_0005.npy\u001b[0m\n","│   │       ├── \u001b[00mreduced_0006.npy\u001b[0m\n","│   │       ├── \u001b[00mreduced_0007.npy\u001b[0m\n","│   │       ├── \u001b[00mreduced_0008.npy\u001b[0m\n","│   │       ├── \u001b[00mreduced_0009.npy\u001b[0m\n","│   │       ├── \u001b[00mreduced_0010.npy\u001b[0m\n","│   │       ├── \u001b[00mreduced_0011.npy\u001b[0m\n","│   │       ├── \u001b[00mreduced_0012.npy\u001b[0m\n","│   │       └── \u001b[00mreduced_0013.npy\u001b[0m\n","│   ├── \u001b[01;34minvestigations_e5_mlg_instruct\u001b[0m\n","│   │   ├── \u001b[00mconfig.json\u001b[0m\n","│   │   ├── \u001b[00minvest_chunks_meta.parquet\u001b[0m\n","│   │   ├── \u001b[00minvest_embeddings.npy\u001b[0m\n","│   │   ├── \u001b[00minvest_embeddings_part001.npy\u001b[0m\n","│   │   ├── \u001b[00minvest_embeddings_part002.npy\u001b[0m\n","│   │   ├── \u001b[00minvest_embeddings_part003.npy\u001b[0m\n","│   │   ├── \u001b[00minvest_embeddings_part004.npy\u001b[0m\n","│   │   ├── \u001b[00minvest_embeddings_part005.npy\u001b[0m\n","│   │   ├── \u001b[00minvest_embeddings_part006.npy\u001b[0m\n","│   │   ├── \u001b[00minvest_embeddings_part007.npy\u001b[0m\n","│   │   ├── \u001b[00minvest_embeddings_part008.npy\u001b[0m\n","│   │   └── \u001b[00mshards.json\u001b[0m\n","│   └── \u001b[01;34mrecalls_e5_mlg_instruct\u001b[0m\n","│       ├── \u001b[00membedding_config.json\u001b[0m\n","│       ├── \u001b[00mid_map.csv\u001b[0m\n","│       ├── \u001b[00mrecalls_chunks_meta.parquet\u001b[0m\n","│       └── \u001b[00mrecalls_embeddings.npy\u001b[0m\n","├── \u001b[01;34mneo4j\u001b[0m\n","│   └── \u001b[01;34mexports\u001b[0m\n","│       ├── \u001b[00minvestigations_neo4j_ready.csv\u001b[0m\n","│       └── \u001b[00mrecalls_neo4j_ready.csv\u001b[0m\n","├── \u001b[01;34mprocessed\u001b[0m\n","│   ├── \u001b[00mcomplaints_components.csv\u001b[0m\n","│   ├── \u001b[00mcomplaints_corpus.jsonl\u001b[0m\n","│   ├── \u001b[00mcomplaints_ids.csv\u001b[0m\n","│   ├── \u001b[00mcomplaints.jsonl\u001b[0m\n","│   ├── \u001b[00mcomplaints.parquet\u001b[0m\n","│   ├── \u001b[00minvestigations_chunks_map.csv\u001b[0m\n","│   ├── \u001b[00minvestigations_chunks.parquet\u001b[0m\n","│   ├── \u001b[00minvestigations_corpus.jsonl\u001b[0m\n","│   ├── \u001b[00minvestigations_ids.csv\u001b[0m\n","│   ├── \u001b[00minvestigations.jsonl\u001b[0m\n","│   ├── \u001b[00minvestigations.parquet\u001b[0m\n","│   ├── \u001b[00mrecalls_by_campaign.csv\u001b[0m\n","│   ├── \u001b[00mrecalls_chunks_fixed.parquet\u001b[0m\n","│   ├── \u001b[00mrecalls_corpus.jsonl\u001b[0m\n","│   ├── \u001b[00mrecalls_master_fixed.parquet\u001b[0m\n","│   └── \u001b[00mrecalls_raw_enriched.csv\u001b[0m\n","└── \u001b[01;34msource\u001b[0m\n","    ├── \u001b[01;31mFLAT_INV.zip\u001b[0m\n","    └── \u001b[01;31mFLAT_RCL_POST_2010.zip\u001b[0m\n","\n","10 directories, 94 files\n"]}]},{"cell_type":"markdown","source":["## Credenciales"],"metadata":{"id":"PuMmP6bcOw8y"}},{"cell_type":"code","source":["QDRANT_URL = \"https://6f99e241-2505-45c5-86f7-ad2aa70e3bb2.us-east-1-1.aws.cloud.qdrant.io\"\n","QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.N4plF_VhgfphnbtCni7VAPvcxer5PNkPSG1xYUc0kLE\""],"metadata":{"id":"C25e0xQTOXUM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path\n","\n","# --- Rutas a los Archivos de Origen ---\n","\n","BASE = Path(\"/content/drive/MyDrive/NHTSA\")\n","\n","# --- INVESTIGATIONS ---\n","INV_EMB_DIR = BASE / \"embeddings\" / \"investigations_e5_mlg_instruct\"\n","INV_META    = INV_EMB_DIR / \"invest_chunks_meta.parquet\"\n","INV_EMB     = INV_EMB_DIR / \"invest_embeddings.npy\" # Asumiendo que usarás el archivo completo\n","INV_PARTS   = sorted(INV_EMB_DIR.glob(\"invest_embeddings_part*.npy\"))\n","\n","# --- RECALLS ---\n","RCL_EMB_DIR = BASE / \"embeddings\" / \"recalls_e5_mlg_instruct\"\n","RCL_META    = RCL_EMB_DIR / \"recalls_chunks_meta.parquet\"\n","RCL_EMB     = RCL_EMB_DIR / \"recalls_embeddings.npy\"\n","\n","# --- COMPLAINTS (Añadido para el futuro) ---\n","# (Puedes ajustar estas rutas cuando llegues a la parte de complaints)\n","CPL_EMB_DIR = BASE / \"embeddings\" / \"complaints_e5_mlg_instruct\"\n","\n","\n","# --- Nombres de las Colecciones en Qdrant ---\n","\n","INV_COLLECTION = \"nhtsa_investigations\"\n","RCL_COLLECTION = \"nhtsa_recalls\"\n","CPL_COLLECTION = \"nhtsa_complaints\" # Para cuando cargues los datos de quejas\n","\n","\n","# --- Archivo de Checkpoint ---\n","CHKPT_FILE = BASE / \"embeddings\" / \"qdrant_upload_checkpoint.json\"\n","\n","print(\"✅ Rutas y nombres de colecciones definidos.\")"],"metadata":{"id":"RcNoab4sO_HB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760315492169,"user_tz":360,"elapsed":48,"user":{"displayName":"Lucero Guadalupe Contreras Hernández","userId":"07832692706572343340"}},"outputId":"33227c4a-ff87-4963-97f9-a35b470b001b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Rutas y nombres de colecciones definidos.\n"]}]},{"cell_type":"code","source":["!pip install qdrant_client"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APupJakVPItW","executionInfo":{"status":"ok","timestamp":1760315500824,"user_tz":360,"elapsed":5139,"user":{"displayName":"Lucero Guadalupe Contreras Hernández","userId":"07832692706572343340"}},"outputId":"9a62ccd3-c780-4d42-f856-139e7b0c2ee1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: qdrant_client in /usr/local/lib/python3.12/dist-packages (1.15.1)\n","Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant_client) (1.75.1)\n","Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant_client) (0.28.1)\n","Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from qdrant_client) (2.0.2)\n","Requirement already satisfied: portalocker<4.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from qdrant_client) (3.2.0)\n","Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant_client) (5.29.5)\n","Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from qdrant_client) (2.11.10)\n","Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant_client) (2.5.0)\n","Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.41.0->qdrant_client) (4.15.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (4.11.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (3.10)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (0.16.0)\n","Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant_client) (4.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (0.4.2)\n","Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (6.1.0)\n","Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (4.1.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.3.1)\n"]}]},{"cell_type":"code","source":["import json, math\n","import numpy as np\n","import pandas as pd\n","from qdrant_client import QdrantClient, models\n","\n","def get_client():\n","    return QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, timeout=180)\n","\n","def recreate_if_needed(client, collection_name, dim, distance=models.Distance.COSINE):\n","    # Crea o ajusta la colección con HNSW por defecto\n","    if collection_name in [c.name for c in client.get_collections().collections]:\n","        info = client.get_collection(collection_name)\n","        same_dim = (info.vectors_count is not None)  # no siempre da tamaño; validamos con config\n","        try:\n","            cfg = client.get_collection(collection_name).config\n","            current_dim = cfg.params.vectors.get(\"size\", None) if isinstance(cfg.params.vectors, dict) else cfg.params.vectors.size\n","            if current_dim != dim:\n","                print(f\"[!] Dimensión distinta ({current_dim} vs {dim}). Recreando colección {collection_name}...\")\n","                client.delete_collection(collection_name)\n","                client.recreate_collection(\n","                    collection_name=collection_name,\n","                    vectors_config=models.VectorParams(size=dim, distance=distance),\n","                )\n","            else:\n","                print(f\"[OK] Colección {collection_name} ya existe con dim={dim}.\")\n","        except Exception:\n","            # Si falla la lectura del tamaño, la recreamos para asegurar coherencia\n","            print(f\"[i] Recreando colección {collection_name} para asegurar dim={dim} ...\")\n","            try:\n","                client.delete_collection(collection_name)\n","            except Exception:\n","                pass\n","            client.recreate_collection(\n","                collection_name=collection_name,\n","                vectors_config=models.VectorParams(size=dim, distance=distance),\n","            )\n","    else:\n","        print(f\"[i] Creando colección {collection_name} (dim={dim}) ...\")\n","        client.recreate_collection(\n","            collection_name=collection_name,\n","            vectors_config=models.VectorParams(size=dim, distance=distance),\n","        )\n","\n","def df_to_payloads(df: pd.DataFrame):\n","    # Reemplaza NaN por None y convierte a tipos JSON-seguros\n","    clean = df.where(pd.notnull(df), None)\n","    # Convierte tipos numpy a nativos (por si acaso)\n","    def to_native(v):\n","        if isinstance(v, (np.generic,)):\n","            return v.item()\n","        return v\n","    return [{k: to_native(v) for k, v in row.items()} for row in clean.to_dict(orient=\"records\")]\n","\n","def load_checkpoint():\n","    if CHKPT_FILE.exists():\n","        try:\n","            return json.loads(CHKPT_FILE.read_text())\n","        except Exception:\n","            return {}\n","    return {}\n","\n","def save_checkpoint(state: dict):\n","    CHKPT_FILE.parent.mkdir(parents=True, exist_ok=True)\n","    CHKPT_FILE.write_text(json.dumps(state, indent=2))\n","\n","def upload_batches(client, collection, embeddings_np, meta_df, global_start_id=0, batch_size=1024, progress_key=None):\n","    \"\"\"\n","    Sube vectors+payloads en lotes. Asume que embeddings_np.shape[0] == len(meta_df).\n","    Los IDs serán enteros consecutivos a partir de global_start_id (para scroll eficiente).\n","    \"\"\"\n","    n = embeddings_np.shape[0]\n","    dim = embeddings_np.shape[1]\n","    print(f\"[i] Subiendo {n} vectores (dim={dim}) a {collection}...\")\n","    state = load_checkpoint()\n","    done_upto = state.get(progress_key, 0) if progress_key else 0\n","\n","    start = done_upto\n","    while start < n:\n","        end = min(start + batch_size, n)\n","        vecs = embeddings_np[start:end].astype(np.float32, copy=False)\n","        payloads = df_to_payloads(meta_df.iloc[start:end].reset_index(drop=True))\n","        ids = list(range(global_start_id + start, global_start_id + end))\n","\n","        client.upsert(\n","            collection_name=collection,\n","            points=models.Batch(ids=ids, vectors=vecs.tolist(), payloads=payloads),\n","            wait=True,\n","        )\n","        start = end\n","\n","        if progress_key:\n","            state[progress_key] = start\n","            save_checkpoint(state)\n","\n","        print(f\"  – Subidos: {start}/{n}\")\n","    print(\"[OK] Carga completada.\")\n","    return global_start_id + n  # siguiente ID disponible\n"],"metadata":{"id":"F2LEIoQDPEMv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4) Subir recalls (un solo .npy)"],"metadata":{"id":"aTqAMJvDPO-7"}},{"cell_type":"code","source":["# ====== RECALLS (Versión Original + Diagnóstico) ======\n","\n","client = get_client()\n","\n","# Carga embeddings y meta\n","recalls_vecs = np.load(RCL_EMB)\n","recalls_meta = pd.read_parquet(RCL_META)\n","\n","# --- LÍNEAS DE VERIFICACIÓN CRUCIALES ---\n","print(\"--- Verificando archivos cargados ---\")\n","print(f\"Dimensiones de los vectores: {recalls_vecs.shape}\")\n","print(f\"Filas en los metadatos: {len(recalls_meta)}\")\n","print(\"------------------------------------\")\n","# ----------------------------------------\n","\n","assert len(recalls_vecs) == len(recalls_meta), f\"Desalineado: {len(recalls_vecs)} vs {len(recalls_meta)}\"\n","\n","# Usa la variable de la celda de configuración\n","print(f\"Preparando la colección '{RCL_COLLECTION}'...\")\n","recreate_if_needed(client, RCL_COLLECTION, dim=recalls_vecs.shape[1], distance=models.Distance.COSINE)\n","\n","next_id = 0\n","\n","# Carga por lotes (reanudable)\n","next_id = upload_batches(\n","    client,\n","    RCL_COLLECTION,\n","    recalls_vecs,\n","    recalls_meta,\n","    global_start_id=next_id,\n","    batch_size=1024,\n","    progress_key=\"recalls_progress\"\n",")\n","\n","print(f\"\\nProceso finalizado. Se intentaron subir {next_id} puntos a '{RCL_COLLECTION}'.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qg0DF7oYPGB_","executionInfo":{"status":"ok","timestamp":1760318009034,"user_tz":360,"elapsed":17553,"user":{"displayName":"Lucero Guadalupe Contreras Hernández","userId":"07832692706572343340"}},"outputId":"cdf82ed1-b473-414f-9ab6-27b13d8a52c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Verificando archivos cargados ---\n","Dimensiones de los vectores: (12871, 1024)\n","Filas en los metadatos: 12871\n","------------------------------------\n","Preparando la colección 'nhtsa_recalls'...\n","[OK] Colección nhtsa_recalls ya existe con dim=1024.\n","[i] Subiendo 12871 vectores (dim=1024) a nhtsa_recalls...\n","  – Subidos: 1024/12871\n","  – Subidos: 2048/12871\n","  – Subidos: 3072/12871\n","  – Subidos: 4096/12871\n","  – Subidos: 5120/12871\n","  – Subidos: 6144/12871\n","  – Subidos: 7168/12871\n","  – Subidos: 8192/12871\n","  – Subidos: 9216/12871\n","  – Subidos: 10240/12871\n","  – Subidos: 11264/12871\n","  – Subidos: 12288/12871\n","  – Subidos: 12871/12871\n","[OK] Carga completada.\n","\n","Proceso finalizado. Se intentaron subir 12871 puntos a 'nhtsa_recalls'.\n"]}]},{"cell_type":"markdown","source":["5) Subir investigations (múltiples partes invest_embeddings_partXXX.npy)"],"metadata":{"id":"-631_4yLPUIN"}},{"cell_type":"code","source":["from pathlib import Path\n","import numpy as np, pandas as pd, json, re, os\n","\n","BASE = Path(\"/content/drive/MyDrive/NHTSA\")\n","INV_DIR = BASE / \"embeddings\" / \"investigations_e5_mlg_instruct\"\n","PROC    = BASE / \"processed\"\n","\n","CANDIDATE_META = [\n","    INV_DIR / \"invest_chunks_meta.parquet\",         # meta guardado junto a embeddings\n","    PROC    / \"investigations_chunks.parquet\",      # meta de la fase \"processed\"\n","]\n","\n","CANDIDATE_EMB = {\n","    \"single\": INV_DIR / \"invest_embeddings.npy\",    # corrida “compacta”\n","    \"shards\": sorted(INV_DIR.glob(\"invest_embeddings_part*.npy\")),  # corrida “grande”\n","}\n","\n","CHKPT = BASE / \"embeddings\" / \"qdrant_upload_checkpoint.json\"\n","\n","def count_meta(meta_path):\n","    try:\n","        df = pd.read_parquet(meta_path)\n","        return len(df)\n","    except Exception as e:\n","        return None\n","\n","def count_emb_single(npy):\n","    try:\n","        arr = np.load(npy, mmap_mode=\"r\")\n","        return arr.shape\n","    except Exception:\n","        return None\n","\n","def count_emb_shards(parts):\n","    total = 0\n","    dimset = set()\n","    for p in parts:\n","        arr = np.load(p, mmap_mode=\"r\")\n","        total += arr.shape[0]\n","        dimset.add(arr.shape[1])\n","    return (total, dimset)\n","\n","print(\"== META candidates ==\")\n","meta_info = []\n","for mp in CANDIDATE_META:\n","    n = count_meta(mp) if mp.exists() else None\n","    print(f\"  {mp}: {'NO' if n is None else n}\")\n","    if n is not None: meta_info.append((mp, n))\n","\n","print(\"\\n== EMBEDDINGS candidates ==\")\n","emb_info = []\n","if CANDIDATE_EMB[\"single\"].exists():\n","    s = count_emb_single(CANDIDATE_EMB[\"single\"])\n","    print(f\"  single {CANDIDATE_EMB['single'].name}: {s}\")\n","    if s: emb_info.append((\"single\", CANDIDATE_EMB[\"single\"], s[0], s[1]))\n","else:\n","    print(\"  single: NO\")\n","\n","if CANDIDATE_EMB[\"shards\"]:\n","    tot, dims = count_emb_shards(CANDIDATE_EMB[\"shards\"])\n","    print(f\"  shards x{len(CANDIDATE_EMB['shards'])}: total={tot}, dims={dims}\")\n","    emb_info.append((\"shards\", CANDIDATE_EMB[\"shards\"], tot, list(dims)[0] if len(dims)==1 else None))\n","else:\n","    print(\"  shards: NO\")\n","\n","# Emparejar por conteo\n","matches = []\n","for mp, nmeta in meta_info:\n","    for kind, emb, nvec, dim in emb_info:\n","        if nmeta == nvec:\n","            matches.append((mp, kind, emb, nmeta, dim))\n","\n","print(\"\\n== MATCHES por conteo (meta filas == embeddings vectores) ==\")\n","if matches:\n","    for mp, kind, emb, nmeta, dim in matches:\n","        print(f\"  ✅ {mp.name}  ↔  {kind} ({nmeta} vectores, dim={dim})\")\n","else:\n","    print(\"  ❌ Ninguno. Meta y embeddings vienen de corridas distintas.\")\n","\n","# Limpia checkpoint para evitar cursores adelantados\n","if CHKPT.exists():\n","    print(\"\\n[i] Borrando checkpoint obsoleto:\", CHKPT)\n","    CHKPT.unlink()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lc-LOlJbPWag","executionInfo":{"status":"ok","timestamp":1760304024328,"user_tz":360,"elapsed":2055,"user":{"displayName":"Lucero Guadalupe Contreras Hernandez","userId":"06647893651071570506"}},"outputId":"6fb4dbc8-03b0-45db-9b6c-9dff8f0d678c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["== META candidates ==\n","  /content/drive/MyDrive/NHTSA/embeddings/investigations_e5_mlg_instruct/invest_chunks_meta.parquet: 6354\n","  /content/drive/MyDrive/NHTSA/processed/investigations_chunks.parquet: 6354\n","\n","== EMBEDDINGS candidates ==\n","  single invest_embeddings.npy: (6354, 1024)\n","  shards x8: total=356139, dims={1024}\n","\n","== MATCHES por conteo (meta filas == embeddings vectores) ==\n","  ✅ invest_chunks_meta.parquet  ↔  single (6354 vectores, dim=1024)\n","  ✅ investigations_chunks.parquet  ↔  single (6354 vectores, dim=1024)\n"]}]},{"cell_type":"code","source":["# ====== Subida INVESTIGATIONS → Qdrant (colección: nhtsa_investigations) ======\n","!pip -q install qdrant-client==1.9.2 pyarrow==17.0.0 pandas==2.2.2 numpy==1.26.4\n","\n","from pathlib import Path\n","import json\n","import numpy as np, pandas as pd\n","from qdrant_client import QdrantClient, models\n","\n","# --- Config Qdrant (asegúrate de tener definidas QDRANT_URL y QDRANT_API_KEY arriba) ---\n","INV_COLLECTION = \"nhtsa_investigations\"\n","\n","# --- Rutas ---\n","BASE   = Path(\"/content/drive/MyDrive/NHTSA\")\n","INVDIR = BASE / \"embeddings\" / \"investigations_e5_mlg_instruct\"\n","META   = INVDIR / \"invest_chunks_meta.parquet\"   # 6,354 filas\n","NPY    = INVDIR / \"invest_embeddings.npy\"        # (6354, 1024)\n","\n","# --- Helpers ---\n","def collection_exists(client: QdrantClient, name: str) -> bool:\n","    # Algunos SDKs viejos no traen .collection_exists(); este helper es compatible\n","    try:\n","        client.get_collection(name)\n","        return True\n","    except Exception:\n","        return False\n","\n","def ensure_collection(client: QdrantClient, collection_name: str, dim: int, distance=models.Distance.COSINE):\n","    \"\"\"\n","    Crea la colección si no existe o la recrea si la dimensión no coincide.\n","    \"\"\"\n","    if not collection_exists(client, collection_name):\n","        print(f\"[i] Creando colección '{collection_name}' (dim={dim}) ...\")\n","        client.create_collection(\n","            collection_name=collection_name,\n","            vectors_config=models.VectorParams(size=dim, distance=distance),\n","        )\n","        return True  # creada nueva\n","    # Existe: validamos dimensión\n","    cfg = client.get_collection(collection_name).config\n","    current_dim = cfg.params.vectors.size if hasattr(cfg.params.vectors, \"size\") else cfg.params.vectors.get(\"size\")\n","    if current_dim != dim:\n","        print(f\"[!] Dimensión distinta ({current_dim} vs {dim}). Recreando '{collection_name}'...\")\n","        client.delete_collection(collection_name)\n","        client.create_collection(\n","            collection_name=collection_name,\n","            vectors_config=models.VectorParams(size=dim, distance=distance),\n","        )\n","        return True  # recreada\n","    print(f\"[OK] Colección '{collection_name}' existe con dim={dim}.\")\n","    return False  # no recreada\n","\n","def df_to_payloads(df: pd.DataFrame):\n","    clean = df.where(pd.notnull(df), None)\n","    def nat(v):\n","        return v.item() if isinstance(v, np.generic) else v\n","    return [{k: nat(v) for k, v in row.items()} for row in clean.to_dict(\"records\")]\n","\n","def upsert_batches(client, collection, vecs_np, meta_df, id_offset=0, batch_size=512):\n","    \"\"\"\n","    Sube vectors+payloads en lotes. Lote 512 recomendado para Qdrant Cloud free.\n","    \"\"\"\n","    n = vecs_np.shape[0]\n","    i = 0\n","    print(f\"[i] Subiendo {n} vectores (dim={vecs_np.shape[1]}) a '{collection}' en lotes de {batch_size} ...\")\n","    while i < n:\n","        j = min(i + batch_size, n)\n","        client.upsert(\n","            collection_name=collection,\n","            points=models.Batch(\n","                ids=list(range(id_offset + i, id_offset + j)),\n","                vectors=vecs_np[i:j].astype(\"float32\", copy=False).tolist(),\n","                payloads=df_to_payloads(meta_df.iloc[i:j].reset_index(drop=True)),\n","            ),\n","            wait=True,\n","        )\n","        i = j\n","        print(f\"  – Subidos {i}/{n}\")\n","    print(\"[OK] Carga completada.\")\n","\n","# --- Carga y checks previos ---\n","meta = pd.read_parquet(META)\n","vecs = np.load(NPY)\n","\n","print(\"--- Verificación local ---\")\n","print(\"Embeddings:\", vecs.shape, \"| Meta filas:\", len(meta))\n","assert vecs.shape == (6354, 1024), f\"Embeddings con forma inesperada: {vecs.shape}\"\n","assert len(meta) == 6354, f\"Metadatos con filas inesperadas: {len(meta)}\"\n","print(\"--------------------------\")\n","\n","# --- Cliente Qdrant ---\n","client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, timeout=180)\n","\n","# --- Asegurar colección con dimensión correcta ---\n","recreated = ensure_collection(client, INV_COLLECTION, dim=vecs.shape[1], distance=models.Distance.COSINE)\n","\n","# Si quieres forzar carga \"desde cero\", asegurarte de no reusar IDs:\n","# Si la colección existía y no se recreó, calculamos el siguiente ID por si ya hay datos\n","try:\n","    current_count = client.count(INV_COLLECTION, exact=True).count\n","except Exception:\n","    current_count = 0\n","\n","# Para esta subida quieres empezar desde 0 y sobrescribir completamente:\n","if not recreated and current_count > 0:\n","    print(f\"[i] La colección tenía {current_count} puntos. Se borrará para recargar limpia.\")\n","    client.delete_collection(INV_COLLECTION)\n","    client.create_collection(\n","        collection_name=INV_COLLECTION,\n","        vectors_config=models.VectorParams(size=vecs.shape[1], distance=models.Distance.COSINE),\n","    )\n","    current_count = 0\n","\n","# --- Subida ---\n","upsert_batches(client, INV_COLLECTION, vecs, meta, id_offset=0, batch_size=512)\n","\n","# --- Verificación post-carga ---\n","cnt = client.count(INV_COLLECTION, exact=True).count\n","print(f\"[✓] Conteo exacto en '{INV_COLLECTION}':\", cnt)\n","\n","# --- Muestra 3 puntos (scroll) ---\n","scrolled, next_page = client.scroll(INV_COLLECTION, limit=3, with_payload=True, with_vectors=False)\n","print(\"[Vista rápida de payloads]\")\n","for p in scrolled:\n","    print(\"-\", p.id, {k: p.payload.get(k) for k in list(p.payload.keys())[:5]})  # primeras 5 claves de cada payload\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aTWEp_lATPmP","executionInfo":{"status":"ok","timestamp":1760318187953,"user_tz":360,"elapsed":32076,"user":{"displayName":"Lucero Guadalupe Contreras Hernández","userId":"07832692706572343340"}},"outputId":"8a1a9ab5-a926-4774-e330-0323dc0880d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.1/230.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.1 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.32.1 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.1 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m--- Verificación local ---\n","Embeddings: (6354, 1024) | Meta filas: 6354\n","--------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2018508940.py:91: UserWarning: Qdrant client version 1.9.2 is incompatible with server version 1.15.5. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n","  client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, timeout=180)\n"]},{"output_type":"stream","name":"stdout","text":["[i] Creando colección 'nhtsa_investigations' (dim=1024) ...\n","[i] Subiendo 6354 vectores (dim=1024) a 'nhtsa_investigations' en lotes de 512 ...\n","  – Subidos 512/6354\n","  – Subidos 1024/6354\n","  – Subidos 1536/6354\n","  – Subidos 2048/6354\n","  – Subidos 2560/6354\n","  – Subidos 3072/6354\n","  – Subidos 3584/6354\n","  – Subidos 4096/6354\n","  – Subidos 4608/6354\n","  – Subidos 5120/6354\n","  – Subidos 5632/6354\n","  – Subidos 6144/6354\n","  – Subidos 6354/6354\n","[OK] Carga completada.\n","[✓] Conteo exacto en 'nhtsa_investigations': 6354\n","[Vista rápida de payloads]\n","- 0 {'chunk_id': 'AQ08001::ch0', 'id': 'AQ08001', 'make': 'PACE AMERICAN', 'model': 'intfloat/multilingual-e5-large-instruct', 'year': None}\n","- 1 {'chunk_id': 'AQ09001::ch0', 'id': 'AQ09001', 'make': 'CAPCEN', 'model': 'intfloat/multilingual-e5-large-instruct', 'year': None}\n","- 2 {'chunk_id': 'AQ09002::ch0', 'id': 'AQ09002', 'make': 'HOLIDAY RAMBLER', 'model': 'intfloat/multilingual-e5-large-instruct', 'year': None}\n"]}]},{"cell_type":"code","source":["import pandas as pd, pathlib as pl\n","csv = pl.Path(\"/content/drive/MyDrive/NHTSA/embeddings/complaints_e5_mlg_instruct/reduced_shards/kmeans_global/representantes_global_kmeans.csv\")\n","reps = pd.read_csv(csv)\n","len(reps), reps.head(3)\n"],"metadata":{"id":"Jhf7kIJjTQAA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760312421901,"user_tz":360,"elapsed":2615,"user":{"displayName":"Lucero Guadalupe Contreras Hernández","userId":"07832692706572343340"}},"outputId":"c01fc1cb-133e-4893-ebee-d48f6781e4e2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20000,\n","       id   make  model    year  \\\n"," 0   9364  DODGE   COLT  1993.0   \n"," 1  14695  BUICK  BUICK  1990.0   \n"," 2  15395  LEXUS  ES300  1995.0   \n"," \n","                                          component                   _h  \\\n"," 0  SERVICE BRAKES, HYDRAULIC:FOUNDATION COMPONENTS -7992261938585274089   \n"," 1                                            TIRES  1890294604798842119   \n"," 2                 SEAT BELTS:FRONT:BUCKLE ASSEMBLY -1669910696480997720   \n"," \n","    shard_idx  row_in_shard  cluster     dist2  \n"," 0          0          9363        0  0.000000  \n"," 1          0         14694        1  0.000000  \n"," 2          0         15394        2  0.017362  )"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# =======================================================\n","# Qdrant — Upload de representantes (complaints → vectors)\n","# con IDs UUID determinísticos (uuid5)\n","# (colección destino: nhtsa_complaints)\n","# =======================================================\n","!pip -q install qdrant-client==1.9.2 pyarrow==17.0.0 pandas==2.2.2 numpy==1.26.4\n","\n","from pathlib import Path\n","import numpy as np, pandas as pd, json, re, gc, uuid\n","from qdrant_client import QdrantClient, models\n","from tqdm import tqdm\n","\n","# ------------- Rutas (ajusta si difieren) ----------\n","BASE     = Path(\"/content/drive/MyDrive/NHTSA\")\n","REDUCED  = BASE / \"embeddings\" / \"complaints_e5_mlg_instruct\" / \"reduced_shards\"\n","CSV_REPS = REDUCED / \"kmeans_global\" / \"representantes_global_kmeans.csv\"\n","if not CSV_REPS.exists():\n","    CSV_REPS = REDUCED / \"representantes_global.csv\"   # fallback\n","\n","# ------------- Parámetros de carga -------------------\n","UPSERT_BATCH = 2000         # tamaño de batch para upsert (ajusta si tu free tier se ahoga)\n","SNIPPET_LEN  = 0            # 0 = no subir snippet; >0 = longitud máxima de snippet\n","\n","# ------------- Colección destino ---------------------\n","CPL_COLLECTION = \"nhtsa_complaints\"\n","\n","# ============ Helpers Qdrant ============\n","def collection_exists(client: QdrantClient, name: str) -> bool:\n","    \"\"\"Compat: algunos SDKs no exponen collection_exists().\"\"\"\n","    try:\n","        client.get_collection(name)\n","        return True\n","    except Exception:\n","        return False\n","\n","def ensure_collection(client: QdrantClient, collection_name: str, dim: int, distance=models.Distance.COSINE):\n","    \"\"\"Crea o valida colección con dimensión adecuada (recrea si no coincide).\"\"\"\n","    if not collection_exists(client, collection_name):\n","        client.create_collection(\n","            collection_name=collection_name,\n","            vectors_config=models.VectorParams(size=dim, distance=distance),\n","        )\n","    else:\n","        cfg = client.get_collection(collection_name).config\n","        current_dim = cfg.params.vectors.size if hasattr(cfg.params.vectors, \"size\") else cfg.params.vectors.get(\"size\")\n","        if current_dim != dim:\n","            client.delete_collection(collection_name)\n","            client.create_collection(\n","                collection_name=collection_name,\n","                vectors_config=models.VectorParams(size=dim, distance=distance),\n","            )\n","\n","def df_to_payloads(df: pd.DataFrame):\n","    \"\"\"Convierte DataFrame → lista de dict con tipos nativos.\"\"\"\n","    clean = df.where(pd.notnull(df), None)\n","    import numpy as _np\n","    def nat(v): return v.item() if isinstance(v, _np.generic) else v\n","    return [{k: nat(v) for k,v in r.items()} for r in clean.to_dict(\"records\")]\n","\n","def upsert_batches(client, collection, vecs_np, meta_df, ids, batch_size=UPSERT_BATCH):\n","    \"\"\"Upsert por lotes. ids puede ser lista de UUID (string) o ints.\"\"\"\n","    n = vecs_np.shape[0]; i = 0\n","    while i < n:\n","        j = min(i+batch_size, n)\n","        client.upsert(\n","            collection_name=collection,\n","            points=models.Batch(\n","                ids=ids[i:j],\n","                vectors=vecs_np[i:j].astype(\"float32\").tolist(),\n","                payloads=df_to_payloads(meta_df.iloc[i:j].reset_index(drop=True))\n","            ),\n","            wait=True\n","        )\n","        i = j\n","        print(f\"  – Subidos {i}/{n}\")\n","\n","# --------- Detección de columnas en reps ---------\n","def detect_rep_columns(df: pd.DataFrame):\n","    cols = {c.lower(): c for c in df.columns}\n","    def pick(cands):\n","        for c in cands:\n","            if c in cols: return cols[c]\n","        return None\n","    mapping = {\n","        \"id\":        pick([\"id\",\"odinumber\",\"cmplid\",\"qid\",\"qdrant_id\"]),\n","        \"make\":      pick([\"make\",\"maketxt\"]),\n","        \"model\":     pick([\"model\",\"modeltxt\"]),\n","        \"year\":      pick([\"year\",\"yeartxt\",\"model_year\"]),\n","        \"component\": pick([\"component\",\"compdesc\",\"compname\"]),\n","        \"hash\":      pick([\"_h\",\"hash\",\"text_hash\"]),\n","        \"shard\":     pick([\"shard_idx\",\"shard\",\"reduced_shard\",\"sidx\"]),\n","        \"row\":       pick([\"row_in_shard\",\"local_idx\",\"offset\",\"row\",\"rid\"]),\n","        \"cluster\":   pick([\"cluster\"]),\n","        \"dist2\":     pick([\"dist2\",\"distance\",\"dist\"]),\n","    }\n","    # Intentar inferir shard/row desde id estilo rep_0005_012345\n","    if mapping[\"shard\"] is None or mapping[\"row\"] is None:\n","        idc = mapping[\"id\"]\n","        if idc:\n","            m = df[idc].astype(str).str.extract(r\"rep_(\\d{4})_(\\d+)\", expand=True)\n","            if m.notna().any().any():\n","                if mapping[\"shard\"] is None:\n","                    df[\"__infer_shard\"] = pd.to_numeric(m[0], errors=\"coerce\").astype(\"Int64\")\n","                    mapping[\"shard\"] = \"__infer_shard\"\n","                if mapping[\"row\"] is None:\n","                    df[\"__infer_row\"]   = pd.to_numeric(m[1], errors=\"coerce\").astype(\"Int64\")\n","                    mapping[\"row\"] = \"__infer_row\"\n","    assert mapping[\"shard\"] is not None, \"CSV reps no tiene 'shard_idx' (ni pude inferirlo de 'id').\"\n","    assert mapping[\"row\"]   is not None, \"CSV reps no tiene 'row_in_shard' (ni pude inferirlo de 'id').\"\n","    return mapping\n","\n","# --------- Carga reps + vectores por shard ----------\n","def load_reps_and_vectors(csv_reps: Path, reduced_dir: Path):\n","    assert csv_reps.exists(), f\"No existe {csv_reps}\"\n","    reps = pd.read_csv(csv_reps)\n","    mapping = detect_rep_columns(reps)\n","\n","    # Normaliza tipos mínimos\n","    reps[\"__shard_idx__\"] = pd.to_numeric(reps[mapping[\"shard\"]], errors=\"coerce\").astype(\"Int64\")\n","    reps[\"__row_in__\"]    = pd.to_numeric(reps[mapping[\"row\"]], errors=\"coerce\").astype(\"Int64\")\n","    reps = reps[reps[\"__shard_idx__\"].notna() & reps[\"__row_in__\"].notna()].copy()\n","    reps[\"__shard_idx__\"] = reps[\"__shard_idx__\"].astype(int)\n","    reps[\"__row_in__\"]    = reps[\"__row_in__\"].astype(int)\n","\n","    # ID estable si falta\n","    if mapping[\"id\"] is None:\n","        reps[\"__id__\"] = reps.apply(lambda r: f\"rep_{int(r['__shard_idx__']):04d}_{int(r['__row_in__']):06d}\", axis=1)\n","        mapping[\"id\"] = \"__id__\"\n","\n","    # Cache de vectores por shard y dimensión\n","    vec_cache = {}\n","    dim = None\n","    needed = sorted(reps[\"__shard_idx__\"].unique().tolist())\n","    for s in needed:\n","        f = reduced_dir / f\"reduced_{s:04d}.npy\"\n","        assert f.exists(), f\"Falta vector reducido: {f}\"\n","        arr = np.load(f, mmap_mode=\"r\")  # float32\n","        vec_cache[s] = arr\n","        if dim is None:\n","            dim = int(arr.shape[1])\n","    return reps, mapping, vec_cache, dim\n","\n","# ---------- Snippets opcionales (desactivado por defecto) ----------\n","def build_payload_df(reps: pd.DataFrame, mapping: dict) -> pd.DataFrame:\n","    cols = {}\n","    for k in [\"id\",\"make\",\"model\",\"year\",\"component\",\"hash\",\"cluster\",\"dist2\"]:\n","        c = mapping.get(k)\n","        if c and c in reps.columns:\n","            cols[k] = c\n","    out = pd.DataFrame({\n","        \"id\": reps[cols.get(\"id\")].astype(str) if \"id\" in cols else reps.index.astype(str),\n","        \"make\": reps[cols[\"make\"]].astype(str).str.strip() if \"make\" in cols else None,\n","        \"model\": reps[cols[\"model\"]].astype(str).str.strip() if \"model\" in cols else None,\n","        \"year\": pd.to_numeric(reps[cols[\"year\"]], errors=\"coerce\").astype(\"Int64\") if \"year\" in cols else None,\n","        \"component\": reps[cols[\"component\"]].astype(str).str.strip() if \"component\" in cols else None,\n","        \"_h\": reps[cols[\"hash\"]].astype(str) if \"hash\" in cols else None,\n","        \"shard_idx\": reps[\"__shard_idx__\"].astype(int),\n","        \"row_in_shard\": reps[\"__row_in__\"].astype(int),\n","    })\n","    if \"cluster\" in cols:\n","        out[\"cluster\"] = pd.to_numeric(reps[cols[\"cluster\"]], errors=\"coerce\").astype(\"Int64\")\n","    if \"dist2\" in cols:\n","        out[\"dist2\"] = pd.to_numeric(reps[cols[\"dist2\"]], errors=\"coerce\")\n","    return out\n","\n","# ---------- IDs → UUID determinísticos ----------\n","NAMESPACE = uuid.uuid5(uuid.NAMESPACE_URL, \"https://nhtsa.example/complaints\")  # namespace fijo del proyecto\n","\n","def make_point_uuid(shard_idx: int, row_in_shard: int) -> str:\n","    \"\"\"Convierte 'rep_{shard}_{row}' → UUID v5 determinístico.\"\"\"\n","    raw = f\"rep_{int(shard_idx):04d}_{int(row_in_shard):06d}\"\n","    return str(uuid.uuid5(NAMESPACE, raw))\n","\n","# ====================== PIPELINE ======================\n","# 1) Cargar reps + vectores por shard\n","reps, mapping, vec_cache, dim = load_reps_and_vectors(CSV_REPS, REDUCED)\n","print(f\"[i] Representantes: {len(reps):,} | dim={dim} | shards={sorted(reps['__shard_idx__'].unique())[:10]}...\")\n","\n","# 2) Preparar cliente y colección (compat check OFF por warning de versión)\n","client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, timeout=180, check_compatibility=False)\n","ensure_collection(client, CPL_COLLECTION, dim=dim, distance=models.Distance.COSINE)\n","\n","# 3) Payload base (ligero)\n","payload_df = build_payload_df(reps, mapping)\n","\n","# 4) Upsert por shard (UUIDs determinísticos)\n","total = len(reps)\n","done = 0\n","for sidx, block in reps.groupby(\"__shard_idx__\"):\n","    arr = vec_cache[sidx]  # [n, dim]\n","    local_rows = block[\"__row_in__\"].tolist()\n","    block_payload = payload_df.loc[block.index].reset_index(drop=True)\n","\n","    # IDs UUID determinísticos\n","    point_ids = [make_point_uuid(sidx, r) for r in local_rows]\n","\n","    # vectores (en el mismo orden)\n","    vecs = np.stack([arr[r] for r in local_rows]).astype(\"float32\", copy=False)\n","\n","    print(f\"[i] Subiendo shard {sidx:04d}: {len(block)} puntos …\")\n","    upsert_batches(client, CPL_COLLECTION, vecs, block_payload, point_ids, batch_size=UPSERT_BATCH)\n","    done += len(block)\n","    print(f\"    – Progreso: {done}/{total}\")\n","\n","print(\"[OK] Complaints (representantes) cargados en Qdrant → colección:\", CPL_COLLECTION)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cuTe9TvHy-Mo","executionInfo":{"status":"ok","timestamp":1760318308849,"user_tz":360,"elapsed":25956,"user":{"displayName":"Lucero Guadalupe Contreras Hernández","userId":"07832692706572343340"}},"outputId":"669fe885-dc88-42e3-bfb4-f3258d1d126e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[i] Representantes: 20,000 | dim=256 | shards=[np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9)]...\n","[i] Subiendo shard 0000: 12755 puntos …\n","  – Subidos 2000/12755\n","  – Subidos 4000/12755\n","  – Subidos 6000/12755\n","  – Subidos 8000/12755\n","  – Subidos 10000/12755\n","  – Subidos 12000/12755\n","  – Subidos 12755/12755\n","    – Progreso: 12755/20000\n","[i] Subiendo shard 0001: 1989 puntos …\n","  – Subidos 1989/1989\n","    – Progreso: 14744/20000\n","[i] Subiendo shard 0002: 997 puntos …\n","  – Subidos 997/997\n","    – Progreso: 15741/20000\n","[i] Subiendo shard 0003: 563 puntos …\n","  – Subidos 563/563\n","    – Progreso: 16304/20000\n","[i] Subiendo shard 0004: 458 puntos …\n","  – Subidos 458/458\n","    – Progreso: 16762/20000\n","[i] Subiendo shard 0005: 366 puntos …\n","  – Subidos 366/366\n","    – Progreso: 17128/20000\n","[i] Subiendo shard 0006: 396 puntos …\n","  – Subidos 396/396\n","    – Progreso: 17524/20000\n","[i] Subiendo shard 0007: 364 puntos …\n","  – Subidos 364/364\n","    – Progreso: 17888/20000\n","[i] Subiendo shard 0008: 280 puntos …\n","  – Subidos 280/280\n","    – Progreso: 18168/20000\n","[i] Subiendo shard 0009: 121 puntos …\n","  – Subidos 121/121\n","    – Progreso: 18289/20000\n","[i] Subiendo shard 0010: 163 puntos …\n","  – Subidos 163/163\n","    – Progreso: 18452/20000\n","[i] Subiendo shard 0011: 161 puntos …\n","  – Subidos 161/161\n","    – Progreso: 18613/20000\n","[i] Subiendo shard 0012: 167 puntos …\n","  – Subidos 167/167\n","    – Progreso: 18780/20000\n","[i] Subiendo shard 0013: 178 puntos …\n","  – Subidos 178/178\n","    – Progreso: 18958/20000\n","[i] Subiendo shard 0014: 173 puntos …\n","  – Subidos 173/173\n","    – Progreso: 19131/20000\n","[i] Subiendo shard 0015: 156 puntos …\n","  – Subidos 156/156\n","    – Progreso: 19287/20000\n","[i] Subiendo shard 0016: 176 puntos …\n","  – Subidos 176/176\n","    – Progreso: 19463/20000\n","[i] Subiendo shard 0017: 144 puntos …\n","  – Subidos 144/144\n","    – Progreso: 19607/20000\n","[i] Subiendo shard 0018: 142 puntos …\n","  – Subidos 142/142\n","    – Progreso: 19749/20000\n","[i] Subiendo shard 0019: 124 puntos …\n","  – Subidos 124/124\n","    – Progreso: 19873/20000\n","[i] Subiendo shard 0020: 127 puntos …\n","  – Subidos 127/127\n","    – Progreso: 20000/20000\n","[OK] Complaints (representantes) cargados en Qdrant → colección: nhtsa_complaints\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"l7z_W7IP1sg_"},"execution_count":null,"outputs":[]}]}