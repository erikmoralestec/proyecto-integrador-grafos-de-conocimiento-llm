{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Documentación técnica — Etapa “Recalls → Limpieza + Export + Ingesta Neo4j (Aura)”\n",
        "\n",
        "> Esta etapa prepara el dataset de **recalls NHTSA** desde la fuente “flat” comprimida, aplica la **misma limpieza y controles** del EDA original (Plan A/B, mapeo Appendix A), **descompone la jerarquía de componentes**, y lo **ingesta** en un **grafo único** en Neo4j Aura con relaciones y jerarquía de `Component`.\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Entradas / Salidas\n",
        "\n",
        "**Entradas**\n",
        "\n",
        "* `drive/MyDrive/NHTSA/source/FLAT_RCL_POST_2010.zip`\n",
        "  Contiene el TXT tab-delimited con los recalls.\n",
        "\n",
        "**Artefactos intermedios**\n",
        "\n",
        "* `drive/MyDrive/NHTSA/processed/recalls_raw_enriched.csv` (dump tras limpieza y tipificación)\n",
        "* `drive/MyDrive/NHTSA/processed/recalls_by_campaign.csv` (agregado por campaña)\n",
        "\n",
        "**Salida para ingesta (definitiva)**\n",
        "\n",
        "* `drive/MyDrive/NHTSA/neo4j/exports/recalls_neo4j_ready.csv`\n",
        "\n",
        "**Resultado en Neo4j Aura (conteos observados)**\n",
        "\n",
        "* `(:Recall)` = **14,340** nodos\n",
        "* `(:Component)` = **509** nodos\n",
        "* Jerarquía `(:Component)-[:SUB_OF]->(:Component)` creada (hasta 5 niveles)\n",
        "* Enlaces `(:Recall)-[:MENTIONS]->(:Component)` al nivel **más específico disponible**\n",
        "* Enlaces `(:Recall)-[:OF_MAKE]->(:Make)` y `(:Recall)-[:OF_MODEL]->(:Model)`\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Lectura robusta (Plan A/B) y mapeo Appendix A\n",
        "\n",
        "La fuente “flat” presenta **inconsistencias de comillas** en narrativas largas. Se implementó un lector **Plan A/B**:\n",
        "\n",
        "* **Plan A (fiel):** `quotechar='\"'` y `csv.QUOTE_MINIMAL`\n",
        "  Falló con el error clásico: `' \\t ' expected after '\"'` → comillas no escapadas.\n",
        "* **Plan B (tolerante):** `quoting=csv.QUOTE_NONE`, `on_bad_lines=\"warn\"`\n",
        "  Éxito: **222,573 filas**, **29 columnas**.\n",
        "\n",
        "Se mapean **estrictamente** las primeras **24 columnas** al **Appendix A**:\n",
        "\n",
        "```text\n",
        "[1..24] → ['RECORD_ID','CAMPNO','MAKETXT','MODELTXT','YEARTXT','MFGCAMPNO','COMPNAME','MFGNAME',\n",
        "           'BGMAN','ENDMAN','RCLTYPECD','POTAFF','ODATE','INFLUENCED_BY','MFGTXT','RCDATE',\n",
        "           'DATEA','RPNO','FMVSS','DESC_DEFECT','CONEQUENCE_DEFECT','CORRECTIVE_ACTION',\n",
        "           'NOTES','RCL_CMPT_ID']\n",
        "```\n",
        "\n",
        "* Columnas extra ≥25 se preservan con prefijo `EXTRA_XX` para **trazabilidad**.\n",
        "* Se corrige el **typo** `CONEQUENCE_DEFECT` → `CONSEQUENCE_DEFECT`.\n",
        "\n",
        "**Rationale**\n",
        "\n",
        "* Mantener la semántica original de NHTSA y reproducibilidad.\n",
        "* Evitar “adivinar” columnas; el mapeo es **determinista** y auditable.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Normalización y tipificación\n",
        "\n",
        "* `YEARTXT`: entero **[1950, 2035]**; `9999` → `NaN`.\n",
        "* Fechas: `ODATE`, `RCDATE`, `DATEA`, `BGMAN`, `ENDMAN` → `datetime` con formato `%Y%m%d`.\n",
        "* `POTAFF` → `POTAFF_num` (numérico auxiliar).\n",
        "* Narrativas (`DESC_DEFECT`, `CONSEQUENCE_DEFECT`, `CORRECTIVE_ACTION`, `NOTES`):\n",
        "\n",
        "  * **Limpieza superficial de HTML**, normalización de espacios.\n",
        "  * Se calculan longitudes (`len_*`) como métricas de calidad.\n",
        "\n",
        "**Rationale**\n",
        "\n",
        "* Rango de años evita outliers y años centinela.\n",
        "* Limpieza de HTML reduce ruido para posteriores embeddings y búsqueda textual.\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Jerarquía de componentes\n",
        "\n",
        "* Campo `COMPNAME` se **descompone por `:`** en **hasta 5 niveles**: `COMP_L1..COMP_L5`.\n",
        "* Se eliminan literales vacíos y “`NONE`”.\n",
        "\n",
        "**Rationale**\n",
        "\n",
        "* La jerarquía soporta consultas semánticas por **nivel de granularidad** y habilita la relación `SUB_OF` entre componentes.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) De-duplicación por campaña y agregado\n",
        "\n",
        "* **De-dup:** Se ordena por (`CAMPNO`, `RCDATE`) y se conserva el **registro más reciente** por campaña.\n",
        "* **Agregado por `CAMPNO`:**\n",
        "\n",
        "  * Métricas agregadas:\n",
        "    `POTAFF_num=max`, fechas (`RCDATE/ODATE/DATEA`) mínimas, `ENDMAN` máxima, `YEARTXT=max`\n",
        "    Atributos descriptivos: `MAKETXT/MODELTXT/COMPNAME/COMP_L*` → `first`\n",
        "* Resultado: `recalls_by_campaign.csv`\n",
        "\n",
        "**Rationale**\n",
        "\n",
        "* Evita **sobrecontar** campañas con múltiples filas.\n",
        "* `max` de `POTAFF_num` y fechas mín/max preservan una vista **conservadora** y coherente.\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Controles de calidad (observados en esta corrida)\n",
        "\n",
        "* **Cobertura de fechas (por campaña)**\n",
        "  `RCDATE`: **100.0%** | `ODATE`: **97.75%** | `DATEA`: **100%**\n",
        "* **Narrativas (crudo)**\n",
        "  `DESC_DEFECT/CONSEQUENCE_DEFECT/CORRECTIVE_ACTION`: **100%** | `NOTES`: **97.3%**\n",
        "* **POTAFF**\n",
        "  `min=0`, `max=17,600,000`, `mediana=466`, `≤0`: **6** campañas\n",
        "* **YEARTXT**\n",
        "  Rango **[1976, 2026]**\n",
        "\n",
        "**Rationale**\n",
        "\n",
        "* Estos checks replican los del EDA original, garantizando **consistencia** y **calidad mínima** antes del grafo.\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Export “listo para Aura”\n",
        "\n",
        "Se genera `neo4j/exports/recalls_neo4j_ready.csv` con columnas:\n",
        "\n",
        "```text\n",
        "['campaign_no','make','model','year','component','recall_date',\n",
        " 'subject','consequence','comp_l1','comp_l2','comp_l3','comp_l4','comp_l5']\n",
        "```\n",
        "\n",
        "* `campaign_no` = `CAMPNO`\n",
        "* `make/model` = `MAKETXT/MODELTXT` normalizados (uppercase, espacios compactados)\n",
        "* `year` = `YEARTXT` (nullable)\n",
        "* `component` = original (`COMPNAME`) si existe; fallback a `comp_l1`\n",
        "* `recall_date` = ISO `YYYY-MM-DD` (string; vacío si no parseable)\n",
        "* `comp_l*` = jerarquía por `:`\n",
        "\n",
        "**Decisión importante:** **no se exige `year`** en el filtro mínimo del export para **no vaciar** el dataset en casos de año faltante. Se garantizan `campaign_no`, `make`, `model` y `comp_l1`.\n",
        "\n",
        "**Resultado:** **14,340** filas exportadas.\n",
        "\n",
        "---\n",
        "\n",
        "## 8) Ingesta en Neo4j Aura (constraints, índices, upsert)\n",
        "\n",
        "### 8.1 Constraints / Índices\n",
        "\n",
        "Se crean **una por una** (Aura requiere 1 statement por query):\n",
        "\n",
        "```cypher\n",
        "CREATE CONSTRAINT IF NOT EXISTS FOR (r:Recall)        REQUIRE r.id IS UNIQUE;\n",
        "CREATE CONSTRAINT IF NOT EXISTS FOR (i:Investigation) REQUIRE i.id IS UNIQUE;\n",
        "CREATE CONSTRAINT IF NOT EXISTS FOR (c:Complaint)     REQUIRE c.id IS UNIQUE;\n",
        "CREATE CONSTRAINT IF NOT EXISTS FOR (c:Component)     REQUIRE c.name IS UNIQUE;\n",
        "CREATE CONSTRAINT IF NOT EXISTS FOR (m:Make)          REQUIRE m.name IS UNIQUE;\n",
        "CREATE CONSTRAINT IF NOT EXISTS FOR (m:Model)         REQUIRE (m.name, m.make) IS UNIQUE;\n",
        "CREATE CONSTRAINT IF NOT EXISTS FOR (x:Issue)         REQUIRE x.id IS UNIQUE;\n",
        "\n",
        "CREATE INDEX comp_name_lower IF NOT EXISTS FOR (c:Component) ON (c.name_lower);\n",
        "```\n",
        "\n",
        "**Rationale**\n",
        "\n",
        "* **Unicidad** por dominio evita duplicados en upserts idempotentes.\n",
        "* Índice `name_lower` acelera comparaciones case-insensitive si se requieren.\n",
        "\n",
        "### 8.2 Cypher de upsert (compatible con Aura)\n",
        "\n",
        "Aura no permite `CALL { WITH ... WHERE ... }` para “importing WITH”. Se usa patrón `FOREACH` condicional (listas vacías) y un `CASE` para seleccionar el nivel objetivo de `MENTIONS`.\n",
        "\n",
        "```cypher\n",
        "UNWIND $rows AS row\n",
        "MERGE (r:Recall {id: row.campaign_no})\n",
        "  SET r.camp_no     = row.campaign_no,\n",
        "      r.recall_date = CASE WHEN row.recall_date = '' THEN NULL ELSE row.recall_date END,\n",
        "      r.make        = row.make,\n",
        "      r.model       = row.model,\n",
        "      r.year        = CASE WHEN row.year IS NULL OR row.year = '' THEN NULL ELSE toInteger(row.year) END,\n",
        "      r.component   = row.component,\n",
        "      r.subject     = coalesce(row.subject, ''),\n",
        "      r.consequence = coalesce(row.consequence, '')\n",
        "\n",
        "MERGE (mk:Make {name: row.make})\n",
        "MERGE (md:Model {name: row.model, make: row.make})\n",
        "MERGE (r)-[:OF_MAKE]->(mk)\n",
        "MERGE (r)-[:OF_MODEL]->(md)\n",
        "\n",
        "// Jerarquía Component\n",
        "MERGE (c1:Component {name: row.comp_l1})\n",
        "  ON CREATE SET c1.name_lower = toLower(row.comp_l1)\n",
        "  ON MATCH  SET c1.name_lower = coalesce(c1.name_lower, toLower(row.comp_l1))\n",
        "\n",
        "FOREACH (_ IN CASE WHEN row.comp_l2 <> '' THEN [1] ELSE [] END |\n",
        "  MERGE (p1:Component {name: row.comp_l1})\n",
        "  MERGE (c2:Component {name: row.comp_l2})\n",
        "    ON CREATE SET c2.name_lower = toLower(row.comp_l2)\n",
        "    ON MATCH  SET c2.name_lower = coalesce(c2.name_lower, toLower(row.comp_l2))\n",
        "  MERGE (c2)-[:SUB_OF]->(p1)\n",
        ")\n",
        "\n",
        "FOREACH (_ IN CASE WHEN row.comp_l3 <> '' AND row.comp_l2 <> '' THEN [1] ELSE [] END |\n",
        "  MERGE (p2:Component {name: row.comp_l2})\n",
        "  MERGE (c3:Component {name: row.comp_l3})\n",
        "    ON CREATE SET c3.name_lower = toLower(row.comp_l3)\n",
        "    ON MATCH  SET c3.name_lower = coalesce(c3.name_lower, toLower(row.comp_l3))\n",
        "  MERGE (c3)-[:SUB_OF]->(p2)\n",
        ")\n",
        "\n",
        "FOREACH (_ IN CASE WHEN row.comp_l4 <> '' AND row.comp_l3 <> '' THEN [1] ELSE [] END |\n",
        "  MERGE (p3:Component {name: row.comp_l3})\n",
        "  MERGE (c4:Component {name: row.comp_l4})\n",
        "    ON CREATE SET c4.name_lower = toLower(row.comp_l4)\n",
        "    ON MATCH  SET c4.name_lower = coalesce(c4.name_lower, toLower(row.comp_l4))\n",
        "  MERGE (c4)-[:SUB_OF]->(p3)\n",
        ")\n",
        "\n",
        "FOREACH (_ IN CASE WHEN row.comp_l5 <> '' AND row.comp_l4 <> '' THEN [1] ELSE [] END |\n",
        "  MERGE (p4:Component {name: row.comp_l4})\n",
        "  MERGE (c5:Component {name: row.comp_l5})\n",
        "    ON CREATE SET c5.name_lower = toLower(row.comp_l5)\n",
        "    ON MATCH  SET c5.name_lower = coalesce(c5.name_lower, toLower(row.comp_l5))\n",
        "  MERGE (c5)-[:SUB_OF]->(p4)\n",
        ")\n",
        "\n",
        "// MENTIONS: nivel más profundo disponible\n",
        "WITH r,\n",
        "     CASE\n",
        "       WHEN row.comp_l5 <> '' THEN row.comp_l5\n",
        "       WHEN row.comp_l4 <> '' THEN row.comp_l4\n",
        "       WHEN row.comp_l3 <> '' THEN row.comp_l3\n",
        "       WHEN row.comp_l2 <> '' THEN row.comp_l2\n",
        "       ELSE row.comp_l1\n",
        "     END AS target_comp\n",
        "MATCH (cx:Component {name: target_comp})\n",
        "MERGE (r)-[:MENTIONS]->(cx)\n",
        "\n",
        "RETURN count(r) AS upserted;\n",
        "```\n",
        "\n",
        "**Rationale**\n",
        "\n",
        "* `FOREACH` evita restricciones de Aura sobre `WITH` importado; patrón **100% compatible**.\n",
        "* `CASE` centraliza la elección del **nivel objetivo** para `MENTIONS`.\n",
        "* `MERGE` asegura **idempotencia** (re-ejecuciones seguras).\n",
        "\n",
        "### 8.3 Ingesta por lotes (rate-limit friendly)\n",
        "\n",
        "* Tamaño de lote usado: **400** (si aparece rate limit, bajar a **200**).\n",
        "* Proceso:\n",
        "\n",
        "  1. Test con 200 filas → verificación de conteos.\n",
        "  2. Ingesta completa.\n",
        "  3. Validaciones post-ingesta (muestras, conteos).\n",
        "\n",
        "**Conteos finales observados**\n",
        "\n",
        "* `Recalls:` **14,340**\n",
        "* `Components:` **509**\n",
        "\n",
        "---\n",
        "\n",
        "## 9) Validaciones en Neo4j (útiles)\n",
        "\n",
        "Componentes enlazados a Recalls:\n",
        "\n",
        "```cypher\n",
        "MATCH (r:Recall)-[:MENTIONS]->(c:Component)\n",
        "RETURN r.id AS campaign, c.name AS component\n",
        "LIMIT 10;\n",
        "```\n",
        "\n",
        "Jerarquía de componentes:\n",
        "\n",
        "```cypher\n",
        "MATCH (c2:Component)-[:SUB_OF]->(c1:Component)\n",
        "RETURN c1.name AS L1, collect(DISTINCT c2.name)[0..10] AS children\n",
        "LIMIT 10;\n",
        "```\n",
        "\n",
        "Recalls por componente:\n",
        "\n",
        "```cypher\n",
        "MATCH (r:Recall)-[:MENTIONS]->(c:Component {name:'POWER TRAIN'})\n",
        "RETURN count(r) AS recalls_powertrain;\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 10) Decisiones de diseño (por qué son las mejores aquí)\n",
        "\n",
        "1. **Plan A/B de lectura**\n",
        "\n",
        "   * **A** conserva fidelidad cuando hay comillas válidas; **B** garantiza **robustez** ante datos sucios sin perder filas (`on_bad_lines=\"warn\"`).\n",
        "   * Minimiza fallas del pipeline y asegura **cobertura total**.\n",
        "\n",
        "2. **Mapeo fijo (Appendix A) y preservación de extras**\n",
        "\n",
        "   * Evita heurísticas “adivinadas”; asegura **reproducibilidad** y **auditoría**.\n",
        "   * Las `EXTRA_XX` mantienen trazabilidad para etapas avanzadas.\n",
        "\n",
        "3. **Normalización conservadora**\n",
        "\n",
        "   * Rango de `YEARTXT`, limpieza HTML y parseo de fechas estandariza sin alterar semántica.\n",
        "\n",
        "4. **Jerarquía de `Component` por “:”**\n",
        "\n",
        "   * Representa la **estructura real** de NHTSA (componente/subcomponente).\n",
        "   * Habilita consultas multi-nivel y razonamiento causal.\n",
        "\n",
        "5. **De-dup + agregado por `CAMPNO`**\n",
        "\n",
        "   * Evita **duplicidades** por campaña y sobreconteos, manteniendo coherencia temporal.\n",
        "\n",
        "6. **Export con `year` no obligatorio**\n",
        "\n",
        "   * Evita **vaciar** el dataset por años faltantes; se preserva información crítica de campaña y componente.\n",
        "\n",
        "7. **Cypher sin subconsultas con `WITH WHERE`**\n",
        "\n",
        "   * Patrón `FOREACH` es **compatible con Aura**, robusto e idempotente.\n",
        "   * Mantiene la lógica de jerarquía y el enlace al nivel más profundo sin hacks.\n",
        "\n",
        "8. **Constraints/Índices**\n",
        "\n",
        "   * Integridad (unicidad) + performance razonable en **Aura Free**.\n",
        "\n",
        "9. **Ingesta por lotes**\n",
        "\n",
        "   * Respeta **rate limits** de Aura Free; balancea throughput y confiabilidad.\n",
        "\n",
        "---\n",
        "\n",
        "## 11) Reproducibilidad (checklist)\n",
        "\n",
        "* [x] `source/FLAT_RCL_POST_2010.zip` presente.\n",
        "* [x] Lectura Plan A/B ejecutada y log impreso.\n",
        "* [x] `processed/recalls_raw_enriched.csv` y `processed/recalls_by_campaign.csv` generados.\n",
        "* [x] Export final `neo4j/exports/recalls_neo4j_ready.csv` con `comp_l1..comp_l5`.\n",
        "* [x] Constraints/índices creados **uno por query**.\n",
        "* [x] Ingesta por lotes (test 200 → full).\n",
        "* [x] Conteos verificados (`Recalls`, `Components`).\n",
        "* [x] Consultas de validación de jerarquía y `MENTIONS`.\n",
        "\n",
        "---\n",
        "\n",
        "## 12) Apéndice — Snippets mínimos\n",
        "\n",
        "**Lectura Plan A/B desde ZIP (resumen):**\n",
        "\n",
        "```python\n",
        "with zipfile.ZipFile(SRC_ZIP, 'r') as zf:\n",
        "    txt = [n for n in zf.namelist() if n.lower().endswith('.txt')][0]\n",
        "    raw = io.BytesIO(zf.read(txt))\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(raw, sep=\"\\t\", header=None, engine=\"python\",\n",
        "                     dtype=str, quotechar='\"', quoting=csv.QUOTE_MINIMAL,\n",
        "                     on_bad_lines=\"error\")\n",
        "except Exception:\n",
        "    raw.seek(0)\n",
        "    df = pd.read_csv(raw, sep=\"\\t\", header=None, engine=\"python\",\n",
        "                     dtype=str, quoting=csv.QUOTE_NONE,\n",
        "                     on_bad_lines=\"warn\")\n",
        "```\n",
        "\n",
        "**Jerarquía de componentes (split por `:`):**\n",
        "\n",
        "```python\n",
        "parts = df['COMPNAME'].fillna('').astype(str).str.split(':', n=4, expand=True)\n",
        "parts.columns = ['COMP_L1','COMP_L2','COMP_L3','COMP_L4','COMP_L5']\n",
        "```\n",
        "\n",
        "**Cypher Aura-friendly (FOREACH + CASE) → Recalls:**\n",
        "\n",
        "> (idéntico al bloque 8.2)\n",
        "\n",
        "---\n",
        "\n",
        "Con esta etapa cerrada, el grafo tiene el **núcleo de Recalls** sólido, **jerárquico** y **consultable**. A partir de aquí podemos añadir:\n",
        "\n",
        "* `Investigation` → `RESULTED_IN` → `Recall` (enlace por `CAMPNO`).\n",
        "* `Complaint` → `Issue` (clusters) y `MENTIONS` → `Component`.\n",
        "* Capa semántica externa (Qdrant) y orquestación (FastAPI) para el agente.\n"
      ],
      "metadata": {
        "id": "VmtOeNeg335N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    !apt-get install tree\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvza-101vW8U",
        "outputId": "e5312178-7e16-4915-afc2-f9234a6520e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 0s (123 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na52yocPvGik",
        "outputId": "c52948b9-6c14-4568-ff27-06178a0a3d14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34mdrive/MyDrive/NHTSA/\u001b[0m\n",
            "├── \u001b[01;34membeddings\u001b[0m\n",
            "│   ├── \u001b[01;34mcomplaints_e5_mlg_instruct\u001b[0m\n",
            "│   │   ├── \u001b[00mcheckpoint.json\u001b[0m\n",
            "│   │   ├── \u001b[00membeddings_shard_0000.npy\u001b[0m\n",
            "│   │   ├── \u001b[00membeddings_shard_0001.npy\u001b[0m\n",
            "│   │   ├── \u001b[00membeddings_shard_0002.npy\u001b[0m\n",
            "│   │   ├── \u001b[00membeddings_shard_0003.npy\u001b[0m\n",
            "│   │   ├── \u001b[00membeddings_shard_0004.npy\u001b[0m\n",
            "│   │   ├── \u001b[00membeddings_shard_0005.npy\u001b[0m\n",
            "│   │   ├── \u001b[00membeddings_shard_0006.npy\u001b[0m\n",
            "│   │   ├── \u001b[00membeddings_shard_0007.npy\u001b[0m\n",
            "│   │   ├── \u001b[00membeddings_shard_0008.npy\u001b[0m\n",
            "│   │   ├── \u001b[00membeddings_shard_0009.npy\u001b[0m\n",
            "│   │   ├── \u001b[00membeddings_shard_0010.npy\u001b[0m\n",
            "│   │   ├── \u001b[00mmanifest.json\u001b[0m\n",
            "│   │   ├── \u001b[00mmeta_shard_0000.parquet\u001b[0m\n",
            "│   │   ├── \u001b[00mmeta_shard_0001.parquet\u001b[0m\n",
            "│   │   ├── \u001b[00mmeta_shard_0002.parquet\u001b[0m\n",
            "│   │   ├── \u001b[00mmeta_shard_0003.parquet\u001b[0m\n",
            "│   │   ├── \u001b[00mmeta_shard_0004.parquet\u001b[0m\n",
            "│   │   ├── \u001b[00mmeta_shard_0005.parquet\u001b[0m\n",
            "│   │   ├── \u001b[00mmeta_shard_0006.parquet\u001b[0m\n",
            "│   │   ├── \u001b[00mmeta_shard_0007.parquet\u001b[0m\n",
            "│   │   ├── \u001b[00mmeta_shard_0008.parquet\u001b[0m\n",
            "│   │   ├── \u001b[00mmeta_shard_0009.parquet\u001b[0m\n",
            "│   │   └── \u001b[00mmeta_shard_0010.parquet\u001b[0m\n",
            "│   ├── \u001b[01;34minvestigations_e5_mlg_instruct\u001b[0m\n",
            "│   │   ├── \u001b[00mconfig.json\u001b[0m\n",
            "│   │   ├── \u001b[00minvest_chunks_meta.parquet\u001b[0m\n",
            "│   │   ├── \u001b[00minvest_embeddings.npy\u001b[0m\n",
            "│   │   ├── \u001b[00minvest_embeddings_part001.npy\u001b[0m\n",
            "│   │   ├── \u001b[00minvest_embeddings_part002.npy\u001b[0m\n",
            "│   │   ├── \u001b[00minvest_embeddings_part003.npy\u001b[0m\n",
            "│   │   ├── \u001b[00minvest_embeddings_part004.npy\u001b[0m\n",
            "│   │   ├── \u001b[00minvest_embeddings_part005.npy\u001b[0m\n",
            "│   │   ├── \u001b[00minvest_embeddings_part006.npy\u001b[0m\n",
            "│   │   ├── \u001b[00minvest_embeddings_part007.npy\u001b[0m\n",
            "│   │   ├── \u001b[00minvest_embeddings_part008.npy\u001b[0m\n",
            "│   │   └── \u001b[00mshards.json\u001b[0m\n",
            "│   └── \u001b[01;34mrecalls_e5_mlg_instruct\u001b[0m\n",
            "│       ├── \u001b[00membedding_config.json\u001b[0m\n",
            "│       ├── \u001b[00mid_map.csv\u001b[0m\n",
            "│       ├── \u001b[00mrecalls_chunks_meta.parquet\u001b[0m\n",
            "│       └── \u001b[00mrecalls_embeddings.npy\u001b[0m\n",
            "├── \u001b[01;34mneo4j\u001b[0m\n",
            "│   └── \u001b[01;34mexports\u001b[0m\n",
            "│       └── \u001b[00mrecalls_neo4j_ready.csv\u001b[0m\n",
            "├── \u001b[01;34mprocessed\u001b[0m\n",
            "│   ├── \u001b[00mcomplaints_components.csv\u001b[0m\n",
            "│   ├── \u001b[00mcomplaints_corpus.jsonl\u001b[0m\n",
            "│   ├── \u001b[00mcomplaints_ids.csv\u001b[0m\n",
            "│   ├── \u001b[00mcomplaints.jsonl\u001b[0m\n",
            "│   ├── \u001b[00mcomplaints.parquet\u001b[0m\n",
            "│   ├── \u001b[00minvestigations_chunks_map.csv\u001b[0m\n",
            "│   ├── \u001b[00minvestigations_chunks.parquet\u001b[0m\n",
            "│   ├── \u001b[00minvestigations_corpus.jsonl\u001b[0m\n",
            "│   ├── \u001b[00minvestigations_ids.csv\u001b[0m\n",
            "│   ├── \u001b[00minvestigations.jsonl\u001b[0m\n",
            "│   ├── \u001b[00minvestigations.parquet\u001b[0m\n",
            "│   ├── \u001b[00mrecalls_chunks_fixed.parquet\u001b[0m\n",
            "│   ├── \u001b[00mrecalls_corpus.jsonl\u001b[0m\n",
            "│   └── \u001b[00mrecalls_master_fixed.parquet\u001b[0m\n",
            "└── \u001b[01;34msource\u001b[0m\n",
            "    └── \u001b[01;31mFLAT_RCL_POST_2010.zip\u001b[0m\n",
            "\n",
            "8 directories, 56 files\n"
          ]
        }
      ],
      "source": [
        "!tree drive/MyDrive/NHTSA/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install neo4j pandas pyarrow python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whKHs4t-vWGT",
        "outputId": "235d5620-a9e3-4886-abe9-b32c0e91b3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/325.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m307.2/325.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.8/325.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "BASE = Path('/content/drive/MyDrive/NHTSA')\n",
        "EMB  = BASE / 'embeddings'\n",
        "PROC = BASE / 'processed'\n",
        "EXPORTS = BASE / 'neo4j' / 'exports'\n",
        "EXPORTS.mkdir(parents=True, exist_ok=True)\n",
        "list(BASE.glob('**/*'))[:5], BASE.exists()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlNl3IdZwYjl",
        "outputId": "31fbfbd6-d05d-4793-aa7a-2d1bf745cc46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([PosixPath('/content/drive/MyDrive/NHTSA/processed'),\n",
              "  PosixPath('/content/drive/MyDrive/NHTSA/embeddings'),\n",
              "  PosixPath('/content/drive/MyDrive/NHTSA/neo4j'),\n",
              "  PosixPath('/content/drive/MyDrive/NHTSA/processed/recalls_corpus.jsonl'),\n",
              "  PosixPath('/content/drive/MyDrive/NHTSA/processed/complaints.parquet')],\n",
              " True)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Credenciales de Neo4j AuraDB\n"
      ],
      "metadata": {
        "id": "-s6Ur1VcwegB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "NEO4J_URI  = os.getenv('NEO4J_URI',  'neo4j+s://66024f48.databases.neo4j.io')\n",
        "NEO4J_USER = os.getenv('NEO4J_USER', 'neo4j')\n",
        "NEO4J_PASS = os.getenv('NEO4J_PASS', 'kDp50qsUISmBomZa8F9htkq-s5zcb-rlxbgyKYzdVEI')\n",
        "print(NEO4J_URI, NEO4J_USER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08Pp99wswb3p",
        "outputId": "6d9ed62b-8fd8-4099-eae7-be8076ceb530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neo4j+s://66024f48.databases.neo4j.io neo4j\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construir `recalls_neo4j_ready.csv` desde tus archivos en `processed/`\n"
      ],
      "metadata": {
        "id": "DPSlzDhHxVS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# NHTSA Recalls → limpieza “EDA original” + export Neo4j\n",
        "# Origen: drive/MyDrive/NHTSA/source/FLAT_RCL_POST_2010.zip\n",
        "# Salidas:\n",
        "#   processed/recalls_raw_enriched.csv\n",
        "#   processed/recalls_by_campaign.csv\n",
        "#   neo4j/exports/recalls_neo4j_ready.csv\n",
        "# ==========================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd, numpy as np, csv, re, zipfile, io\n",
        "from pathlib import Path\n",
        "\n",
        "BASE     = Path('/content/drive/MyDrive/NHTSA')\n",
        "SRC_ZIP  = BASE / 'source' / 'FLAT_RCL_POST_2010.zip'\n",
        "PROC_DIR = BASE / 'processed'\n",
        "EXP_DIR  = BASE / 'neo4j' / 'exports'\n",
        "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "assert SRC_ZIP.exists(), f\"No encuentro: {SRC_ZIP}\"\n",
        "\n",
        "# --------- utilidades ----------\n",
        "def strip_html_series(s: pd.Series) -> pd.Series:\n",
        "    def _clean(t):\n",
        "        if not isinstance(t, str): return t\n",
        "        t = re.sub(r\"<\\s*/?\\s*a\\b[^>]*>\", \" \", t, flags=re.I)  # quita <a>\n",
        "        t = re.sub(r\"<[^>]+>\", \" \", t)                         # quita cualquier tag\n",
        "        return re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return s.fillna(\"\").map(_clean)\n",
        "\n",
        "def split_component_hierarchy(s: pd.Series, max_levels: int = 5) -> pd.DataFrame:\n",
        "    \"\"\"divide COMPNAME por ':' en hasta max_levels niveles, sin dejar 'None' literales.\"\"\"\n",
        "    vals = s.fillna('').astype(str).tolist()\n",
        "    out = []\n",
        "    for v in vals:\n",
        "        parts = [p.strip() for p in v.split(':') if isinstance(p,str)]\n",
        "        parts = [p for p in parts if p and p.upper() != 'NONE']\n",
        "        parts = parts[:max_levels] + ['']*(max_levels-len(parts))\n",
        "        out.append(parts)\n",
        "    cols = [f'COMP_L{i}' for i in range(1, max_levels+1)]\n",
        "    return pd.DataFrame(out, columns=cols, index=s.index)\n",
        "\n",
        "# --------- lector Plan A/B DESDE ZIP ----------\n",
        "# El ZIP puede traer 1 TXT (FLAT_RCL_POST_2010.txt). Lo detectamos y leemos su contenido.\n",
        "with zipfile.ZipFile(SRC_ZIP, 'r') as zf:\n",
        "    txt_members = [n for n in zf.namelist() if n.lower().endswith('.txt')]\n",
        "    assert len(txt_members) >= 1, \"El ZIP no contiene .txt\"\n",
        "    TXT_NAME = txt_members[0]   # usamos el primero\n",
        "    raw_bytes = zf.read(TXT_NAME)\n",
        "    raw_io = io.BytesIO(raw_bytes)\n",
        "\n",
        "# Intento A: lector fiel con quotechar='\"'\n",
        "try:\n",
        "    dfA = pd.read_csv(\n",
        "        raw_io,\n",
        "        sep=\"\\t\",\n",
        "        header=None,\n",
        "        engine=\"python\",\n",
        "        dtype=str,\n",
        "        na_filter=True,\n",
        "        on_bad_lines=\"error\",\n",
        "        quotechar='\"',\n",
        "        quoting=csv.QUOTE_MINIMAL\n",
        "    )\n",
        "    df_raw = dfA\n",
        "    print(f\"[Plan A] OK filas={len(dfA)} cols={dfA.shape[1]}\")\n",
        "except Exception as eA:\n",
        "    print(\"[Plan A] Falló:\", eA)\n",
        "    # reiniciar el buffer para Plan B\n",
        "    raw_io.seek(0)\n",
        "    dfB = pd.read_csv(\n",
        "        raw_io,\n",
        "        sep=\"\\t\",\n",
        "        header=None,\n",
        "        engine=\"python\",\n",
        "        dtype=str,\n",
        "        na_filter=True,\n",
        "        on_bad_lines=\"warn\",\n",
        "        quoting=csv.QUOTE_NONE\n",
        "    )\n",
        "    df_raw = dfB\n",
        "    print(f\"[Plan B] OK filas={len(dfB)} cols={dfB.shape[1]}\")\n",
        "\n",
        "# --------- mapeo fijo de columnas (Appendix A: primeras 24) ----------\n",
        "APPX24 = [\n",
        "    'RECORD_ID','CAMPNO','MAKETXT','MODELTXT','YEARTXT','MFGCAMPNO','COMPNAME','MFGNAME',\n",
        "    'BGMAN','ENDMAN','RCLTYPECD','POTAFF','ODATE','INFLUENCED_BY','MFGTXT','RCDATE',\n",
        "    'DATEA','RPNO','FMVSS','DESC_DEFECT','CONEQUENCE_DEFECT','CORRECTIVE_ACTION',\n",
        "    'NOTES','RCL_CMPT_ID'\n",
        "]\n",
        "ncols = df_raw.shape[1]\n",
        "df_raw.columns = [f'c{i+1:02d}' for i in range(ncols)]\n",
        "rename_map = {f'c{i:02d}': APPX24[i-1] for i in range(1, min(24, ncols)+1)}\n",
        "df = df_raw.rename(columns=rename_map).copy()\n",
        "\n",
        "# Etiqueta columnas extra (c25..)\n",
        "for j in range(25, ncols+1):\n",
        "    c = f'c{j:02d}'\n",
        "    if c in df.columns:\n",
        "        df.rename(columns={c: f'EXTRA_{j:02d}'}, inplace=True)\n",
        "\n",
        "# Corregir typo CONEQUENCE_DEFECT → CONSEQUENCE_DEFECT\n",
        "if 'CONEQUENCE_DEFECT' in df.columns and 'CONSEQUENCE_DEFECT' not in df.columns:\n",
        "    df.rename(columns={'CONEQUENCE_DEFECT': 'CONSEQUENCE_DEFECT'}, inplace=True)\n",
        "\n",
        "# --------- tipificación / normalización EXACTA ----------\n",
        "# YEARTXT: enteros plausibles (1950..2035), 9999→NaN\n",
        "if 'YEARTXT' in df.columns:\n",
        "    df['YEARTXT'] = pd.to_numeric(df['YEARTXT'].replace({'9999': None}), errors='coerce')\n",
        "    df.loc[(df['YEARTXT'] < 1950) | (df['YEARTXT'] > 2035), 'YEARTXT'] = pd.NA\n",
        "    df['YEARTXT'] = df['YEARTXT'].astype('Int64')\n",
        "\n",
        "# Fechas a datetime (formato yyyyMMdd)\n",
        "for col in ['ODATE','RCDATE','DATEA','BGMAN','ENDMAN']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col], format='%Y%m%d', errors='coerce')\n",
        "\n",
        "# POTAFF → numérico auxiliar\n",
        "if 'POTAFF' in df.columns:\n",
        "    df['POTAFF_num'] = pd.to_numeric(df['POTAFF'], errors='coerce')\n",
        "\n",
        "# Limpieza superficial de HTML en narrativas y longitud\n",
        "for c in ['DESC_DEFECT','CONSEQUENCE_DEFECT','CORRECTIVE_ACTION','NOTES']:\n",
        "    if c in df.columns:\n",
        "        df[c] = strip_html_series(df[c])\n",
        "        df[f'len_{c}'] = df[c].str.len()\n",
        "\n",
        "# Jerarquía COMPNAME → COMP_L1..COMP_L5 (respetando tu criterio de 3, pero dejamos 5 por si acaso)\n",
        "if 'COMPNAME' in df.columns:\n",
        "    comp_df = split_component_hierarchy(df['COMPNAME'], max_levels=5)\n",
        "    df = pd.concat([df, comp_df], axis=1)\n",
        "\n",
        "# --------- deduplicación por campaña (quédanos el registro más reciente por RCDATE) ----------\n",
        "if {'CAMPNO','RCDATE'}.issubset(df.columns):\n",
        "    df = df.sort_values(['CAMPNO','RCDATE'], na_position='last').drop_duplicates('CAMPNO', keep='last')\n",
        "\n",
        "# --------- agregado por campaña (mantiene consistencia) ----------\n",
        "agg_spec = {\n",
        "    'POTAFF_num':'max',\n",
        "    'MFGNAME':'first',\n",
        "    'MAKETXT':'first',\n",
        "    'MODELTXT':'first',\n",
        "    'COMPNAME':'first',\n",
        "    'COMP_L1':'first','COMP_L2':'first','COMP_L3':'first','COMP_L4':'first','COMP_L5':'first',\n",
        "    'RCLTYPECD':'first','FMVSS':'first',\n",
        "    'RCDATE':'min','ODATE':'min','DATEA':'min',\n",
        "    'BGMAN':'min','ENDMAN':'max',\n",
        "    'YEARTXT':'max',\n",
        "    'DESC_DEFECT':'first','CONSEQUENCE_DEFECT':'first','CORRECTIVE_ACTION':'first','NOTES':'first'\n",
        "}\n",
        "present_agg = {k:v for k,v in agg_spec.items() if k in df.columns}\n",
        "by_camp = df.groupby('CAMPNO', as_index=False).agg(**{k:(k,v) for k,v in present_agg.items()})\n",
        "\n",
        "# --------- guardados intermedios (trazabilidad) ----------\n",
        "df.to_csv(PROC_DIR/'recalls_raw_enriched.csv', index=False)\n",
        "by_camp.to_csv(PROC_DIR/'recalls_by_campaign.csv', index=False)\n",
        "print(\"Crudo (dedup):\", df.shape, \" | Por campaña:\", by_camp.shape)\n",
        "\n",
        "# --------- controles de calidad (idénticos al EDA original) ----------\n",
        "if {'CAMPNO','POTAFF_num'}.issubset(df.columns):\n",
        "    var_por_camp = (df.groupby('CAMPNO')['POTAFF_num']\n",
        "                      .agg(lambda s: pd.Series(s.dropna().unique()).size))\n",
        "    inconsistentes = var_por_camp[var_por_camp > 1]\n",
        "    print(\"Campañas con >1 valor de POTAFF_num:\", int((inconsistentes>1).sum()))\n",
        "\n",
        "for c in ['RCDATE','ODATE','DATEA']:\n",
        "    if c in by_camp.columns:\n",
        "        print(f\"{c} cobertura:\", (by_camp[c].notna().mean()*100).round(2), \"%\")\n",
        "\n",
        "text_flags = {}\n",
        "for c in ['DESC_DEFECT','CONSEQUENCE_DEFECT','CORRECTIVE_ACTION','NOTES']:\n",
        "    if c in df.columns:\n",
        "        text_flags[c] = (df[c].fillna('').str.strip().str.len()>0).mean()\n",
        "if text_flags:\n",
        "    print(\"Cobertura de textos (crudo):\", {k: round(v*100,1) for k,v in text_flags.items()})\n",
        "\n",
        "# Outliers y rangos (como antes)\n",
        "if 'POTAFF_num' in by_camp.columns:\n",
        "    p = by_camp['POTAFF_num'].dropna()\n",
        "    print(\"POTAFF min/max/mediana:\", p.min(), p.max(), p.median())\n",
        "    print(\"POTAFF <= 0:\", int((p <= 0).sum()))\n",
        "if 'YEARTXT' in by_camp.columns and by_camp['YEARTXT'].notna().any():\n",
        "    yr = by_camp['YEARTXT'].dropna().astype(int)\n",
        "    print(\"YEARTXT min/max:\", yr.min(), yr.max())\n",
        "\n",
        "# --------- export minimal PARA AURA (nombres fijos y jerarquía explicitada) ----------\n",
        "rec = by_camp.copy()\n",
        "\n",
        "# Fechas ISO (string yyyy-mm-dd)\n",
        "for col in ['RCDATE']:\n",
        "    if col in rec.columns:\n",
        "        rec[col] = pd.to_datetime(rec[col], errors='coerce').dt.strftime('%Y-%m-%d').fillna('')\n",
        "\n",
        "# Asegura tipos sencillos\n",
        "if 'YEARTXT' in rec.columns:\n",
        "    rec['YEARTXT'] = pd.to_numeric(rec['YEARTXT'], errors='coerce').astype('Int64')\n",
        "\n",
        "# Renombrado a columnas “amables”:\n",
        "rename = {\n",
        "    'CAMPNO':'campaign_no',\n",
        "    'MAKETXT':'make',\n",
        "    'MODELTXT':'model',\n",
        "    'YEARTXT':'year',\n",
        "    'COMPNAME':'component',\n",
        "    'RCDATE':'recall_date',\n",
        "    'DESC_DEFECT':'subject',\n",
        "    'CONSEQUENCE_DEFECT':'consequence',\n",
        "    'COMP_L1':'comp_l1',\n",
        "    'COMP_L2':'comp_l2',\n",
        "    'COMP_L3':'comp_l3',\n",
        "    'COMP_L4':'comp_l4',\n",
        "    'COMP_L5':'comp_l5'\n",
        "}\n",
        "for k,v in rename.items():\n",
        "    if k in rec.columns:\n",
        "        rec.rename(columns={k:v}, inplace=True)\n",
        "\n",
        "# Normaliza strings clave (mayúsculas, espacios)\n",
        "def norm_text_upper(x):\n",
        "    if not isinstance(x, str): return ''\n",
        "    s = x.upper().strip()\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    # compactar ':' y '/'\n",
        "    s = s.replace(' :', ':').replace(': ', ':')\n",
        "    s = re.sub(r'\\s*/\\s*', '/', s)\n",
        "    return s\n",
        "\n",
        "for c in ['campaign_no','make','model','component','subject','consequence','recall_date',\n",
        "          'comp_l1','comp_l2','comp_l3','comp_l4','comp_l5']:\n",
        "    if c in rec.columns:\n",
        "        rec[c] = rec[c].fillna('').map(norm_text_upper)\n",
        "\n",
        "# Filtro mínimo (misma lógica que antes, PERO sin exigir year para no vaciar)\n",
        "mask_ok = (\n",
        "    rec['campaign_no'].str.len().gt(0) &\n",
        "    rec['make'].str.len().gt(0) &\n",
        "    rec['model'].str.len().gt(0) &\n",
        "    rec['comp_l1'].str.len().gt(0)\n",
        ")\n",
        "good = rec[mask_ok].copy()\n",
        "\n",
        "# Si no hay 'component' original, usar comp_l1\n",
        "if 'component' not in good.columns:\n",
        "    good['component'] = good['comp_l1']\n",
        "else:\n",
        "    good['component'] = np.where(good['component'].str.len().gt(0), good['component'], good['comp_l1'])\n",
        "\n",
        "# Export final para Neo4j Aura\n",
        "out_cols = ['campaign_no','make','model','year','component','recall_date','subject','consequence',\n",
        "            'comp_l1','comp_l2','comp_l3','comp_l4','comp_l5']\n",
        "for c in out_cols:\n",
        "    if c not in good.columns:\n",
        "        good[c] = ''  # rellenar ausentes esperados\n",
        "\n",
        "out_path = EXP_DIR / 'recalls_neo4j_ready.csv'\n",
        "good[out_cols].to_csv(out_path, index=False)\n",
        "\n",
        "print(\"==================================\")\n",
        "print(\"Export listo para Aura →\", out_path)\n",
        "print(\"Filas:\", len(good))\n",
        "print(good[out_cols].head(8))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF_3sI9-xKi0",
        "outputId": "de9f0856-7e59-4838-fc2f-183b3d25d733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Plan A] Falló: '\t' expected after '\"'\n",
            "[Plan B] OK filas=222573 cols=29\n",
            "Crudo (dedup): (14340, 39)  | Por campaña: (14340, 23)\n",
            "Campañas con >1 valor de POTAFF_num: 0\n",
            "RCDATE cobertura: 100.0 %\n",
            "ODATE cobertura: 97.75 %\n",
            "DATEA cobertura: 100.0 %\n",
            "Cobertura de textos (crudo): {'DESC_DEFECT': np.float64(100.0), 'CONSEQUENCE_DEFECT': np.float64(100.0), 'CORRECTIVE_ACTION': np.float64(100.0), 'NOTES': np.float64(97.3)}\n",
            "POTAFF min/max/mediana: 0 17600000 466.0\n",
            "POTAFF <= 0: 6\n",
            "YEARTXT min/max: 1976 2026\n",
            "==================================\n",
            "Export listo para Aura → /content/drive/MyDrive/NHTSA/neo4j/exports/recalls_neo4j_ready.csv\n",
            "Filas: 14340\n",
            "  campaign_no            make                      model  year  \\\n",
            "0   10C001000       MAXI-COSI                 22-371 HFL  <NA>   \n",
            "1   10C003000           CYBEX             SOLUTION X-FIX  <NA>   \n",
            "2   10C005000         EVENFLO              310 (MAESTRO)  <NA>   \n",
            "3   10C006000          BRITAX  CHAPERONE E9L69P3 SAVANNA  <NA>   \n",
            "4   10E002000          TOYOTA                     CELICA  1986   \n",
            "5   10E003000          SPICER                      SD231  <NA>   \n",
            "6   10E004000             B&W                   TEXA4200  <NA>   \n",
            "7   10E005000  DETROIT DIESEL            EPA07 SERIES 60  <NA>   \n",
            "\n",
            "                          component recall_date  \\\n",
            "0                   CHILD SEAT:BASE  2010-02-04   \n",
            "1                        CHILD SEAT  2010-07-26   \n",
            "2                        CHILD SEAT  2010-10-13   \n",
            "3                        CHILD SEAT  2010-10-28   \n",
            "4                        SUSPENSION  2010-02-11   \n",
            "5  POWER TRAIN:DRIVELINE:DRIVESHAFT  2010-02-17   \n",
            "6                   TRAILER HITCHES  2010-02-19   \n",
            "7         ENGINE AND ENGINE COOLING  2010-03-19   \n",
            "\n",
            "                                             subject  \\\n",
            "0  DOREL JUVENILE GROUP (DJG) IS RECALLING CERTAI...   \n",
            "1  CERTAIN CYBEX SOLUTION X-FIX MODEL BOOSTER SEA...   \n",
            "2  EVENFLO IS RECALLING CERTAIN MAESTRO CHILD RES...   \n",
            "3  BRITAX IS RECALLING CERTAIN BRITAX CHAPERON IN...   \n",
            "4  RIDE CONTROL IS RECALLING CERTAIN FRONT STRUT ...   \n",
            "5  DANA HOLDING CORPORATION HAS NOTIFIED NHTSA AB...   \n",
            "6  B&W CUSTOM TRUCK BEDS, INC. HAS NOTIFIED NHTSA...   \n",
            "7  DAIMLER TRUCKS NORTH AMERICA (DTNA) HAS NOTIFI...   \n",
            "\n",
            "                                         consequence  \\\n",
            "0  IF THE SHELL IS IMPROPERLY MOUNTED TO THE BASE...   \n",
            "1  IN A SHARP TURN, CRASH, OR SUDDEN STOP, AN UNS...   \n",
            "2  IN THE EVENT OF A CRASH, A CRACK MAY FORM AT T...   \n",
            "3  THE SHARP EDGES OF THE BROKEN CHEST CLIP COULD...   \n",
            "4  THE ABSENCE OF THE WELD CAN ALLOW THE STRUT TO...   \n",
            "5  THESE CRACKS MAY LEAD TO DRIVESHAFT FAILURE WH...   \n",
            "6  IF THIS OCCURS, A TRAILER UNDER TOW MAY SEPARA...   \n",
            "7  UNEXPECTED EMERGENCY VEHICLE ENGINE SHUTDOWN C...   \n",
            "\n",
            "                     comp_l1    comp_l2     comp_l3 comp_l4 comp_l5  \n",
            "0                 CHILD SEAT       BASE                              \n",
            "1                 CHILD SEAT                                         \n",
            "2                 CHILD SEAT                                         \n",
            "3                 CHILD SEAT                                         \n",
            "4                 SUSPENSION                                         \n",
            "5                POWER TRAIN  DRIVELINE  DRIVESHAFT                  \n",
            "6            TRAILER HITCHES                                         \n",
            "7  ENGINE AND ENGINE COOLING                                         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from neo4j import GraphDatabase\n",
        "import os\n",
        "\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
        "CONSTRAINTS = [\n",
        "  \"CREATE CONSTRAINT IF NOT EXISTS FOR (r:Recall)        REQUIRE r.id IS UNIQUE\",\n",
        "  \"CREATE CONSTRAINT IF NOT EXISTS FOR (i:Investigation) REQUIRE i.id IS UNIQUE\",\n",
        "  \"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Complaint)     REQUIRE c.id IS UNIQUE\",\n",
        "  \"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Component)     REQUIRE c.name IS UNIQUE\",\n",
        "  \"CREATE CONSTRAINT IF NOT EXISTS FOR (m:Make)          REQUIRE m.name IS UNIQUE\",\n",
        "  \"CREATE CONSTRAINT IF NOT EXISTS FOR (m:Model)         REQUIRE (m.name, m.make) IS UNIQUE\",\n",
        "  \"CREATE CONSTRAINT IF NOT EXISTS FOR (x:Issue)         REQUIRE x.id IS UNIQUE\",\n",
        "  # índice auxiliar (no único) para búsquedas por minúsculas\n",
        "  \"CREATE INDEX comp_name_lower IF NOT EXISTS FOR (c:Component) ON (c.name_lower)\",\n",
        "]\n",
        "\n",
        "with driver.session(database=\"neo4j\") as s:\n",
        "    for q in CONSTRAINTS:\n",
        "        s.run(q)\n",
        "        print(\"OK:\", q[:60])\n",
        "print(\"Listo ✅\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh5QhSaf0Ban",
        "outputId": "ad985a61-5ab2-48d9-99b4-7e47af40353c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: CREATE CONSTRAINT IF NOT EXISTS FOR (r:Recall)        REQUIR\n",
            "OK: CREATE CONSTRAINT IF NOT EXISTS FOR (i:Investigation) REQUIR\n",
            "OK: CREATE CONSTRAINT IF NOT EXISTS FOR (c:Complaint)     REQUIR\n",
            "OK: CREATE CONSTRAINT IF NOT EXISTS FOR (c:Component)     REQUIR\n",
            "OK: CREATE CONSTRAINT IF NOT EXISTS FOR (m:Make)          REQUIR\n",
            "OK: CREATE CONSTRAINT IF NOT EXISTS FOR (m:Model)         REQUIR\n",
            "OK: CREATE CONSTRAINT IF NOT EXISTS FOR (x:Issue)         REQUIR\n",
            "OK: CREATE INDEX comp_name_lower IF NOT EXISTS FOR (c:Component)\n",
            "Listo ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CYPHER_UPSERT_RECALL_HIER_5 = \"\"\"\n",
        "UNWIND $rows AS row\n",
        "// Recall + MM\n",
        "MERGE (r:Recall {id: row.campaign_no})\n",
        "  SET r.camp_no     = row.campaign_no,\n",
        "      r.recall_date = CASE WHEN row.recall_date = '' THEN NULL ELSE row.recall_date END,\n",
        "      r.make        = row.make,\n",
        "      r.model       = row.model,\n",
        "      r.year        = CASE WHEN row.year IS NULL OR row.year = '' THEN NULL ELSE toInteger(row.year) END,\n",
        "      r.component   = row.component,\n",
        "      r.subject     = coalesce(row.subject, ''),\n",
        "      r.consequence = coalesce(row.consequence, '')\n",
        "\n",
        "MERGE (mk:Make {name: row.make})\n",
        "MERGE (md:Model {name: row.model, make: row.make})\n",
        "MERGE (r)-[:OF_MAKE]->(mk)\n",
        "MERGE (r)-[:OF_MODEL]->(md)\n",
        "\n",
        "// L1 (siempre existe por el filtro que hicimos)\n",
        "MERGE (c1:Component {name: row.comp_l1})\n",
        "  ON CREATE SET c1.name_lower = toLower(row.comp_l1)\n",
        "  ON MATCH  SET c1.name_lower = coalesce(c1.name_lower, toLower(row.comp_l1))\n",
        "\n",
        "// L2 (condicional)\n",
        "FOREACH (_ IN CASE WHEN row.comp_l2 <> '' THEN [1] ELSE [] END |\n",
        "  MERGE (p1:Component {name: row.comp_l1})\n",
        "  MERGE (c2:Component {name: row.comp_l2})\n",
        "    ON CREATE SET c2.name_lower = toLower(row.comp_l2)\n",
        "    ON MATCH  SET c2.name_lower = coalesce(c2.name_lower, toLower(row.comp_l2))\n",
        "  MERGE (c2)-[:SUB_OF]->(p1)\n",
        ")\n",
        "\n",
        "// L3 (requiere L2 y L3 no vacíos)\n",
        "FOREACH (_ IN CASE WHEN row.comp_l3 <> '' AND row.comp_l2 <> '' THEN [1] ELSE [] END |\n",
        "  MERGE (p2:Component {name: row.comp_l2})\n",
        "  MERGE (c3:Component {name: row.comp_l3})\n",
        "    ON CREATE SET c3.name_lower = toLower(row.comp_l3)\n",
        "    ON MATCH  SET c3.name_lower = coalesce(c3.name_lower, toLower(row.comp_l3))\n",
        "  MERGE (c3)-[:SUB_OF]->(p2)\n",
        ")\n",
        "\n",
        "// L4 (requiere L3 y L4)\n",
        "FOREACH (_ IN CASE WHEN row.comp_l4 <> '' AND row.comp_l3 <> '' THEN [1] ELSE [] END |\n",
        "  MERGE (p3:Component {name: row.comp_l3})\n",
        "  MERGE (c4:Component {name: row.comp_l4})\n",
        "    ON CREATE SET c4.name_lower = toLower(row.comp_l4)\n",
        "    ON MATCH  SET c4.name_lower = coalesce(c4.name_lower, toLower(row.comp_l4))\n",
        "  MERGE (c4)-[:SUB_OF]->(p3)\n",
        ")\n",
        "\n",
        "// L5 (requiere L4 y L5)\n",
        "FOREACH (_ IN CASE WHEN row.comp_l5 <> '' AND row.comp_l4 <> '' THEN [1] ELSE [] END |\n",
        "  MERGE (p4:Component {name: row.comp_l4})\n",
        "  MERGE (c5:Component {name: row.comp_l5})\n",
        "    ON CREATE SET c5.name_lower = toLower(row.comp_l5)\n",
        "    ON MATCH  SET c5.name_lower = coalesce(c5.name_lower, toLower(row.comp_l5))\n",
        "  MERGE (c5)-[:SUB_OF]->(p4)\n",
        ")\n",
        "\n",
        "// Elegimos el componente más profundo disponible y creamos :MENTIONS\n",
        "WITH r,\n",
        "     CASE\n",
        "       WHEN row.comp_l5 <> '' THEN row.comp_l5\n",
        "       WHEN row.comp_l4 <> '' THEN row.comp_l4\n",
        "       WHEN row.comp_l3 <> '' THEN row.comp_l3\n",
        "       WHEN row.comp_l2 <> '' THEN row.comp_l2\n",
        "       ELSE row.comp_l1\n",
        "     END AS target_comp\n",
        "MATCH (cx:Component {name: target_comp})\n",
        "MERGE (r)-[:MENTIONS]->(cx)\n",
        "\n",
        "RETURN count(r) AS upserted;\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "ufDkfoHhxXY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "CSV = Path('/content/drive/MyDrive/NHTSA/neo4j/exports/recalls_neo4j_ready.csv')\n",
        "df = pd.read_csv(CSV, dtype=str, keep_default_na=False)\n",
        "\n",
        "def ingest(csv_df, cypher, batch=400):\n",
        "    total, i = len(csv_df), 0\n",
        "    with driver.session(database=\"neo4j\") as s:\n",
        "        while i < total:\n",
        "            rows = csv_df.iloc[i:i+batch].to_dict('records')\n",
        "            s.run(cypher, rows=rows)\n",
        "            i += batch\n",
        "            print(f\"→ {min(i,total)}/{total}\")\n",
        "    print(\"Ingesta completa ✅\")\n",
        "\n",
        "# Test con 200 filas\n",
        "ingest(df.head(200), CYPHER_UPSERT_RECALL_HIER_5, batch=200)\n",
        "\n",
        "# Verificación rápida\n",
        "with driver.session(database=\"neo4j\") as s:\n",
        "    print(\"Recalls:\", s.run(\"MATCH (r:Recall) RETURN count(r) AS n\").single()['n'])\n",
        "    print(\"Components:\", s.run(\"MATCH (c:Component) RETURN count(c) AS n\").single()['n'])\n",
        "    print(s.run(\"\"\"\n",
        "      MATCH (r:Recall)-[:MENTIONS]->(c:Component)\n",
        "      RETURN r.id AS camp, c.name AS comp\n",
        "      LIMIT 5\n",
        "    \"\"\").data())\n",
        "\n",
        "# Ingesta total\n",
        "ingest(df, CYPHER_UPSERT_RECALL_HIER_5, batch=400)\n",
        "\n",
        "with driver.session(database=\"neo4j\") as s:\n",
        "    print(\"Recalls:\", s.run(\"MATCH (r:Recall) RETURN count(r) AS n\").single()['n'])\n",
        "    print(\"Components:\", s.run(\"MATCH (c:Component) RETURN count(c) AS n\").single()['n'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PVv7UOWxa0y",
        "outputId": "26780f17-82eb-4997-a089-f28eabf54489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ 200/200\n",
            "Ingesta completa ✅\n",
            "Recalls: 200\n",
            "Components: 111\n",
            "[{'camp': '10C003000', 'comp': 'CHILD SEAT'}, {'camp': '10C005000', 'comp': 'CHILD SEAT'}, {'camp': '10C006000', 'comp': 'CHILD SEAT'}, {'camp': '10E002000', 'comp': 'SUSPENSION'}, {'camp': '10E047000', 'comp': 'SUSPENSION'}]\n",
            "→ 400/14340\n",
            "→ 800/14340\n",
            "→ 1200/14340\n",
            "→ 1600/14340\n",
            "→ 2000/14340\n",
            "→ 2400/14340\n",
            "→ 2800/14340\n",
            "→ 3200/14340\n",
            "→ 3600/14340\n",
            "→ 4000/14340\n",
            "→ 4400/14340\n",
            "→ 4800/14340\n",
            "→ 5200/14340\n",
            "→ 5600/14340\n",
            "→ 6000/14340\n",
            "→ 6400/14340\n",
            "→ 6800/14340\n",
            "→ 7200/14340\n",
            "→ 7600/14340\n",
            "→ 8000/14340\n",
            "→ 8400/14340\n",
            "→ 8800/14340\n",
            "→ 9200/14340\n",
            "→ 9600/14340\n",
            "→ 10000/14340\n",
            "→ 10400/14340\n",
            "→ 10800/14340\n",
            "→ 11200/14340\n",
            "→ 11600/14340\n",
            "→ 12000/14340\n",
            "→ 12400/14340\n",
            "→ 12800/14340\n",
            "→ 13200/14340\n",
            "→ 13600/14340\n",
            "→ 14000/14340\n",
            "→ 14340/14340\n",
            "Ingesta completa ✅\n",
            "Recalls: 14340\n",
            "Components: 509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ThfRigIfycQ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}