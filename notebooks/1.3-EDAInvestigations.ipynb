{"cells":[{"cell_type":"markdown","metadata":{"id":"_iqkvWuRSASH"},"source":["## Etapa de Ingesta y Normalización de Datos de *Investigations* (NHTSA)\n","\n","### 1. Descripción General\n","\n","Esta etapa tuvo como finalidad la **ingesta, depuración y estandarización del conjunto de datos de investigaciones de defectos** (*Investigations*) publicado por la *Office of Defects Investigation (ODI)* de la *National Highway Traffic Safety Administration* (NHTSA).\n","El objetivo principal fue integrar la información de investigaciones vehiculares en un formato coherente con los demás datasets (*Complaints* y *Recalls*), preservando trazabilidad, calidad estructural y compatibilidad con un modelo de grafo de conocimiento.\n","\n","El archivo fuente, `FLAT_INV.zip`, contiene expedientes históricos de investigaciones abiertas por la NHTSA desde 1972/1973 hasta la fecha, derivados de quejas de consumidores u otros indicios (p. ej., información de fabricantes).\n","Cada registro corresponde a un **expediente único de investigación**, asociado a marca, modelo y año del vehículo, componente técnico analizado y, en muchos casos, a una o más campañas de retiro (*Recalls*) vinculadas.\n","\n","---\n","\n","### 2. Procedimiento de Ingesta\n","\n","Se implementó un proceso de lectura con **tolerancia controlada a errores (Plan A/B)** y, adicionalmente, se **forzaron los encabezados oficiales definidos en `INV.txt`** para evitar que la primera fila de datos fuese tomada como cabecera:\n","\n","* **Forzado de esquema**: lectura con `header=None` y `names=` **igual a los 11 campos oficiales** del diccionario (*INV.txt*):\n","  `[\"NHTSA ACTION NUMBER\",\"MAKE\",\"MODEL\",\"YEAR\",\"COMPNAME\",\"MFR_NAME\",\"ODATE\",\"CDATE\",\"CAMPNO\",\"SUBJECT\",\"SUMMARY\"]`.\n","* **Plan A** (*preferido*): lectura *tab-delimited* (`sep='\\t'`, `quotechar='\"'`) con motor *python* de `pandas`.\n","* **Plan B** (*contingencia*): lectura tolerante con `quoting=csv.QUOTE_NONE` y `on_bad_lines=\"warn\"` para registros con comillas no balanceadas.\n","\n","En la ejecución realizada, **la totalidad del archivo se procesó exitosamente con Plan A**, sin errores ni advertencias.\n","El resultado fue la carga de **153,550 registros** en **12 columnas** (las 11 oficiales + `__SOURCE_FILE__` para trazabilidad).\n","\n","---\n","\n","### 3. Estructura y Campos Principales (Esquema Oficial)\n","\n","El dataset presenta una estructura compacta (un registro por **caso de investigación**: PE, EA, AQ, RQ, DP, etc.).\n","Se utilizaron los siguientes **nombres canónicos** (conforme a `INV.txt`):\n","\n","* **Identificador de caso**: `NHTSA ACTION NUMBER`\n","* **Vehículo**: `MAKE`, `MODEL`, `YEAR` (con `9999` como valor desconocido en el origen)\n","* **Componente**: `COMPNAME` (descripción jerárquica separada por “:”)\n","* **Fechas**: `ODATE` (apertura, `YYYYMMDD`) y `CDATE` (cierre, `YYYYMMDD`)\n","* **Relación con *Recalls***: `CAMPNO` (número de campaña)\n","* **Texto**: `SUBJECT` (resumen breve) y `SUMMARY` (detalle extenso)\n","* **Trazabilidad**: `__SOURCE_FILE__` (archivo interno dentro del ZIP)\n","\n","---\n","\n","### 4. Normalización y Tipificación\n","\n","Se aplicaron las siguientes transformaciones:\n","\n","* **Alineación de nombres a diccionario oficial** (`INV.txt`) con alias tolerantes cuando fue necesario.\n","* **Conversión de fechas** `ODATE` y `CDATE` a tipo `datetime` (formato `YYYYMMDD`).\n","* **Conversión de año de modelo** `YEAR` a entero con validación de rango plausible [1949–2035] y conversión de `9999` a nulo.\n","* **Limpieza ligera de texto** en `SUBJECT`, `SUMMARY` y `COMPNAME` (normalización de espacios) y **cálculo de métricas** de longitud por campo y total (`_LEN`, `TEXT_TOTAL_LEN`).\n","* **Preservación de extras**: además de los 11 campos oficiales, se mantuvo `__SOURCE_FILE__` como metadato de trazabilidad.\n","\n","No se aplicaron transformaciones agresivas de puntuación o HTML, atendiendo al carácter institucional de las descripciones.\n","\n","---\n","\n","### 5. Control de Calidad\n","\n","El control de calidad incluyó:\n","\n","* **Integridad estructural total**: ningún registro descartado.\n","* **Encabezados correctos**: el forzado de esquema evitó desplazamientos y garantizó la correspondencia exacta con `INV.txt`.\n","* **Cobertura temporal**: `ODATE` presente en el 100 % de los casos; `CDATE` presente en la gran mayoría de los registros.\n","* **Ausencia de advertencias**: `warnings = 0`.\n","* **Uso exclusivo de Plan A**: confirma formato bien codificado.\n","\n","Los productos finales se almacenaron en:\n","\n","* **Parquet:** `/content/drive/MyDrive/NHTSA/processed/investigations.parquet`\n","* **JSONL:** `/content/drive/MyDrive/NHTSA/processed/investigations.jsonl`\n","\n","---\n","\n","### 6. Derivación de Vistas Temáticas\n","\n","A partir del dataset normalizado se generaron vistas auxiliares para análisis y modelado de grafo:\n","\n","1. **`investigations_MMY.csv`**\n","   Pares únicos *Make–Model–Year* (`MAKE`, `MODEL`, `YEAR`) con identificador canónico `MMY_ID` (normalizado en mayúsculas).\n","\n","2. **`investigations_components.csv`**\n","   Descomposición jerárquica de `COMPNAME` en `COMP_L1`, `COMP_L2`, `COMP_L3`, facilitando la alineación con componentes de *Complaints* y *Recalls*.\n","\n","3. **`investigations_ids.csv`**\n","   Identificadores y clave de enlace con *Recalls*: `NHTSA ACTION NUMBER` y `CAMPNO`. Permite relaciones del tipo `(Investigation)-[:RELATES_TO]->(Recall)` y cruces con *Complaints* por MMY/componente y ventana temporal.\n","\n","4. **`investigations_corpus.jsonl`** *(opcional)*\n","   Corpus textual para *embeddings* basado en `SUMMARY` (o `SUBJECT` cuando aplique), con metadatos (`MAKE`, `MODEL`, `YEAR`, `COMPNAME`, `ODATE`, `CDATE`, `CAMPNO`), orientado a consultas semánticas y vinculación de textos entre *Investigations*, *Recalls* y *Complaints*.\n","\n","---\n","\n","### 7. Conclusiones\n","\n","El procesamiento de *Investigations* se completó con **forzado de encabezados oficiales** y **normalización canónica** conforme a `INV.txt`, asegurando compatibilidad estricta entre fuentes.\n","El resultado es un conjunto de **153,550 investigaciones** con tipificación coherente, fechas válidas y textos normalizados, preservando relaciones explícitas con campañas de retiro (*Recalls*) y habilitando su integración con *Complaints* y entidades vehiculares en el grafo de conocimiento.\n","\n","La base resultante constituye una **fuente analítica de alta calidad**, adecuada para análisis relacional, seguimiento de defectos, y explotación semántica de la evidencia histórica de seguridad automotriz de la NHTSA.\n","\n","---\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xkwMIEvXWxRT","executionInfo":{"status":"ok","timestamp":1760154065370,"user_tz":360,"elapsed":21078,"user":{"displayName":"Lucero Guadalupe Contreras Hernández","userId":"07832692706572343340"}},"outputId":"1d9bb245-3bea-4b65-a23c-5f28a44b34cb"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"id":"eWP3c72KRpxM","executionInfo":{"status":"ok","timestamp":1760154827569,"user_tz":360,"elapsed":49,"user":{"displayName":"Lucero Guadalupe Contreras Hernández","userId":"07832692706572343340"}}},"outputs":[],"source":["# Esquema oficial de INV.txt (11 campos)\n","INV_SCHEMA_OFFICIAL = [\n","    \"NHTSA ACTION NUMBER\",\"MAKE\",\"MODEL\",\"YEAR\",\"COMPNAME\",\"MFR_NAME\",\n","    \"ODATE\",\"CDATE\",\"CAMPNO\",\"SUBJECT\",\"SUMMARY\"\n","]\n","\n","def read_investigations_zip_with_schema(zip_path: str, encoding=\"utf-8\"):\n","    \"\"\"\n","    Lee FLAT_INV.zip forzando header=None + names=INV_SCHEMA_OFFICIAL,\n","    con Plan A/B tolerante.\n","    \"\"\"\n","    import io, re, csv, zipfile\n","    import pandas as pd\n","\n","    zf = zipfile.ZipFile(zip_path, \"r\")\n","    frames, warns = [], []\n","    plan_stats = {\"A\": 0, \"B\": 0}\n","\n","    for name in zf.namelist():\n","        if not re.search(r\"\\.(txt|tsv|csv)$\", name, flags=re.I):\n","            continue\n","        raw = zf.read(name)\n","\n","        # Plan A: tab + quotechar\n","        try:\n","            df = pd.read_csv(\n","                io.BytesIO(raw),\n","                sep=\"\\t\",\n","                engine=\"python\",\n","                quotechar='\"',\n","                encoding=encoding,\n","                header=None,                             # <-- fuerza SIN header\n","                names=INV_SCHEMA_OFFICIAL,               # <-- nombres oficiales\n","                on_bad_lines=\"error\",\n","                dtype=str\n","            )\n","            plan_stats[\"A\"] += 1\n","        except Exception as eA:\n","            warns.append(f\"{name} [Plan A] {type(eA).__name__}: {eA}\")\n","            # Plan B: sin comillas, tolerante\n","            df = pd.read_csv(\n","                io.BytesIO(raw),\n","                sep=\"\\t\",\n","                engine=\"python\",\n","                quoting=csv.QUOTE_NONE,\n","                escapechar=\"\\\\\",\n","                encoding=encoding,\n","                header=None,                             # <-- fuerza SIN header\n","                names=INV_SCHEMA_OFFICIAL,               # <-- nombres oficiales\n","                on_bad_lines=\"warn\",\n","                dtype=str\n","            )\n","            plan_stats[\"B\"] += 1\n","\n","        df[\"__SOURCE_FILE__\"] = name\n","        frames.append(df)\n","\n","    if not frames:\n","        raise RuntimeError(\"No se encontraron .txt/.tsv/.csv en el ZIP.\")\n","\n","    wide = pd.concat(frames, ignore_index=True, sort=False)\n","    return wide, plan_stats, warns\n"]},{"cell_type":"code","source":["def run_pipeline_investigations(zip_path: str):\n","    # Usa el lector que fuerza el header oficial\n","    df_raw, plan_stats, warns = read_investigations_zip_with_schema(zip_path)\n","\n","    # Normalización oficial (alias ya no debería hacer falta, pero no estorba)\n","    df_norm = normalize_investigations_official(df_raw)\n","\n","    # QC\n","    DATE_INV_OFFICIAL = [\"ODATE\", \"CDATE\"]\n","    TEXT_INV_OFFICIAL = [\"SUMMARY\", \"SUBJECT\"]\n","\n","    qc = {\n","        \"n_rows\": len(df_norm),\n","        \"n_cols\": df_norm.shape[1],\n","        \"plans_usage\": plan_stats,\n","        \"warnings_count\": len(warns),\n","        \"date_coverage\": {c: float(df_norm[c].notna().mean()) for c in DATE_INV_OFFICIAL if c in df_norm.columns},\n","        \"text_coverage\": {c: float(df_norm[c].notna().mean()) for c in TEXT_INV_OFFICIAL if c in df_norm.columns},\n","        \"year_range\": None\n","    }\n","    if \"YEAR\" in df_norm.columns:\n","        yrs = pd.to_numeric(df_norm[\"YEAR\"], errors=\"coerce\").dropna()\n","        qc[\"year_range\"] = (int(yrs.min()) if not yrs.empty else None,\n","                            int(yrs.max()) if not yrs.empty else None)\n","\n","    # Persistencia\n","    parquet_path = OUT_DIR / \"investigations.parquet\"\n","    jsonl_path   = OUT_DIR / \"investigations.jsonl\"\n","    df_norm.to_parquet(parquet_path, index=False)\n","    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n","        for _, row in df_norm.iterrows():\n","            f.write(json.dumps({k: (None if pd.isna(v) else v) for k, v in row.to_dict().items()},\n","                               default=str, ensure_ascii=False) + \"\\n\")\n","\n","    # Resumen (ahora sí deben verse los NOMBRES oficiales)\n","    summary = {\n","        \"dataset\": \"investigations\",\n","        \"rows\": qc[\"n_rows\"],\n","        \"cols\": qc[\"n_cols\"],\n","        \"plans_usage\": qc[\"plans_usage\"],\n","        \"warnings\": min(qc[\"warnings_count\"], 15),\n","        \"out_parquet\": str(parquet_path),\n","        \"out_jsonl\": str(jsonl_path),\n","        \"first_columns\": list(df_norm.columns[:30]),\n","    }\n","    print(json.dumps(summary, indent=2, ensure_ascii=False))\n","    return df_norm, qc, warns\n"],"metadata":{"id":"JY8Mrxr_Wc9_","executionInfo":{"status":"ok","timestamp":1760154834571,"user_tz":360,"elapsed":6,"user":{"displayName":"Lucero Guadalupe Contreras Hernández","userId":"07832692706572343340"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Ejecuta\n","dfI, qcI, warnsI = run_pipeline_investigations(ZIP_PATH_INVESTIGATIONS)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YOvIjReUZTOJ","executionInfo":{"status":"ok","timestamp":1760154898520,"user_tz":360,"elapsed":60050,"user":{"displayName":"Lucero Guadalupe Contreras Hernández","userId":"07832692706572343340"}},"outputId":"be5b0d02-adb9-43be-8323-ac3abf249e87"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"dataset\": \"investigations\",\n","  \"rows\": 153551,\n","  \"cols\": 16,\n","  \"plans_usage\": {\n","    \"A\": 1,\n","    \"B\": 0\n","  },\n","  \"warnings\": 0,\n","  \"out_parquet\": \"/content/drive/MyDrive/NHTSA/processed/investigations.parquet\",\n","  \"out_jsonl\": \"/content/drive/MyDrive/NHTSA/processed/investigations.jsonl\",\n","  \"first_columns\": [\n","    \"NHTSA ACTION NUMBER\",\n","    \"MAKE\",\n","    \"MODEL\",\n","    \"YEAR\",\n","    \"COMPNAME\",\n","    \"MFR_NAME\",\n","    \"ODATE\",\n","    \"CDATE\",\n","    \"CAMPNO\",\n","    \"SUBJECT\",\n","    \"SUMMARY\",\n","    \"__SOURCE_FILE__\",\n","    \"SUBJECT_LEN\",\n","    \"SUMMARY_LEN\",\n","    \"COMPNAME_LEN\",\n","    \"TEXT_TOTAL_LEN\"\n","  ]\n","}\n"]}]},{"cell_type":"code","source":["# MMY\n","cols = [c for c in [\"MAKETXT\",\"MODELTXT\",\"YEARTXT\"] if c in dfI.columns]\n","if cols:\n","    v = dfI[cols].dropna().copy()\n","    v[\"MAKETXT\"] = v[\"MAKETXT\"].astype(str).str.upper().str.strip()\n","    v[\"MODELTXT\"]= v[\"MODELTXT\"].astype(str).str.upper().str.strip()\n","    v[\"YEARTXT\"] = pd.to_numeric(v[\"YEARTXT\"], errors=\"coerce\").astype(\"Int64\")\n","    v[\"MMY_ID\"]  = v[\"MAKETXT\"] + \"|\" + v[\"MODELTXT\"] + \"|\" + v[\"YEARTXT\"].astype(str)\n","    v = v.drop_duplicates().sort_values([\"MAKETXT\",\"MODELTXT\",\"YEARTXT\"])\n","    v.to_csv(OUT_DIR/\"investigations_MMY.csv\", index=False)\n","    print(\"OK -> investigations_MMY.csv\", len(v))\n","\n","# Componentes (L1–L3)\n","if \"COMPONENT\" in dfI.columns:\n","    parts = dfI[\"COMPONENT\"].fillna(\"\").astype(str).str.split(\":\", n=2, expand=True)\n","    comp = pd.DataFrame({\n","        \"COMP_L1\": parts[0].str.strip() if 0 in parts.columns else None,\n","        \"COMP_L2\": parts[1].str.strip() if 1 in parts.columns else None,\n","        \"COMP_L3\": parts[2].str.strip() if 2 in parts.columns else None,\n","    }).replace({\"\": np.nan}).dropna(how=\"all\").drop_duplicates()\n","    comp.to_csv(OUT_DIR/\"investigations_components.csv\", index=False)\n","    print(\"OK -> investigations_components.csv\", len(comp))\n","\n","# IDs y enlace a recalls\n","id_cols = [c for c in [\"ODINUMBER\",\"ACTIONNUMBER\",\"CAMPNO\"] if c in dfI.columns]\n","if id_cols:\n","    ids = dfI[id_cols].dropna(how=\"all\").drop_duplicates()\n","    ids.to_csv(OUT_DIR/\"investigations_ids.csv\", index=False)\n","    print(\"OK -> investigations_ids.csv\", len(ids))\n","\n","# Corpus JSONL (para embeddings)\n","text_col = \"SUMMARY\" if \"SUMMARY\" in dfI.columns else (\"NARRATIVE\" if \"NARRATIVE\" in dfI.columns else None)\n","if text_col:\n","    meta_cols = [c for c in [\"ODINUMBER\",\"ACTIONNUMBER\",\"MAKETXT\",\"MODELTXT\",\"YEARTXT\",\"COMPONENT\",\"DATEOPEN\",\"DATECLOSE\",\"CAMPNO\",\"STATUS\",\"PRIORTYP\"] if c in dfI.columns]\n","    with open(OUT_DIR/\"investigations_corpus.jsonl\", \"w\", encoding=\"utf-8\") as f:\n","        for _, row in dfI[dfI[text_col].notna()].iterrows():\n","            rec = {\n","                \"id\": str(row[\"ACTIONNUMBER\"]) if \"ACTIONNUMBER\" in row and pd.notna(row[\"ACTIONNUMBER\"]) else (str(row[\"ODINUMBER\"]) if \"ODINUMBER\" in row and pd.notna(row[\"ODINUMBER\"]) else None),\n","                \"text\": str(row[text_col]),\n","                \"metadata\": {k: (None if pd.isna(row[k]) else str(row[k])) for k in meta_cols}\n","            }\n","            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n","    print(\"OK -> investigations_corpus.jsonl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n47y8XqwXP2s","executionInfo":{"status":"ok","timestamp":1760154967427,"user_tz":360,"elapsed":18712,"user":{"displayName":"Lucero Guadalupe Contreras Hernández","userId":"07832692706572343340"}},"outputId":"8cfb676e-c091-463d-a156-e3acf1c42d30"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["OK -> investigations_ids.csv 2789\n","OK -> investigations_corpus.jsonl\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9zT1C4fLYhK1"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1XI_7NeK6bq9xpxHMO5YE2zuGWW0kOjvs","timestamp":1760153497470}],"authorship_tag":"ABX9TyOdI+oqwcbCh0tF6REexG5G"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}